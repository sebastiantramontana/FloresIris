<?xml version="1.0"?>
<doc>
    <assembly>
        <name>NeuralNetwork.NET</name>
    </assembly>
    <members>
        <member name="T:NeuralNetworkNET.APIs.CuDnnNetworkLayers">
            <summary>
            A static class that exposes the available cuDNN network layer types
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.APIs.CuDnnNetworkLayers.IsCudaSupportAvailable">
            <summary>
            Gets whether or not the Cuda acceleration is supported on the current system
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.APIs.CuDnnNetworkLayers.FullyConnected(System.Int32,NeuralNetworkNET.APIs.Enums.ActivationType,NeuralNetworkNET.APIs.Enums.WeightsInitializationMode,NeuralNetworkNET.APIs.Enums.BiasInitializationMode)">
            <summary>
            Creates a new fully connected layer with the specified number of input and output neurons, and the given activation function
            </summary>
            <param name="neurons">The number of output neurons</param>
            <param name="activation">The desired activation function to use in the network layer</param>
            <param name="weightsMode">The desired initialization mode for the weights in the network layer</param>
            <param name="biasMode">The desired initialization mode to use for the layer bias values</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.CuDnnNetworkLayers.Softmax(System.Int32,NeuralNetworkNET.APIs.Enums.WeightsInitializationMode,NeuralNetworkNET.APIs.Enums.BiasInitializationMode)">
            <summary>
            Creates a fully connected softmax output layer (used for classification problems with mutually-exclusive classes)
            </summary>
            <param name="outputs">The number of output neurons</param>
            <param name="weightsMode">The desired initialization mode for the weights in the network layer</param>
            <param name="biasMode">The desired initialization mode to use for the layer bias values</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.CuDnnNetworkLayers.Convolutional(System.ValueTuple{System.Int32,System.Int32},System.Int32,NeuralNetworkNET.APIs.Enums.ActivationType,NeuralNetworkNET.APIs.Enums.BiasInitializationMode)">
            <summary>
            Creates a convolutional layer with the desired number of kernels
            </summary>
            <param name="kernel">The volume information of the kernels used in the layer</param>
            <param name="kernels">The number of convolution kernels to apply to the input volume</param>
            <param name="activation">The desired activation function to use in the network layer</param>
            <param name="biasMode">Indicates the desired initialization mode to use for the layer bias values</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.CuDnnNetworkLayers.Convolutional(NeuralNetworkNET.APIs.Structs.ConvolutionInfo,System.ValueTuple{System.Int32,System.Int32},System.Int32,NeuralNetworkNET.APIs.Enums.ActivationType,NeuralNetworkNET.APIs.Enums.BiasInitializationMode)">
            <summary>
            Creates a convolutional layer with the desired number of kernels
            </summary>
            <param name="info">The info on the convolution operation to perform</param>
            <param name="kernel">The volume information of the kernels used in the layer</param>
            <param name="kernels">The number of convolution kernels to apply to the input volume</param>
            <param name="activation">The desired activation function to use in the network layer</param>
            <param name="biasMode">Indicates the desired initialization mode to use for the layer bias values</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.CuDnnNetworkLayers.Convolutional(NeuralNetworkNET.APIs.Delegates.ConvolutionInfoFactory,System.ValueTuple{System.Int32,System.Int32},System.Int32,NeuralNetworkNET.APIs.Enums.ActivationType,NeuralNetworkNET.APIs.Enums.BiasInitializationMode)">
            <summary>
            Creates a convolutional layer with the desired number of kernels
            </summary>
            <param name="factory">The <see cref="T:NeuralNetworkNET.APIs.Delegates.ConvolutionInfoFactory"/> instance to create a <see cref="T:NeuralNetworkNET.APIs.Structs.ConvolutionInfo"/> value to use</param>
            <param name="kernel">The volume information of the kernels used in the layer</param>
            <param name="kernels">The number of convolution kernels to apply to the input volume</param>
            <param name="activation">The desired activation function to use in the network layer</param>
            <param name="biasMode">Indicates the desired initialization mode to use for the layer bias values</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.CuDnnNetworkLayers.Pooling(NeuralNetworkNET.APIs.Enums.ActivationType)">
            <summary>
            Creates a pooling layer with a window of size 2 and a stride of 2
            </summary>
            <param name="activation">The desired activation function to use in the network layer</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.CuDnnNetworkLayers.Pooling(NeuralNetworkNET.APIs.Structs.PoolingInfo,NeuralNetworkNET.APIs.Enums.ActivationType)">
            <summary>
            Creates a pooling layer with a custom mode, window size and stride
            </summary>
            <param name="info">The info on the pooling operation to perform</param>
            <param name="activation">The desired activation function to use in the network layer</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.CuDnnNetworkLayers.Inception(NeuralNetworkNET.APIs.Structs.InceptionInfo,NeuralNetworkNET.APIs.Enums.BiasInitializationMode)">
            <summary>
            Creates a new inception layer with the given features
            </summary>
            <param name="info">The info on the operations to execute inside the layer</param>
            <param name="biasMode">Indicates the desired initialization mode to use for the layer bias values</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.CuDnnNetworkLayers.BatchNormalization(NeuralNetworkNET.APIs.Enums.NormalizationMode,NeuralNetworkNET.APIs.Enums.ActivationType)">
            <summary>
            Creates a new batch normalization layer
            </summary>
            <param name="mode">The normalization mode to use for the new layer</param>
            <param name="activation">The desired activation function to use in the network layer</param>
        </member>
        <member name="T:NeuralNetworkNET.APIs.DatasetLoader">
            <summary>
            A static class with helper methods to easily create datasets to use to train and test a network
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.APIs.DatasetLoader.Training(System.Collections.Generic.IEnumerable{System.ValueTuple{System.Single[],System.Single[]}},System.Int32)">
            <summary>
            Creates a new <see cref="T:NeuralNetworkNET.APIs.Interfaces.Data.ITrainingDataset"/> instance to train a network from the input collection, with the specified batch size
            </summary>
            <param name="data">The source collection to use to build the training dataset</param>
            <param name="size">The desired dataset batch size</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.DatasetLoader.Training(System.Collections.Generic.IEnumerable{System.Func{System.ValueTuple{System.Single[],System.Single[]}}},System.Int32)">
            <summary>
            Creates a new <see cref="T:NeuralNetworkNET.APIs.Interfaces.Data.ITrainingDataset"/> instance to train a network from the input collection, with the specified batch size
            </summary>
            <param name="data">The source collection to use to build the training dataset, where the samples will be extracted from the input <see cref="T:System.Func`1"/> instances in parallel</param>
            <param name="size">The desired dataset batch size</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.DatasetLoader.Training(System.ValueTuple{System.Single[0:,0:],System.Single[0:,0:]},System.Int32)">
            <summary>
            Creates a new <see cref="T:NeuralNetworkNET.APIs.Interfaces.Data.ITrainingDataset"/> instance to train a network from the input matrices, with the specified batch size
            </summary>
            <param name="data">The source matrices to use to build the training dataset</param>
            <param name="size">The desired dataset batch size</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.DatasetLoader.Training``1(System.Collections.Generic.IEnumerable{System.ValueTuple{System.String,System.Single[]}},System.Int32,NeuralNetworkNET.APIs.Enums.ImageNormalizationMode,System.Action{SixLabors.ImageSharp.Processing.IImageProcessingContext{``0}}[])">
            <summary>
            Creates a new <see cref="T:NeuralNetworkNET.APIs.Interfaces.Data.ITrainingDataset"/> instance to train a network from the input data, where each input sample is an image in a specified format
            </summary>
            <typeparam name="TPixel">The type of image pixels. It must be either <see cref="T:SixLabors.ImageSharp.PixelFormats.Alpha8"/>, <see cref="T:SixLabors.ImageSharp.PixelFormats.Rgb24"/>, <see cref="T:SixLabors.ImageSharp.PixelFormats.Argb32"/> or <see cref="T:SixLabors.ImageSharp.PixelFormats.Rgba32"/></typeparam>
            <param name="data">A list of <see cref="T:System.ValueTuple`2"/> items, where the first element is the image path and the second is a vector with the expected outputs</param>
            <param name="size">The desired dataset batch size</param>
            <param name="normalization">The desired image normalization mode to use when loading the images</param>
            <param name="modifiers">The optional <see cref="T:System.Action`1"/> instances to use to modify the loaded image. If no modifiers are provided, each loaded image will not me tweaked. If one or more
            modifiers are passed to the method, a different image will be added to the dataset for each given modifier. This can be used to easily expand an image dataset.</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.DatasetLoader.Training``1(System.Collections.Generic.IEnumerable{System.ValueTuple{System.String,System.Func{System.Single[]}}},System.Int32,NeuralNetworkNET.APIs.Enums.ImageNormalizationMode,System.Action{SixLabors.ImageSharp.Processing.IImageProcessingContext{``0}}[])">
            <summary>
            Creates a new <see cref="T:NeuralNetworkNET.APIs.Interfaces.Data.ITrainingDataset"/> instance to train a network from the input data, where each input sample is an image in a specified format
            </summary>
            <typeparam name="TPixel">The type of image pixels. It must be either <see cref="T:SixLabors.ImageSharp.PixelFormats.Alpha8"/>, <see cref="T:SixLabors.ImageSharp.PixelFormats.Rgb24"/>, <see cref="T:SixLabors.ImageSharp.PixelFormats.Argb32"/> or <see cref="T:SixLabors.ImageSharp.PixelFormats.Rgba32"/></typeparam>
            <param name="data">A list of <see cref="T:System.ValueTuple`2"/> items, where the first element is the image path and the second is a <see cref="T:System.Func`1"/> returning a vector with the expected outputs</param>
            <param name="size">The desired dataset batch size</param>
            <param name="normalization">The desired image normalization mode to use when loading the images</param>
            <param name="modifiers">The optional <see cref="T:System.Action`1"/> instances to use to modify the loaded image. If no modifiers are provided, each loaded image will not me tweaked. If one or more
            modifiers are passed to the method, a different image will be added to the dataset for each given modifier. This can be used to easily expand an image dataset.</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.DatasetLoader.Validation(System.Collections.Generic.IEnumerable{System.ValueTuple{System.Single[],System.Single[]}},System.Single,System.Int32)">
            <summary>
            Creates a new <see cref="T:NeuralNetworkNET.APIs.Interfaces.Data.IValidationDataset"/> instance to validate a network accuracy from the input collection
            </summary>
            <param name="data">The source collection to use to build the validation dataset</param>
            <param name="tolerance">The desired tolerance to test the network for convergence</param>
            <param name="epochs">The epochs interval to consider when testing the network for convergence</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.DatasetLoader.Validation(System.Collections.Generic.IEnumerable{System.Func{System.ValueTuple{System.Single[],System.Single[]}}},System.Single,System.Int32)">
            <summary>
            Creates a new <see cref="T:NeuralNetworkNET.APIs.Interfaces.Data.IValidationDataset"/> instance to validate a network accuracy from the input collection
            </summary>
            <param name="data">The source collection to use to build the validation dataset, where the samples will be extracted from the input <see cref="T:System.Func`1"/> instances in parallel</param>
            <param name="tolerance">The desired tolerance to test the network for convergence</param>
            <param name="epochs">The epochs interval to consider when testing the network for convergence</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.DatasetLoader.Validation(System.ValueTuple{System.Single[0:,0:],System.Single[0:,0:]},System.Single,System.Int32)">
            <summary>
            Creates a new <see cref="T:NeuralNetworkNET.APIs.Interfaces.Data.IValidationDataset"/> instance to validate a network accuracy from the input collection
            </summary>
            <param name="data">The source collection to use to build the validation dataset</param>
            <param name="tolerance">The desired tolerance to test the network for convergence</param>
            <param name="epochs">The epochs interval to consider when testing the network for convergence</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.DatasetLoader.Validation``1(System.Collections.Generic.IEnumerable{System.ValueTuple{System.String,System.Single[]}},System.Single,System.Int32,NeuralNetworkNET.APIs.Enums.ImageNormalizationMode,System.Action{SixLabors.ImageSharp.Processing.IImageProcessingContext{``0}}[])">
            <summary>
            Creates a new <see cref="T:NeuralNetworkNET.APIs.Interfaces.Data.IValidationDataset"/> instance to validate a network accuracy from the input collection
            </summary>
            <typeparam name="TPixel">The type of image pixels. It must be either <see cref="T:SixLabors.ImageSharp.PixelFormats.Alpha8"/>, <see cref="T:SixLabors.ImageSharp.PixelFormats.Rgb24"/>, <see cref="T:SixLabors.ImageSharp.PixelFormats.Argb32"/> or <see cref="T:SixLabors.ImageSharp.PixelFormats.Rgba32"/></typeparam>
            <param name="data">A list of <see cref="T:System.ValueTuple`2"/> items, where the first element is the image path and the second is a vector with the expected outputs</param>
            <param name="tolerance">The desired tolerance to test the network for convergence</param>
            <param name="epochs">The epochs interval to consider when testing the network for convergence</param>
            <param name="normalization">The desired image normalization mode to use when loading the images</param>
            <param name="modifiers">The optional <see cref="T:System.Action`1"/> instances to use to modify the loaded image. If no modifiers are provided, each loaded image will not me tweaked. If one or more
            modifiers are passed to the method, a different image will be added to the dataset for each given modifier. This can be used to easily expand an image dataset.</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.DatasetLoader.Validation``1(System.Collections.Generic.IEnumerable{System.ValueTuple{System.String,System.Func{System.Single[]}}},System.Single,System.Int32,NeuralNetworkNET.APIs.Enums.ImageNormalizationMode,System.Action{SixLabors.ImageSharp.Processing.IImageProcessingContext{``0}}[])">
            <summary>
            Creates a new <see cref="T:NeuralNetworkNET.APIs.Interfaces.Data.IValidationDataset"/> instance to validate a network accuracy from the input collection
            </summary>
            <typeparam name="TPixel">The type of image pixels. It must be either <see cref="T:SixLabors.ImageSharp.PixelFormats.Alpha8"/>, <see cref="T:SixLabors.ImageSharp.PixelFormats.Rgb24"/>, <see cref="T:SixLabors.ImageSharp.PixelFormats.Argb32"/> or <see cref="T:SixLabors.ImageSharp.PixelFormats.Rgba32"/></typeparam>
            <param name="data">A list of <see cref="T:System.ValueTuple`2"/> items, where the first element is the image path and the second is a <see cref="T:System.Func`1"/> returning a vector with the expected outputs</param>
            <param name="tolerance">The desired tolerance to test the network for convergence</param>
            <param name="epochs">The epochs interval to consider when testing the network for convergence</param>
            <param name="normalization">The desired image normalization mode to use when loading the images</param>
            <param name="modifiers">The optional <see cref="T:System.Action`1"/> instances to use to modify the loaded image. If no modifiers are provided, each loaded image will not me tweaked. If one or more
            modifiers are passed to the method, a different image will be added to the dataset for each given modifier. This can be used to easily expand an image dataset.</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.DatasetLoader.Test(System.Collections.Generic.IEnumerable{System.ValueTuple{System.Single[],System.Single[]}},System.Action{NeuralNetworkNET.SupervisedLearning.Progress.TrainingProgressEventArgs})">
            <summary>
            Creates a new <see cref="T:NeuralNetworkNET.APIs.Interfaces.Data.ITestDataset"/> instance to test a network from the input collection
            </summary>
            <param name="data">The source collection to use to build the test dataset</param>
            <param name="progress">The optional progress callback to use</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.DatasetLoader.Test(System.Collections.Generic.IEnumerable{System.Func{System.ValueTuple{System.Single[],System.Single[]}}},System.Action{NeuralNetworkNET.SupervisedLearning.Progress.TrainingProgressEventArgs})">
            <summary>
            Creates a new <see cref="T:NeuralNetworkNET.APIs.Interfaces.Data.ITestDataset"/> instance to test a network from the input collection
            </summary>
            <param name="data">The source collection to use to build the test dataset, where the samples will be extracted from the input <see cref="T:System.Func`1"/> instances in parallel</param>
            <param name="progress">The optional progress callback to use</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.DatasetLoader.Test(System.ValueTuple{System.Single[0:,0:],System.Single[0:,0:]},System.Action{NeuralNetworkNET.SupervisedLearning.Progress.TrainingProgressEventArgs})">
            <summary>
            Creates a new <see cref="T:NeuralNetworkNET.APIs.Interfaces.Data.ITestDataset"/> instance to test a network from the input collection
            </summary>
            <param name="data">The source collection to use to build the test dataset</param>
            <param name="progress">The optional progress callback to use</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.DatasetLoader.Test``1(System.Collections.Generic.IEnumerable{System.ValueTuple{System.String,System.Single[]}},System.Action{NeuralNetworkNET.SupervisedLearning.Progress.TrainingProgressEventArgs},NeuralNetworkNET.APIs.Enums.ImageNormalizationMode,System.Action{SixLabors.ImageSharp.Processing.IImageProcessingContext{``0}}[])">
            <summary>
            Creates a new <see cref="T:NeuralNetworkNET.APIs.Interfaces.Data.ITestDataset"/> instance to test a network from the input collection
            </summary>
            <typeparam name="TPixel">The type of image pixels. It must be either <see cref="T:SixLabors.ImageSharp.PixelFormats.Alpha8"/>, <see cref="T:SixLabors.ImageSharp.PixelFormats.Rgb24"/>, <see cref="T:SixLabors.ImageSharp.PixelFormats.Argb32"/> or <see cref="T:SixLabors.ImageSharp.PixelFormats.Rgba32"/></typeparam>
            <param name="data">A list of <see cref="T:System.ValueTuple`2"/> items, where the first element is the image path and the second is a vector with the expected outputs</param>
            <param name="progress">The optional progress callback to use</param>
            <param name="normalization">The desired image normalization mode to use when loading the images</param>
            <param name="modifiers">The optional <see cref="T:System.Action`1"/> instances to use to modify the loaded image. If no modifiers are provided, each loaded image will not me tweaked. If one or more
            modifiers are passed to the method, a different image will be added to the dataset for each given modifier. This can be used to easily expand an image dataset.</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.DatasetLoader.Test``1(System.Collections.Generic.IEnumerable{System.ValueTuple{System.String,System.Func{System.Single[]}}},System.Action{NeuralNetworkNET.SupervisedLearning.Progress.TrainingProgressEventArgs},NeuralNetworkNET.APIs.Enums.ImageNormalizationMode,System.Action{SixLabors.ImageSharp.Processing.IImageProcessingContext{``0}}[])">
            <summary>
            Creates a new <see cref="T:NeuralNetworkNET.APIs.Interfaces.Data.ITestDataset"/> instance to test a network from the input collection
            </summary>
            <typeparam name="TPixel">The type of image pixels. It must be either <see cref="T:SixLabors.ImageSharp.PixelFormats.Alpha8"/>, <see cref="T:SixLabors.ImageSharp.PixelFormats.Rgb24"/>, <see cref="T:SixLabors.ImageSharp.PixelFormats.Argb32"/> or <see cref="T:SixLabors.ImageSharp.PixelFormats.Rgba32"/></typeparam>
            <param name="data">A list of <see cref="T:System.ValueTuple`2"/> items, where the first element is the image path and the second is a <see cref="T:System.Func`1"/> returning a vector with the expected outputs</param>
            <param name="progress">The optional progress callback to use</param>
            <param name="normalization">The desired image normalization mode to use when loading the images</param>
            <param name="modifiers">The optional <see cref="T:System.Action`1"/> instances to use to modify the loaded image. If no modifiers are provided, each loaded image will not me tweaked. If one or more
            modifiers are passed to the method, a different image will be added to the dataset for each given modifier. This can be used to easily expand an image dataset.</param>
        </member>
        <member name="T:NeuralNetworkNET.APIs.Datasets.Cifar10">
            <summary>
            A static class that provides quick access to the CIFAR-10 database, see <a href="https://www.cs.toronto.edu/~kriz/cifar.html">cs.toronto.edu/~kriz/cifar.html</a>
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Datasets.Cifar10.GetTrainingDatasetAsync(System.Int32,System.IProgress{NeuralNetworkNET.Extensions.HttpProgress},System.Threading.CancellationToken)">
            <summary>
            Downloads the CIFAR-10 training datasets and returns a new <see cref="T:NeuralNetworkNET.APIs.Interfaces.Data.ITestDataset"/> instance
            </summary>
            <param name="size">The desired dataset batch size</param>
            <param name="callback">The optional progress calback</param>
            <param name="token">An optional cancellation token for the operation</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Datasets.Cifar10.GetTestDatasetAsync(System.Action{NeuralNetworkNET.SupervisedLearning.Progress.TrainingProgressEventArgs},System.IProgress{NeuralNetworkNET.Extensions.HttpProgress},System.Threading.CancellationToken)">
            <summary>
            Downloads the CIFAR-10 test datasets and returns a new <see cref="T:NeuralNetworkNET.APIs.Interfaces.Data.ITestDataset"/> instance
            </summary>
            <param name="progress">The optional progress callback to use</param>
            <param name="callback">The optional progress calback</param>
            <param name="token">An optional cancellation token for the operation</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Datasets.Cifar10.ExportDatasetAsync(System.IO.DirectoryInfo,System.Threading.CancellationToken)">
            <summary>
            Downloads and exports the full CIFAR-10 dataset (both training and test samples) to the target directory
            </summary>
            <param name="directory">The target directory</param>
            <param name="token">The cancellation token for the operation</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Datasets.Cifar10.ParseSamples(System.Func{System.IO.Stream},System.Int32)">
            <summary>
            Parses a CIFAR-10 .bin file
            </summary>
            <param name="factory">A <see cref="T:System.Func`1"/> that returns the <see cref="T:System.IO.Stream"/> to read</param>
            <param name="count">The number of samples to parse</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Datasets.Cifar10.ExportSamples(System.IO.DirectoryInfo,System.ValueTuple{System.String,System.Func{System.IO.Stream}},System.Int32,System.Threading.CancellationToken)">
            <summary>
            Exports a CIFAR-10 .bin file
            </summary>
            <param name="folder">The target folder to use to save the images</param>
            <param name="source">The source filename and a <see cref="T:System.Func`1"/> that returns the <see cref="T:System.IO.Stream"/> to read</param>
            <param name="count">The number of samples to parse</param>
            <param name="token">A token for the operation</param>
        </member>
        <member name="T:NeuralNetworkNET.APIs.Datasets.Cifar100">
            <summary>
            A static class that provides quick access to the CIFAR-100 database, see <a href="https://www.cs.toronto.edu/~kriz/cifar.html">cs.toronto.edu/~kriz/cifar.html</a>
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Datasets.Cifar100.GetTrainingDatasetAsync(System.Int32,NeuralNetworkNET.APIs.Datasets.Cifar100.Cifar100ClassificationMode,System.IProgress{NeuralNetworkNET.Extensions.HttpProgress},System.Threading.CancellationToken)">
            <summary>
            Downloads the CIFAR-100 training datasets and returns a new <see cref="T:NeuralNetworkNET.APIs.Interfaces.Data.ITestDataset"/> instance
            </summary>
            <param name="size">The desired dataset batch size</param>
            <param name="mode">The desired output mode for the dataset classes</param>
            <param name="callback">The optional progress calback</param>
            <param name="token">An optional cancellation token for the operation</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Datasets.Cifar100.GetTestDatasetAsync(System.Action{NeuralNetworkNET.SupervisedLearning.Progress.TrainingProgressEventArgs},NeuralNetworkNET.APIs.Datasets.Cifar100.Cifar100ClassificationMode,System.IProgress{NeuralNetworkNET.Extensions.HttpProgress},System.Threading.CancellationToken)">
            <summary>
            Downloads the CIFAR-100 test datasets and returns a new <see cref="T:NeuralNetworkNET.APIs.Interfaces.Data.ITestDataset"/> instance
            </summary>
            <param name="progress">The optional progress callback to use</param>
            <param name="mode">The desired output mode for the dataset classes</param>
            <param name="callback">The optional progress calback</param>
            <param name="token">An optional cancellation token for the operation</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Datasets.Cifar100.ExportDatasetAsync(System.IO.DirectoryInfo,System.Threading.CancellationToken)">
            <summary>
            Downloads and exports the full CIFAR-100 dataset (both training and test samples) to the target directory
            </summary>
            <param name="directory">The target directory</param>
            <param name="token">The cancellation token for the operation</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Datasets.Cifar100.ParseSamples(System.Func{System.IO.Stream},System.Int32,NeuralNetworkNET.APIs.Datasets.Cifar100.Cifar100ClassificationMode)">
            <summary>
            Parses a CIFAR-100 .bin file
            </summary>
            <param name="factory">A <see cref="T:System.Func`1"/> that returns the <see cref="T:System.IO.Stream"/> to read</param>
            <param name="count">The number of samples to parse</param>
            <param name="mode">The desired output mode for the dataset classes</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Datasets.Cifar100.ExportSamples(System.IO.DirectoryInfo,System.ValueTuple{System.String,System.Func{System.IO.Stream}},System.Int32,System.Threading.CancellationToken)">
            <summary>
            Exports a CIFAR-100 .bin file
            </summary>
            <param name="folder">The target folder to use to save the images</param>
            <param name="source">The source filename and a <see cref="T:System.Func`1"/> that returns the <see cref="T:System.IO.Stream"/> to read</param>
            <param name="count">The number of samples to parse</param>
            <param name="token">A token for the operation</param>
        </member>
        <member name="T:NeuralNetworkNET.APIs.Datasets.Cifar100.Cifar100ClassificationMode">
            <summary>
            An <see langword="enum"/> indicating the type of outputs for a CIFAR-100 dataset instance
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Datasets.Cifar100.Cifar100ClassificationMode.Coarse">
            <summary>
            Each sample is classified using the 20 available superclasses
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Datasets.Cifar100.Cifar100ClassificationMode.Fine">
            <summary>
            Each sample is classified using the 100 available narrow classes
            </summary>
        </member>
        <member name="T:NeuralNetworkNET.APIs.Datasets.Mnist">
            <summary>
            A static class that provides quick access to the MNIST database, see <a href="http://yann.lecun.com/exdb/mnist/">yann.lecun.com/exdb/mnist/</a>
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Datasets.Mnist.GetTrainingDatasetAsync(System.Int32,System.Threading.CancellationToken)">
            <summary>
            Downloads the MNIST training datasets and returns a new <see cref="T:NeuralNetworkNET.APIs.Interfaces.Data.ITestDataset"/> instance
            </summary>
            <param name="size">The desired dataset batch size</param>
            <param name="token">An optional cancellation token for the operation</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Datasets.Mnist.GetTestDatasetAsync(System.Action{NeuralNetworkNET.SupervisedLearning.Progress.TrainingProgressEventArgs},System.Threading.CancellationToken)">
            <summary>
            Downloads the MNIST test datasets and returns a new <see cref="T:NeuralNetworkNET.APIs.Interfaces.Data.ITestDataset"/> instance
            </summary>
            <param name="progress">The optional progress callback to use</param>
            <param name="token">An optional cancellation token for the operation</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Datasets.Mnist.ExportDatasetAsync(System.IO.DirectoryInfo,System.Threading.CancellationToken)">
            <summary>
            Downloads and exports the full MNIST dataset (both training and test samples) to the target directory
            </summary>
            <param name="directory">The target directory</param>
            <param name="token">The cancellation token for the operation</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Datasets.Mnist.ParseSamples(System.ValueTuple{System.Func{System.IO.Stream},System.Func{System.IO.Stream}},System.Int32)">
            <summary>
            Parses a MNIST dataset
            </summary>
            <param name="factory">A pair of factories for the input <see cref="T:System.IO.Stream"/> instances to read</param>
            <param name="count">The number of samples to parse</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Datasets.Mnist.ExportSamples(System.IO.DirectoryInfo,System.ValueTuple{System.String,System.Func{System.IO.Stream},System.Func{System.IO.Stream}},System.Int32,System.Threading.CancellationToken)">
            <summary>
            Exports a MNIST dataset file
            </summary>
            <param name="folder">The target folder to use to save the images</param>
            <param name="source">A pair of factories for the input <see cref="T:System.IO.Stream"/> instances to read</param>
            <param name="count">The number of samples to parse</param>
            <param name="token">A token for the operation</param>
        </member>
        <member name="T:NeuralNetworkNET.APIs.Delegates.AccuracyTester">
            <summary>
            A <see langword="delegate"/> that wraps a function used to test whether or not the outputs for an evaluation sample match the expected results
            </summary>
            <param name="yHat">The sample outputs produced by the current network</param>
            <param name="y">The expected outputs for the sample</param>
        </member>
        <member name="T:NeuralNetworkNET.APIs.Delegates.ConvolutionInfoFactory">
            <summary>
            A <see langword="delegate"/> that calculates a <see cref="T:NeuralNetworkNET.APIs.Structs.ConvolutionInfo"/> value from the input <see cref="T:NeuralNetworkNET.APIs.Structs.TensorInfo"/> instances
            </summary>
            <param name="input">The <see cref="T:NeuralNetworkNET.APIs.Structs.TensorInfo"/> instance that describes the input shape</param>
            <param name="kernel">The size info on the convolutional kernels to apply to the input data</param>
        </member>
        <member name="T:NeuralNetworkNET.APIs.Delegates.LayerFactory">
            <summary>
            A <see langword="delegate"/> that represents a factory that produces instances of a specific layer type, with user-defined parameters.
            This wrapper acts as an intemediary to streamline the user-side C# sintax when building up a new network structure, as all the input
            details for each layer will be automatically computed during the network setup.
            </summary>
            <param name="info">The <see cref="T:NeuralNetworkNET.APIs.Structs.TensorInfo"/> for the inputs of the upcoming network layer</param>
            <remarks>It is also possible to invoke a <see cref="T:NeuralNetworkNET.APIs.Delegates.LayerFactory"/> instance just like any other <see langword="delegate"/> to immediately get an <see cref="T:NeuralNetworkNET.APIs.Interfaces.INetworkLayer"/> value</remarks>
        </member>
        <member name="T:NeuralNetworkNET.APIs.Enums.ActivationType">
            <summary>
            Indicates an activation function to use in a neural network
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Enums.ActivationType.Sigmoid">
            <summary>
            The sigmoid function, 1 / (1 + e^(-x))
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Enums.ActivationType.Tanh">
            <summary>
            The tanh function, (e^x - e^(-x)) / (e^x + e^(-x))
            </summary>
            <remarks>It has the advantage of being centered vertically at the origin, instead
            of being shifted upwards like the classic sigmoid function</remarks>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Enums.ActivationType.LeCunTanh">
            <summary>
            A customized tanh function proposed by LeCun (http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Enums.ActivationType.ReLU">
            <summary>
            The linear rectified function, max(0, x)
            </summary>
            <remarks>It doesn't saturate like the sigmoid or tanh function and it converges faster</remarks>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Enums.ActivationType.LeakyReLU">
            <summary>
            The leaky ReLU function, max(0.01x, x)
            </summary>
            <remarks>It has the advance of having a nonzero gradient for negative values of x, so
            a negative neuron won't be stuck there during the rest of the training</remarks>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Enums.ActivationType.AbsoluteReLU">
            <summary>
            The absolute linear rectified function, |x|
            </summary>
            <remarks>It can perform well with images when the dataset contains samples with different brightness levels</remarks>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Enums.ActivationType.Softmax">
            <summary>
            The softmax function, e^x/sum{k}(e^x(k))
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Enums.ActivationType.Softplus">
            <summary>
            The softplus function, ln(1 + e^x)
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Enums.ActivationType.ELU">
            <summary>
            The exponential linear unit function, [{ x, x positive}, { e^x - 1, otherwise}];
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Enums.ActivationType.Identity">
            <summary>
            A linear activation function that just returns the input value
            </summary>
        </member>
        <member name="T:NeuralNetworkNET.APIs.Enums.BiasInitializationMode">
            <summary>
            An <see langword="enum"/> indicating an initialization mode for the biases in a network layer
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Enums.BiasInitializationMode.Zero">
            <summary>
            All the bias values are initially set to 0
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Enums.BiasInitializationMode.Gaussian">
            <summary>
            Every bias value is assigned from a gaussian distribution ~N(0, 1)
            </summary>
        </member>
        <member name="T:NeuralNetworkNET.APIs.Enums.ComputationGraphNodeType">
            <summary>
            An <see langword="enum"/> that indicates the type of a node in a graph network
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Enums.ComputationGraphNodeType.Input">
            <summary>
            The root node for a computation graph, that forwards the network inputs through the computation pipeline(s)
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Enums.ComputationGraphNodeType.Processing">
            <summary>
            A computation graph node with an associated <see cref="T:NeuralNetworkNET.APIs.Interfaces.INetworkLayer"/> that processes an input <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/>
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Enums.ComputationGraphNodeType.TrainingBranch">
            <summary>
            The root node for a training sub-graph, a secondary training branch used during backpropagation to inject partial gradients
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Enums.ComputationGraphNodeType.DepthConcatenation">
            <summary>
            A computation graph node that merges a series of inputs by stacking them along the depth axis
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Enums.ComputationGraphNodeType.Sum">
            <summary>
            A computation graph node that merges a series of inputs by summing their values together
            </summary>
        </member>
        <member name="T:NeuralNetworkNET.APIs.Enums.ConvolutionMode">
            <summary>
            A simple <see langword="enum"/> indicating the type of convolution operation to perform
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Enums.ConvolutionMode.Convolution">
            <summary>
            The default convolution mode, with the kernel taargeting pixels in the opposite position
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Enums.ConvolutionMode.CrossCorrelation">
            <summary>
            The cross-correlation mode (equivalent to a convolution with a flipped kernel)
            </summary>
        </member>
        <member name="T:NeuralNetworkNET.APIs.Enums.ExecutionModePreference">
            <summary>
            Indicates the preferred mode of execution for all expensive operations in the library
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Enums.ExecutionModePreference.Cpu">
            <summary>
            Only CPU-powered functions are executed. This means that network layers are deserialized through the <see cref="T:NeuralNetworkNET.APIs.NetworkLayers"/> class,
            and that all other computations will only be scheduled on the CPU.
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Enums.ExecutionModePreference.Cuda">
            <summary>
            CUDA-powered functions are supported. When using this mode, network layers are deserialized through the <see cref="T:NeuralNetworkNET.APIs.CuDnnNetworkLayers"/> class,
            and all other available operations will be scheduled on the GPU.
            </summary>
        </member>
        <member name="T:NeuralNetworkNET.APIs.Enums.ImageNormalizationMode">
            <summary>
            Indicates the type of normalization to apply to a loaded image
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Enums.ImageNormalizationMode.Sigmoid">
            <summary>
            The individual pixel values are mapped in the [0,1] range
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Enums.ImageNormalizationMode.Normal">
            <summary>
            The individual pixel values are mapped in the [-1,1] range
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Enums.ImageNormalizationMode.None">
            <summary>
            No normalization is applied, and all the pixel values are loaded with their original value
            </summary>
        </member>
        <member name="T:NeuralNetworkNET.APIs.Enums.LayerType">
            <summary>
            Indicates the type of a neural network layer
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Enums.LayerType.FullyConnected">
            <summary>
            A fully connected layer, mapping n inputs to m outputs
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Enums.LayerType.Convolutional">
            <summary>
            A convolutional layer, which keeps spatial information on the input volume
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Enums.LayerType.Pooling">
            <summary>
            A pooling layer, useful to reduce the size of the input data volume
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Enums.LayerType.Output">
            <summary>
            A fully connected output layer, with an arbitrary activation and cost function
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Enums.LayerType.Softmax">
            <summary>
            A softmax layer, with the softmax activation and log-likelyhood cost function
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Enums.LayerType.BatchNormalization">
            <summary>
            A batch normalization layer, used to scale the input batch into a 0-mean, 1-variance activations map
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Enums.LayerType.Inception">
            <summary>
            An inception module, combining different kinds of convolution with a pooling operation
            </summary>
        </member>
        <member name="T:NeuralNetworkNET.APIs.Enums.NetworkType">
            <summary>
            Indicates the type of a given neural network
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Enums.NetworkType.Sequential">
            <summary>
            The classic network model, consisting in a linear stack of connected layers
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Enums.NetworkType.ComputationGraph">
            <summary>
            A network with a custom computation graph used to process its inputs
            </summary>
        </member>
        <member name="T:NeuralNetworkNET.APIs.Enums.NormalizationMode">
            <summary>
            An <see langword="enum"/> indicating the normalization mode to apply to the input data of a layer
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Enums.NormalizationMode.PerActivation">
            <summary>
            Activation-wise normalization, with a separate mean and variance value per activation
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Enums.NormalizationMode.Spatial">
            <summary>
            Spatial normalization, with a single mean and variance value per input channel (feature map)
            </summary>
        </member>
        <member name="T:NeuralNetworkNET.APIs.Enums.PoolingMode">
            <summary>
            Indicates the pooling mode performed by a pooling layer
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Enums.PoolingMode.Max">
            <summary>
            Only the highest neuron in each input receptive field is propagated
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Enums.PoolingMode.AverageIncludingPadding">
            <summary>
            The average value in each input receptive field (including padding neurons) is propagated
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Enums.PoolingMode.AverageExcludingPadding">
            <summary>
            The average value in each input receptive field (excluding padding neurons) is propagated
            </summary>
        </member>
        <member name="T:NeuralNetworkNET.APIs.Enums.TrainingReportType">
            <summary>
            Indicates a type of training progress report
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Enums.TrainingReportType.Accuracy">
            <summary>
            The dataset accuracy according to the expected outputs
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Enums.TrainingReportType.Cost">
            <summary>
            The cost function value for the current dataset being evaluated
            </summary>
        </member>
        <member name="T:NeuralNetworkNET.APIs.Enums.TrainingStopReason">
            <summary>
            Indicates the reason why a network training session has stopped
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Enums.TrainingStopReason.EpochsCompleted">
            <summary>
            All the expected training epochs have been completed correctly
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Enums.TrainingStopReason.EarlyStopping">
            <summary>
            The validation test has detected a convergence withing the specified parameters and the training has been halted
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Enums.TrainingStopReason.TrainingCanceled">
            <summary>
            The training was explicitly stopped before its completion
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Enums.TrainingStopReason.NumericOverflow">
            <summary>
            The weights or biases in the network became invalid during the training
            </summary>
        </member>
        <member name="T:NeuralNetworkNET.APIs.Enums.WeightsInitializationMode">
            <summary>
            An <see langword="enum"/> indicating an initialization mode to use for the weights in a network layer
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Enums.WeightsInitializationMode.LeCunUniform">
            <summary>
            LeCun uniform distribution, from LeCun 98, Efficient Backprop, see <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf">yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf</a>
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Enums.WeightsInitializationMode.GlorotNormal">
            <summary>
            Glorot &amp; Bengio normal distribution, from Glorot &amp; Bengio, AISTATS 2010
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Enums.WeightsInitializationMode.GlorotUniform">
            <summary>
            Glorot &amp; Bengio uniform distribution, see <a href="http://github.com/fchollet/keras/blob/998efc04eefa0c14057c1fa87cab71df5b24bf7e/keras/initializations.py">github.com/fchollet/keras/blob/998efc04eefa0c14057c1fa87cab71df5b24bf7e/keras/initializations.py</a>
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Enums.WeightsInitializationMode.HeEtAlNormal">
            <summary>
            He et al. normal distribution, see <a href="http://arxiv.org/abs/1502.01852">arxiv.org/abs/1502.01852</a>
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Enums.WeightsInitializationMode.HeEtAlUniform">
            <summary>
            He et al. uniform distribution, see <a href="http://arxiv.org/abs/1502.01852">arxiv.org/abs/1502.01852</a>
            </summary>
        </member>
        <member name="T:NeuralNetworkNET.APIs.Interfaces.Data.IDataset">
            <summary>
            An interface for a dataset used to train or test a network
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.APIs.Interfaces.Data.IDataset.Count">
            <summary>
            Gets the number of samples in the current dataset
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.APIs.Interfaces.Data.IDataset.InputFeatures">
            <summary>
            Gets the number of features in each dataset sample
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.APIs.Interfaces.Data.IDataset.OutputFeatures">
            <summary>
            Gets the number of features in each output sample in the dataset
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.APIs.Interfaces.Data.IDataset.ByteSize">
            <summary>
            Gets the total raw size in bytes for the current dataset
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.APIs.Interfaces.Data.IDataset.Item(System.Int32)">
            <summary>
            Gets the dataset sample at the input position. Note that the dataset is shuffled during training.
            </summary>
            <param name="i">The index of the sample to retrieve</param>
        </member>
        <member name="P:NeuralNetworkNET.APIs.Interfaces.Data.IDataset.Id">
            <summary>
            Gets a unique content id for the current dataset (not the same as <see cref="M:System.Object.GetHashCode"/> method
            that can be used to compare two <see cref="T:NeuralNetworkNET.APIs.Interfaces.Data.IDataset"/> instances and check if they contain the same samples (regardless of their relative order)
            </summary>
        </member>
        <member name="T:NeuralNetworkNET.APIs.Interfaces.Data.ITestDataset">
            <summary>
            An interface for a dataset used to test a network being trained
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.APIs.Interfaces.Data.ITestDataset.ProgressCallback">
            <summary>
            Gets the callback used to report the training progress
            </summary>
        </member>
        <member name="T:NeuralNetworkNET.APIs.Interfaces.Data.ITrainingDataset">
            <summary>
            An interface for a batched dataset used to train a network
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.APIs.Interfaces.Data.ITrainingDataset.BatchSize">
            <summary>
            Gets or sets the number of samples in each samples batch in the current dataset
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.APIs.Interfaces.Data.ITrainingDataset.BatchesCount">
            <summary>
            Gets the number of training batches in the current dataset (according to the number of samples and the batch size)
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Interfaces.Data.ITrainingDataset.Expand(System.Func{System.Single[],System.Single[]}[])">
            <summary>
            Artificially expands the current dataset applying the input transformation to each sample to create new ones
            </summary>
            <param name="factories">The list of functions to use to generate new samples from each one in the dataset</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Interfaces.Data.ITrainingDataset.ExtractTest(System.Single,System.Action{NeuralNetworkNET.SupervisedLearning.Progress.TrainingProgressEventArgs})">
            <summary>
            Removes a specified fraction of samples from the current instance and returns a new <see cref="T:NeuralNetworkNET.APIs.Interfaces.Data.ITestDataset"/>
            </summary>
            <param name="ratio">The ratio of samples to include in the returned <see cref="T:NeuralNetworkNET.APIs.Interfaces.Data.ITrainingDataset"/></param>
            <param name="progress">The optional progress callback to use</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Interfaces.Data.ITrainingDataset.ExtractValidation(System.Single,System.Single,System.Int32)">
            <summary>
            Returns a pair of new datasets, where the first is an <see cref="T:NeuralNetworkNET.APIs.Interfaces.Data.ITrainingDataset"/> with the specified fraction of samples and 
            the second is an <see cref="T:NeuralNetworkNET.APIs.Interfaces.Data.IValidationDataset"/> with the remaining number of samples from the current dataset
            </summary>
            <param name="ratio">The ratio of samples to include in the returned <see cref="T:NeuralNetworkNET.APIs.Interfaces.Data.ITrainingDataset"/></param>
            <param name="tolerance">The desired tolerance to test the network for convergence</param>
            <param name="epochs">The epochs interval to consider when testing the network for convergence</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Interfaces.Data.ITrainingDataset.PartitionWithTest(System.Single,System.Action{NeuralNetworkNET.SupervisedLearning.Progress.TrainingProgressEventArgs})">
            <summary>
            Returns a pair of new datasets, where the first is an <see cref="T:NeuralNetworkNET.APIs.Interfaces.Data.ITrainingDataset"/> with the specified fraction of samples and 
            the second is an <see cref="T:NeuralNetworkNET.APIs.Interfaces.Data.ITestDataset"/> with the remaining number of samples from the current dataset
            </summary>
            <param name="ratio">The ratio of samples to include in the returned <see cref="T:NeuralNetworkNET.APIs.Interfaces.Data.ITrainingDataset"/></param>
            <param name="progress">The optional progress callback to use</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Interfaces.Data.ITrainingDataset.PartitionWithValidation(System.Single,System.Single,System.Int32)">
            <summary>
            Returns a pair of new datasets, where the first is an <see cref="T:NeuralNetworkNET.APIs.Interfaces.Data.ITrainingDataset"/> with the specified fraction of samples and 
            the second is an <see cref="T:NeuralNetworkNET.APIs.Interfaces.Data.IValidationDataset"/> with the remaining number of samples from the current dataset
            </summary>
            <param name="ratio">The ratio of samples to include in the returned <see cref="T:NeuralNetworkNET.APIs.Interfaces.Data.ITrainingDataset"/></param>
            <param name="tolerance">The desired tolerance to test the network for convergence</param>
            <param name="epochs">The epochs interval to consider when testing the network for convergence</param>
        </member>
        <member name="T:NeuralNetworkNET.APIs.Interfaces.Data.IValidationDataset">
            <summary>
            An interface for a validation dataset, with user-defined tolerance parameters
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.APIs.Interfaces.Data.IValidationDataset.Tolerance">
            <summary>
            Gets the convergence tolerance for the validation dataset
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.APIs.Interfaces.Data.IValidationDataset.EpochsInterval">
            <summary>
            Gets the maximum number of epochs allowed to pass within the tolerance threshold before stopping the training
            </summary>
        </member>
        <member name="T:NeuralNetworkNET.APIs.Interfaces.IClonable`1">
            <summary>
            An interface for an object that supports deep cloning
            </summary>
            <typeparam name="T"></typeparam>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Interfaces.IClonable`1.Clone">
            <summary>
            Returns a deep clone of the current instance
            </summary>
        </member>
        <member name="T:NeuralNetworkNET.APIs.Interfaces.IComputationGraphNode">
            <summary>
            The base <see langword="interface"/> for the various types of nodes in a graph network
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.APIs.Interfaces.IComputationGraphNode.Type">
            <summary>
            Indicates the type of the current node
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.APIs.Interfaces.IComputationGraphNode.Children">
            <summary>
            Gets the collection of child nodes for the current computation graph node
            </summary>
        </member>
        <member name="T:NeuralNetworkNET.APIs.Interfaces.INetworkLayer">
            <summary>
            An interface that represents a single layer in a multilayer neural network
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.APIs.Interfaces.INetworkLayer.LayerType">
            <summary>
            Gets the kind of neural network layer
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.APIs.Interfaces.INetworkLayer.InputInfo">
            <summary>
            Gets the info on the layer inputs
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.APIs.Interfaces.INetworkLayer.OutputInfo">
            <summary>
            Gets the info on the layer outputs
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.APIs.Interfaces.INetworkLayer.InputValues">
            <summary>
            The input values
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.APIs.Interfaces.INetworkLayer.SumValues">
            <summary>
            The sum values
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.APIs.Interfaces.INetworkLayer.OutputValues">
            <summary>
            The output values
            </summary>
        </member>
        <member name="T:NeuralNetworkNET.APIs.Interfaces.INeuralNetwork">
            <summary>
            An interface to mask a neural network implementation
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.APIs.Interfaces.INeuralNetwork.NetworkType">
            <summary>
            Gets the type of a given <see cref="T:NeuralNetworkNET.APIs.Interfaces.INeuralNetwork"/> instance
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.APIs.Interfaces.INeuralNetwork.InputInfo">
            <summary>
            Gets the size of the input layer
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.APIs.Interfaces.INeuralNetwork.OutputInfo">
            <summary>
            Gets the size of the output layer
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.APIs.Interfaces.INeuralNetwork.Layers">
            <summary>
            Gets the list of layers in the network
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.APIs.Interfaces.INeuralNetwork.Size">
            <summary>
            Gets the number of units in the current network (layers or graph nodes)
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.APIs.Interfaces.INeuralNetwork.Parameters">
            <summary>
            Gets the total number of parameters in the current network layer
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.APIs.Interfaces.INeuralNetwork.IsInNumericOverflow">
            <summary>
            Gets whether or not a numeric overflow has occurred in the network
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Interfaces.INeuralNetwork.Forward(System.Single[])">
            <summary>
            Forwards the input through the network
            </summary>
            <param name="x">The input to process</param>
            <remarks>This methods processes a single input row and outputs a single result</remarks>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Interfaces.INeuralNetwork.Forward(System.Single[0:,0:])">
            <summary>
            Forwards the inputs through the network
            </summary>
            <param name="x">The input to process</param>
            <remarks>This methods processes multiple inputs at the same time, one per input row</remarks>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Interfaces.INeuralNetwork.ExtractDeepFeatures(System.Single[])">
            <summary>
            Forwards the input through the network and returns a list of all the activity and activations computed by each layer
            </summary>
            <param name="x">The input to process</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Interfaces.INeuralNetwork.ExtractDeepFeatures(System.Single[0:,0:])">
            <summary>
            Forwards the inputs through the network and returns a list of all the activity and activations computed by each layer
            </summary>
            <param name="x">The input to process</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Interfaces.INeuralNetwork.CalculateCost(System.Single[],System.Single[])">
            <summary>
            Calculates the cost function for the current instance and the input values
            </summary>
            <param name="x">The input values for the network</param>
            <param name="y">The expected result to use to calculate the error</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Interfaces.INeuralNetwork.CalculateCost(System.Single[0:,0:],System.Single[0:,0:])">
            <summary>
            Calculates the cost function for the current instance and the input values
            </summary>
            <param name="x">The input values for the network</param>
            <param name="y">The expected result to use to calculate the error</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Interfaces.INeuralNetwork.Evaluate(NeuralNetworkNET.APIs.Interfaces.Data.IDataset)">
            <summary>
            Calculates the current network performances with the given dataset
            </summary>
            <param name="dataset">The <see cref="T:NeuralNetworkNET.APIs.Interfaces.Data.IDataset"/> instance to evaluate</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Interfaces.INeuralNetwork.SerializeMetadataAsJson">
            <summary>
            Serializes the network metadata as a JSON string
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Interfaces.INeuralNetwork.Save(System.IO.FileInfo)">
            <summary>
            Saves the network to the target file
            </summary>
            <param name="target">The <see cref="T:System.IO.FileInfo"/> instance for the target file (it may not exist yet)</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Interfaces.INeuralNetwork.Save(System.IO.Stream)">
            <summary>
            Saves the network to the target stream
            </summary>
            <param name="stream">The <see cref="T:System.IO.Stream"/> to use to write the network data</param>
        </member>
        <member name="T:NeuralNetworkNET.APIs.Interfaces.ITrainingAlgorithmInfo">
            <summary>
            A common interface for all the available training algorithms in the library
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.APIs.Interfaces.ITrainingAlgorithmInfo.AlgorithmType">
            <summary>
            Gets the type of training algorithm for the current instance
            </summary>
        </member>
        <member name="T:NeuralNetworkNET.APIs.Interfaces.IWeightedLayer">
            <summary>
            
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.APIs.Interfaces.IWeightedLayer.Biases">
            <summary>
            
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.APIs.Interfaces.IWeightedLayer.Weights">
            <summary>
            
            </summary>
        </member>
        <member name="T:NeuralNetworkNET.APIs.NetworkLayers">
            <summary>
            A static class that exposes the available network layer types
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.APIs.NetworkLayers.FullyConnected(System.Int32,NeuralNetworkNET.APIs.Enums.ActivationType,NeuralNetworkNET.APIs.Enums.WeightsInitializationMode,NeuralNetworkNET.APIs.Enums.BiasInitializationMode)">
            <summary>
            Creates a new fully connected layer with the specified number of output neurons, and the given activation function
            </summary>
            <param name="neurons">The number of output neurons</param>
            <param name="activation">The desired activation function to use in the network layer</param>
            <param name="weightsMode">The desired initialization mode for the weights in the network layer</param>
            <param name="biasMode">The desired initialization mode to use for the layer bias values</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.NetworkLayers.FullyConnected(System.Int32,NeuralNetworkNET.APIs.Enums.ActivationType,NeuralNetworkNET.Networks.Cost.CostFunctionType,NeuralNetworkNET.APIs.Enums.WeightsInitializationMode,NeuralNetworkNET.APIs.Enums.BiasInitializationMode)">
            <summary>
            Creates an output fully connected layer, with the specified cost function to use
            </summary>
            <param name="neurons">The number of output neurons</param>
            <param name="activation">The desired activation function to use in the network layer</param>
            <param name="cost">The cost function that should be used by the output layer</param>
            <param name="weightsMode">The desired initialization mode for the weights in the network layer</param>
            <param name="biasMode">The desired initialization mode to use for the layer bias values</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.NetworkLayers.Softmax(System.Int32,NeuralNetworkNET.APIs.Enums.WeightsInitializationMode,NeuralNetworkNET.APIs.Enums.BiasInitializationMode)">
            <summary>
            Creates a fully connected softmax output layer (used for classification problems with mutually-exclusive classes)
            </summary>
            <param name="outputs">The number of output neurons</param>
            <param name="weightsMode">The desired initialization mode for the weights in the network layer</param>
            <param name="biasMode">The desired initialization mode to use for the layer bias values</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.NetworkLayers.Convolutional(System.ValueTuple{System.Int32,System.Int32},System.Int32,NeuralNetworkNET.APIs.Enums.ActivationType,NeuralNetworkNET.APIs.Enums.BiasInitializationMode)">
            <summary>
            Creates a convolutional layer with the desired number of kernels
            </summary>
            <param name="kernel">The volume information of the kernels used in the layer</param>
            <param name="kernels">The number of convolution kernels to apply to the input volume</param>
            <param name="activation">The desired activation function to use in the network layer</param>
            <param name="biasMode">Indicates the desired initialization mode to use for the layer bias values</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.NetworkLayers.Pooling(NeuralNetworkNET.APIs.Enums.ActivationType)">
            <summary>
            Creates a pooling layer with a window of size 2 and a stride of 2
            </summary>
            <param name="activation">The desired activation function to use in the network layer</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.NetworkLayers.BatchNormalization(NeuralNetworkNET.APIs.Enums.NormalizationMode,NeuralNetworkNET.APIs.Enums.ActivationType)">
            <summary>
            Creates a new batch normalization layer
            </summary>
            <param name="mode">The normalization mode to use for the new layer</param>
            <param name="activation">The desired activation function to use in the network layer</param>
        </member>
        <member name="T:NeuralNetworkNET.APIs.NetworkLoader">
            <summary>
            A static class that handles the JSON deserialization for the neural networks
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.NetworkLoader.NetworkFileExtension">
            <summary>
            Gets the file extension used when saving a network
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.APIs.NetworkLoader.TryLoad(System.IO.FileInfo,NeuralNetworkNET.APIs.Enums.ExecutionModePreference)">
            <summary>
            Tries to deserialize a network from the input file
            </summary>
            <param name="file">The <see cref="T:System.IO.FileInfo"/> instance for the file to load</param>
            <param name="preference">The layers deserialization preference</param>
            <returns>The deserialized network, or <see langword="null"/> if the operation fails</returns>
        </member>
        <member name="M:NeuralNetworkNET.APIs.NetworkLoader.TryLoad(System.IO.Stream,NeuralNetworkNET.APIs.Enums.ExecutionModePreference)">
            <summary>
            Tries to deserialize a network from the input <see cref="T:System.IO.Stream"/>
            </summary>
            <param name="stream">The <see cref="T:System.IO.Stream"/> instance for the network to load</param>
            <param name="preference">The layers deserialization preference</param>
            <returns>The deserialized network, or <see langword="null"/> if the operation fails</returns>
        </member>
        <member name="M:NeuralNetworkNET.APIs.NetworkLoader.CpuLayerDeserialize(System.IO.Stream,NeuralNetworkNET.APIs.Enums.LayerType)">
            <summary>
            Tries to deserialize a CPU-powered network layer
            </summary>
            <param name="stream">The source <see cref="T:System.IO.Stream"/></param>
            <param name="type">The target network layer type</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.NetworkLoader.CuDnnLayerDeserialize(System.IO.Stream,NeuralNetworkNET.APIs.Enums.LayerType)">
            <summary>
            Tries to deserialize a Cuda-powered network layer
            </summary>
            <param name="stream">The source <see cref="T:System.IO.Stream"/></param>
            <param name="type">The target network layer type</param>
        </member>
        <member name="T:NeuralNetworkNET.APIs.NetworkManager">
            <summary>
            A static class that create and trains a neural network for the input data and expected results
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.APIs.NetworkManager.NewSequential(NeuralNetworkNET.APIs.Structs.TensorInfo,NeuralNetworkNET.APIs.Delegates.LayerFactory[])">
            <summary>
            Creates a new network with a linear structure and the specified parameters
            </summary>
            <param name="input">The input <see cref="T:NeuralNetworkNET.APIs.Structs.TensorInfo"/> description</param>
            <param name="factories">A list of factories to create the different layers in the new network</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.NetworkManager.NewGraph(NeuralNetworkNET.APIs.Structs.TensorInfo,System.Action{NeuralNetworkNET.Networks.Graph.NodeBuilder})">
            <summary>
            Creates a computational graph network with a custom structure
            </summary>
            <param name="input">The input <see cref="T:NeuralNetworkNET.APIs.Structs.TensorInfo"/> description</param>
            <param name="builder">An <see cref="T:System.Action`1"/> used to build the graph from the input <see cref="T:NeuralNetworkNET.Networks.Graph.NodeBuilder"/> node</param>
        </member>
        <member name="P:NeuralNetworkNET.APIs.NetworkManager.TrainingInProgress">
            <summary>
            Gets whether or not a neural network is currently being trained
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.APIs.NetworkManager.TrainNetwork(NeuralNetworkNET.APIs.Interfaces.INeuralNetwork,NeuralNetworkNET.APIs.Interfaces.Data.ITrainingDataset,NeuralNetworkNET.APIs.Interfaces.ITrainingAlgorithmInfo,System.Int32,System.Single,System.Action{NeuralNetworkNET.SupervisedLearning.Progress.BatchProgress},System.Action{NeuralNetworkNET.SupervisedLearning.Progress.TrainingProgressEventArgs},NeuralNetworkNET.APIs.Interfaces.Data.IValidationDataset,NeuralNetworkNET.APIs.Interfaces.Data.ITestDataset,System.Threading.CancellationToken)">
            <summary>
            Trains a neural network with the given parameters
            </summary>
            <param name="network">The existing <see cref="T:NeuralNetworkNET.APIs.Interfaces.INeuralNetwork"/> to train with the given dataset(s)</param>
            <param name="dataset">The <see cref="T:NeuralNetworkNET.APIs.Interfaces.Data.ITrainingDataset"/> instance to use to train the network</param>
            <param name="algorithm">The desired training algorithm to use</param>
            <param name="epochs">The number of epochs to run with the training data</param>
            <param name="dropout">Indicates the dropout probability for neurons in a <see cref="F:NeuralNetworkNET.APIs.Enums.LayerType.FullyConnected"/> layer</param>
            <param name="batchCallback">An optional callback to monitor the training progress (in terms of dataset completion)</param>
            <param name="trainingCallback">An optional progress callback to monitor progress on the training dataset (in terms of classification performance)</param>
            <param name="validationDataset">An optional dataset used to check for convergence and avoid overfitting</param>
            <param name="testDataset">The optional test dataset to use to monitor the current generalized training progress</param>       
            <param name="token">The <see cref="T:System.Threading.CancellationToken"/> for the training session</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.NetworkManager.TrainNetworkAsync(NeuralNetworkNET.APIs.Interfaces.INeuralNetwork,NeuralNetworkNET.APIs.Interfaces.Data.ITrainingDataset,NeuralNetworkNET.APIs.Interfaces.ITrainingAlgorithmInfo,System.Int32,System.Single,System.Action{NeuralNetworkNET.SupervisedLearning.Progress.BatchProgress},System.Action{NeuralNetworkNET.SupervisedLearning.Progress.TrainingProgressEventArgs},NeuralNetworkNET.APIs.Interfaces.Data.IValidationDataset,NeuralNetworkNET.APIs.Interfaces.Data.ITestDataset,System.Threading.CancellationToken)">
            <summary>
            Trains a neural network with the given parameters
            </summary>
            <param name="network">The existing <see cref="T:NeuralNetworkNET.APIs.Interfaces.INeuralNetwork"/> to train with the given dataset(s)</param>
            <param name="dataset">The <see cref="T:NeuralNetworkNET.APIs.Interfaces.Data.ITrainingDataset"/> instance to use to train the network</param>
            <param name="algorithm">The desired training algorithm to use</param>
            <param name="epochs">The number of epochs to run with the training data</param>
            <param name="dropout">Indicates the dropout probability for neurons in a <see cref="F:NeuralNetworkNET.APIs.Enums.LayerType.FullyConnected"/> layer</param>
            <param name="batchCallback">An optional callback to monitor the training progress (in terms of dataset completion)</param>
            <param name="trainingCallback">An optional progress callback to monitor progress on the training dataset (in terms of classification performance)</param>
            <param name="validationDataset">An optional dataset used to check for convergence and avoid overfitting</param>
            <param name="testDataset">The optional test dataset to use to monitor the current generalized training progress</param>       
            <param name="token">The <see cref="T:System.Threading.CancellationToken"/> for the training session</param>
        </member>
        <member name="T:NeuralNetworkNET.APIs.Results.DatasetEvaluationResult">
            <summary>
            A struct that contains information on a single completed evaluation of a test dataset
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.APIs.Results.DatasetEvaluationResult.Cost">
            <summary>
            Gets the total cost for the evaluated dataset
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.APIs.Results.DatasetEvaluationResult.Accuracy">
            <summary>
            Gets the classification accuracy for the evaluated dataset
            </summary>
        </member>
        <member name="T:NeuralNetworkNET.APIs.Results.TrainingSessionResult">
            <summary>
            A class that contains all the information on a completed training session
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.APIs.Results.TrainingSessionResult.StopReason">
            <summary>
            Gets the result for the training session
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.APIs.Results.TrainingSessionResult.CompletedEpochs">
            <summary>
            Gets the number of completed training epochs
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.APIs.Results.TrainingSessionResult.TrainingTime">
            <summary>
            Gets the approximate training time for the current session
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.APIs.Results.TrainingSessionResult.ValidationReports">
            <summary>
            Gets the evaluation reports for the validation dataset, if provided
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.APIs.Results.TrainingSessionResult.TestReports">
            <summary>
            Gets the evaluation reports for the test set, if provided
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Results.TrainingSessionResult.SerializeAsJson">
            <summary>
            Serializes the current instance as a JSON string with all the current training info
            </summary>
        </member>
        <member name="T:NeuralNetworkNET.APIs.Settings.AccuracyTesters">
            <summary>
            A static class exposing different <see cref="T:NeuralNetworkNET.APIs.Delegates.AccuracyTester"/> options to monitor the accuracy of a neural network
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Settings.AccuracyTesters.Argmax">
            <summary>
            Gets an <see cref="T:NeuralNetworkNET.APIs.Delegates.AccuracyTester"/> <see langword="delegate"/> that can be used classification problems with mutually-exclusive classes
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Settings.AccuracyTesters.Threshold(System.Single)">
            <summary>
            Gets an <see cref="T:NeuralNetworkNET.APIs.Delegates.AccuracyTester"/> <see langword="delegate"/> that checks if all the output values match the expected threshold
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Settings.AccuracyTesters.Distance(System.Single)">
            <summary>
            Gets an <see cref="T:NeuralNetworkNET.APIs.Delegates.AccuracyTester"/> <see langword="delegate"/> that checks if all the output values are within a specific distance from the expected values
            </summary>
        </member>
        <member name="T:NeuralNetworkNET.APIs.Settings.NetworkSettings">
            <summary>
            A static class with some shared settings for the library
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.APIs.Settings.NetworkSettings.MaximumBatchSize">
            <summary>
            Gets or sets the maximum batch size (used to optimize the memory usage during validation/test processing)
            </summary>
            <remarks>Adjust this setting to the highest possible value according to the available RAM/VRAM and the size of the dataset. If the validation/test dataset has more
            samples than <see cref="P:NeuralNetworkNET.APIs.Settings.NetworkSettings.MaximumBatchSize"/>, it will be automatically divided into batches so that it won't cause an <see cref="T:System.OutOfMemoryException"/> or other problems</remarks>
        </member>
        <member name="P:NeuralNetworkNET.APIs.Settings.NetworkSettings.AccuracyTester">
            <summary>
            Gets or sets the <see cref="T:NeuralNetworkNET.APIs.Delegates.AccuracyTester"/> instance to use to test a network being trained. The default value is <see cref="M:NeuralNetworkNET.APIs.Settings.AccuracyTesters.Argmax"/>.
            </summary>
        </member>
        <member name="T:NeuralNetworkNET.APIs.Structs.ConvolutionInfo">
            <summary>
            A <see langword="struct"/> containing all the info on a convolution operation
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Structs.ConvolutionInfo.Mode">
            <summary>
            The current convolution mode for the layer
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Structs.ConvolutionInfo.VerticalPadding">
            <summary>
            The optional vertical padding for the convolution operation
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Structs.ConvolutionInfo.HorizontalPadding">
            <summary>
            The optional horizontal padding for the convolution operation
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Structs.ConvolutionInfo.VerticalStride">
            <summary>
            The vertical stride length while sliding the receptive window over the input
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Structs.ConvolutionInfo.HorizontalStride">
            <summary>
            The horizontal stride length while sliding the receptive window over the input
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.APIs.Structs.ConvolutionInfo.Default">
            <summary>
            Gets the default convolution info, with no padding and a stride of 1 in both directions
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.APIs.Structs.ConvolutionInfo.CrossCorrelation">
            <summary>
            Gets the default cross correlation mode, with no padding and a stride of 1 in both directions
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Structs.ConvolutionInfo.New(NeuralNetworkNET.APIs.Enums.ConvolutionMode,System.Int32,System.Int32,System.Int32,System.Int32)">
            <summary>
            Creates a new convolution operation description with the input parameters
            </summary>
            <param name="mode">The desired convolution mode to use</param>
            <param name="verticalPadding">The optional convolution vertical padding</param>
            <param name="horizontalPadding">The optional convolution horizontal padding</param>
            <param name="verticalStride">The convolution vertical stride size</param>
            <param name="horizontalStride">The convolution horizontal stride size</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Structs.ConvolutionInfo.Same(NeuralNetworkNET.APIs.Enums.ConvolutionMode,System.Int32,System.Int32)">
            <summary>
            Creates a new <see cref="T:NeuralNetworkNET.APIs.Delegates.ConvolutionInfoFactory"/> instance that returns a <see cref="T:NeuralNetworkNET.APIs.Structs.ConvolutionInfo"/> value
            with the appropriate padding to keep the input size the same after the specified convolution operation
            </summary>
            <param name="mode">The desired convolution mode to use</param>
            <param name="verticalStride">The convolution vertical stride size</param>
            <param name="horizontalStride">The convolution horizontal stride size</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Structs.ConvolutionInfo.GetForwardOutputTensorInfo(NeuralNetworkNET.APIs.Structs.TensorInfo@,System.ValueTuple{System.Int32,System.Int32},System.Int32)">
            <summary>
            Calculates the output size after applying a convolution operation to the input tensor
            </summary>
            <param name="input">The info on the input tensor</param>
            <param name="field">The size of the convolution kernels</param>
            <param name="kernels">The number of convolution kernels to be used</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Structs.ConvolutionInfo.Equals(NeuralNetworkNET.APIs.Structs.ConvolutionInfo)">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Structs.ConvolutionInfo.Equals(System.Object)">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Structs.ConvolutionInfo.GetHashCode">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Structs.ConvolutionInfo.op_Equality(NeuralNetworkNET.APIs.Structs.ConvolutionInfo@,NeuralNetworkNET.APIs.Structs.ConvolutionInfo@)">
            <summary>
            Checks whether or not two <see cref="T:NeuralNetworkNET.APIs.Structs.ConvolutionInfo"/> instances have the same parameters
            </summary>
            <param name="a">The first instance</param>
            <param name="b">The second instance</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Structs.ConvolutionInfo.op_Inequality(NeuralNetworkNET.APIs.Structs.ConvolutionInfo@,NeuralNetworkNET.APIs.Structs.ConvolutionInfo@)">
            <summary>
            Checks whether or not two <see cref="T:NeuralNetworkNET.APIs.Structs.ConvolutionInfo"/> instances have different parameters
            </summary>
            <param name="a">The first instance</param>
            <param name="b">The second instance</param>
        </member>
        <member name="T:NeuralNetworkNET.APIs.Structs.DatasetSample">
            <summary>
            A stack-only struct containing two <see cref="T:System.Span`1"/> instances pointing to a single dataset sample
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.APIs.Structs.DatasetSample.X">
            <summary>
            Gets the <see cref="T:System.Span`1"/> referencing the current sample inputs
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.APIs.Structs.DatasetSample.Y">
            <summary>
            Gets the <see cref="T:System.Span`1"/> referencing the current sample expected outputs
            </summary>
        </member>
        <member name="T:NeuralNetworkNET.APIs.Structs.InceptionInfo">
            <summary>
            A <see langword="struct"/> containing all the info on an inception module
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Structs.InceptionInfo.Primary1x1ConvolutionKernels">
            <summary>
            The number of 1x1 convolution kernels used in the first step of the forward pass
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Structs.InceptionInfo.Primary3x3Reduce1x1ConvolutionKernels">
            <summary>
            The number of 1x1 convolution kernels before the 3x3 convolution
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Structs.InceptionInfo.Secondary3x3ConvolutionKernels">
            <summary>
            The number of 3x3 convolution kernels
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Structs.InceptionInfo.Primary5x5Reduce1x1ConvolutionKernels">
            <summary>
            The number of 1x1 convolution kernels before the 5x5 convolution
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Structs.InceptionInfo.Secondary5x5ConvolutionKernels">
            <summary>
            The number of 5x5 convolution kernels
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Structs.InceptionInfo.Pooling">
            <summary>
            The kind of pooling operation performed on the layer
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Structs.InceptionInfo.Secondary1x1AfterPoolingConvolutionKernels">
            <summary>
            The number of 1x1 convolution kernels after the pooling operation
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.APIs.Structs.InceptionInfo.OutputChannels">
            <summary>
            Gets the number of output channels after the depth concatenation
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.APIs.Structs.InceptionInfo.ConvolutionKernels">
            <summary>
            Gets the total number of convolution kernels for the current instance
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Structs.InceptionInfo.New(System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,NeuralNetworkNET.APIs.Enums.PoolingMode,System.Int32)">
            <summary>
            Creates a new inception layer description with the input parameters
            </summary>
            <param name="_1x1Kernels">The number of 1x1 primary convolution kernels</param>
            <param name="_3x3Reduce1x1Kernels">The number of 3x3 reduction 1x1 kernels</param>
            <param name="_3x3Kernels">The number of 3x3 convolution kernels</param>
            <param name="_5x5Reduce1x1Kernels">The number of 5x5 reduction 1x1 kernels</param>
            <param name="_5x5Kernels">The number of 5x5 convolution kernels</param>
            <param name="poolingMode">The pooling mode for the pooling pipeline</param>
            <param name="_1x1SecondaryKernels">The number of secondary 1x1 convolution kernels</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Structs.InceptionInfo.Equals(NeuralNetworkNET.APIs.Structs.InceptionInfo)">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Structs.InceptionInfo.Equals(System.Object)">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Structs.InceptionInfo.GetHashCode">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Structs.InceptionInfo.op_Equality(NeuralNetworkNET.APIs.Structs.InceptionInfo@,NeuralNetworkNET.APIs.Structs.InceptionInfo@)">
            <summary>
            Checks whether or not two <see cref="T:NeuralNetworkNET.APIs.Structs.InceptionInfo"/> instances have the same parameters
            </summary>
            <param name="a">The first instance</param>
            <param name="b">The second instance</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Structs.InceptionInfo.op_Inequality(NeuralNetworkNET.APIs.Structs.InceptionInfo@,NeuralNetworkNET.APIs.Structs.InceptionInfo@)">
            <summary>
            Checks whether or not two <see cref="T:NeuralNetworkNET.APIs.Structs.InceptionInfo"/> instances have different parameters
            </summary>
            <param name="a">The first instance</param>
            <param name="b">The second instance</param>
        </member>
        <member name="T:NeuralNetworkNET.APIs.Structs.PoolingInfo">
            <summary>
            A <see langword="struct"/> containing all the info on a pooling operation
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Structs.PoolingInfo.Mode">
            <summary>
            The current pooling mode for the layer
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Structs.PoolingInfo.WindowHeight">
            <summary>
            The height of each input local receptive field
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Structs.PoolingInfo.WindowWidth">
            <summary>
            The width of each input local receptive field
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Structs.PoolingInfo.VerticalPadding">
            <summary>
            The optional vertical padding for the pooling operation
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Structs.PoolingInfo.HorizontalPadding">
            <summary>
            The optional horizontal padding for the pooling operation
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Structs.PoolingInfo.VerticalStride">
            <summary>
            The vertical stride length while sliding the receptive window over the input
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Structs.PoolingInfo.HorizontalStride">
            <summary>
            The horizontal stride length while sliding the receptive window over the input
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.APIs.Structs.PoolingInfo.Default">
            <summary>
            Gets the default 2x2 max pooling info, with a stride of 2 in both directions and no padding
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Structs.PoolingInfo.New(NeuralNetworkNET.APIs.Enums.PoolingMode,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32)">
            <summary>
            Creates a new pooling operation description with the input parameters
            </summary>
            <param name="mode">The desired pooling mode to use</param>
            <param name="windowHeight">The pooling window height to use</param>
            <param name="windowWidth">The pooling window width to use</param>
            <param name="verticalPadding">The optional pooling vertical padding</param>
            <param name="horizontalPadding">The optional pooling horizontal padding</param>
            <param name="verticalStride">The pooling vertical stride size</param>
            <param name="horizontalStride">The pooling horizontal stride size</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Structs.PoolingInfo.GetForwardOutputTensorInfo(NeuralNetworkNET.APIs.Structs.TensorInfo@)">
            <summary>
            Calculates the output size after applying a pooling operation to the input tensor
            </summary>
            <param name="input">The info on the input tensor</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Structs.PoolingInfo.Equals(NeuralNetworkNET.APIs.Structs.PoolingInfo)">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Structs.PoolingInfo.Equals(System.Object)">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Structs.PoolingInfo.GetHashCode">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Structs.PoolingInfo.op_Equality(NeuralNetworkNET.APIs.Structs.PoolingInfo@,NeuralNetworkNET.APIs.Structs.PoolingInfo@)">
            <summary>
            Checks whether or not two <see cref="T:NeuralNetworkNET.APIs.Structs.PoolingInfo"/> instances have the same parameters
            </summary>
            <param name="a">The first instance</param>
            <param name="b">The second instance</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Structs.PoolingInfo.op_Inequality(NeuralNetworkNET.APIs.Structs.PoolingInfo@,NeuralNetworkNET.APIs.Structs.PoolingInfo@)">
            <summary>
            Checks whether or not two <see cref="T:NeuralNetworkNET.APIs.Structs.PoolingInfo"/> instances have different parameters
            </summary>
            <param name="a">The first instance</param>
            <param name="b">The second instance</param>
        </member>
        <member name="T:NeuralNetworkNET.APIs.Structs.Tensor">
            <summary>
            A readonly struct that holds the info on an unmanaged memory area that has been allocated
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Structs.Tensor.Ptr">
            <summary>
            The <see cref="T:System.IntPtr"/> value to the allocated memory
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Structs.Tensor.Entities">
            <summary>
            The number of entities (rows) in the current <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/>
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Structs.Tensor.Length">
            <summary>
            The size of each entity in the current <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/>
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.APIs.Structs.Tensor.Size">
            <summary>
            The total size (the number of <see cref="T:System.Single"/> values) in the current <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/>
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.APIs.Structs.Tensor.IsNull">
            <summary>
            Gets whether or not the current instance is linked to an allocated memory area
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.APIs.Structs.Tensor.Ref">
            <summary>
            Gets a managed reference for the current <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> data
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.APIs.Structs.Tensor.Item(System.Int32)">
            <summary>
            Gets or sets the <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> value for the specific index
            </summary>
            <param name="i">The target index to read or write</param>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Structs.Tensor.Null">
            <summary>
            Gets a <see langword="null"/> instance
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Structs.Tensor.New(System.Int32,System.Int32,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <summary>
            Creates a new instance with the specified shape
            </summary>
            <param name="n">The height of the <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/></param>
            <param name="chw">The width of the <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/></param>
            <param name="tensor">The resulting instance</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Structs.Tensor.NewZeroed(System.Int32,System.Int32,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <summary>
            Creates a new instance with the specified shape and initializes the allocated memory to 0s
            </summary>
            <param name="n">The height of the <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/></param>
            <param name="chw">The width of the <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/></param>
            <param name="tensor">The resulting instance</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Structs.Tensor.Reshape(System.Single*,System.Int32,System.Int32,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <summary>
            Creates a new instance by wrapping the input pointer
            </summary>
            <param name="p">The target memory area</param>
            <param name="n">The height of the final <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/></param>
            <param name="chw">The width of the final <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/></param>
            <param name="tensor">The resulting instance</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Structs.Tensor.Like(NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <summary>
            Creates a new instance with the same shape as the input <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/>
            </summary>
            <param name="mask">The <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> to use to copy the shape</param>
            <param name="tensor">The output <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/></param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Structs.Tensor.LikeZeroed(NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <summary>
            Creates a new instance with the same shape as the input <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> and all the values initializes to 0
            </summary>
            <param name="mask">The <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> to use to copy the shape</param>
            <param name="tensor">The output <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/></param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Structs.Tensor.From(System.Single*,System.Int32,System.Int32,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <summary>
            Creates a new instance by copying the contents at the given memory location and reshaping it to the desired size
            </summary>
            <param name="p">The target memory area to copy</param>
            <param name="n">The height of the final <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/></param>
            <param name="chw">The width of the final <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/></param>
            <param name="tensor">The resulting instance</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Structs.Tensor.From(System.Single[0:,0:],NeuralNetworkNET.APIs.Structs.Tensor@)">
            <summary>
            Creates a new instance by copying the contents of the input matrix
            </summary>
            <param name="m">The input matrix to copy</param>
            <param name="tensor">The resulting instance</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Structs.Tensor.From(System.Single[],System.Int32,System.Int32,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <summary>
            Creates a new instance by copying the contents of the input vector and reshaping it to the desired size
            </summary>
            <param name="v">The input vector to copy</param>
            <param name="n">The height of the final <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/></param>
            <param name="chw">The width of the final <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/></param>
            <param name="tensor">The resulting instance</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Structs.Tensor.Reshape(System.Int32,System.Int32,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <summary>
            Creates a new instance by wrapping the current memory area
            </summary>
            <param name="n">The height of the final <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/></param>
            <param name="chw">The width of the final <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/></param>
            <param name="tensor">The resulting instance</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Structs.Tensor.MatchShape(NeuralNetworkNET.APIs.Structs.Tensor@)">
            <summary>
            Checks whether or not the current instance has the same shape of the input <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/>
            </summary>
            <param name="tensor">The instance to compare</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Structs.Tensor.MatchShape(System.Int32,System.Int32)">
            <summary>
            Checks whether or not the current instance has the same shape as the input arguments
            </summary>
            <param name="entities">The expected number of entities</param>
            <param name="length">The expected length of each entity</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Structs.Tensor.Overwrite(NeuralNetworkNET.APIs.Structs.Tensor@)">
            <summary>
            Overwrites the contents of the current instance with the input <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/>
            </summary>
            <param name="tensor">The input <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> to copy</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Structs.Tensor.Overwrite(System.Single[])">
            <summary>
            Overwrites the contents of the current <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> with the input array
            </summary>
            <param name="array">The input array to copy</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Structs.Tensor.Duplicate(NeuralNetworkNET.APIs.Structs.Tensor@)">
            <summary>
            Duplicates the current instance to an output <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/>
            </summary>
            <param name="tensor">The output tensor</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Structs.Tensor.ToArray(System.Boolean)">
            <summary>
            Copies the contents of the unmanaged array to a managed <see cref="T:System.Array"/>
            </summary>
            <param name="keepAlive">Indicates whether or not to automatically dispose the current instance</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Structs.Tensor.ToArray2D(System.Boolean)">
            <summary>
            Copies the contents of the unmanaged array to a managed 2D <see cref="T:System.Array"/>
            </summary>
            <param name="keepAlive">Indicates whether or not to automatically dispose the current instance</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Structs.Tensor.AsSpan">
            <summary>
            Returns a <see cref="T:System.Span`1"/> representing the current <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/>
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Structs.Tensor.GetLastValues">
            <summary>
            Returns the last values from this Tensor
            </summary>
            <returns></returns>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Structs.Tensor.Free">
            <summary>
            Frees the memory associated with the current instance
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Structs.Tensor.TryFree">
            <summary>
            Frees the memory associated with the current instance, if needed
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Structs.Tensor.Free(NeuralNetworkNET.APIs.Structs.Tensor[])">
            <summary>
            Frees the input sequence of <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> instances
            </summary>
            <param name="tensors">The tensors to free</param>
            <remarks>The <see langword="params"/> usage in the method arguments will cause a heap allocation
            when this method is called. Manually calling <see cref="M:NeuralNetworkNET.APIs.Structs.Tensor.Free"/> on each target <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/>
            should have a slightly better performance. The same is true for the <see cref="M:NeuralNetworkNET.APIs.Structs.Tensor.TryFree(NeuralNetworkNET.APIs.Structs.Tensor[])"/> method as well.</remarks>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Structs.Tensor.TryFree(NeuralNetworkNET.APIs.Structs.Tensor[])">
            <summary>
            Frees the input sequence of <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> instances, if possible
            </summary>
            <param name="tensors">The tensors to free</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Structs.Tensor.op_Implicit(NeuralNetworkNET.APIs.Structs.Tensor@)~System.Single*">
            <summary>
            Gets a raw pointer to the <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> data
            </summary>
            <param name="tensor"></param>
        </member>
        <member name="T:NeuralNetworkNET.APIs.Structs.Tensor._TensorProxy">
            <summary>
            A proxy type to debug instances of the <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> <see langword="struct"/>
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.APIs.Structs.Tensor._TensorProxy.RowsPreview">
            <summary>
            Gets a preview of the underlying memory area wrapped by this instance
            </summary>
        </member>
        <member name="T:NeuralNetworkNET.APIs.Structs.TensorInfo">
            <summary>
            A <see langword="struct"/> that contains info on the size of a given tensor
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Structs.TensorInfo.Height">
            <summary>
            The height of each 2D slice
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Structs.TensorInfo.Width">
            <summary>
            The width of each 2D slice
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.APIs.Structs.TensorInfo.Channels">
            <summary>
            The number of channels for the tensor description
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.APIs.Structs.TensorInfo.Size">
            <summary>
            Gets the total number of entries in the data volume
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.APIs.Structs.TensorInfo.SliceSize">
            <summary>
            Gets the size of each 2D size
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.APIs.Structs.TensorInfo.IsEmptyOrInvalid">
            <summary>
            Gets whether the current <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> instance is invalid (empty or with invalid parameters)
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Structs.TensorInfo.Linear(System.Int32)">
            <summary>
            Creates a new <see cref="T:NeuralNetworkNET.APIs.Structs.TensorInfo"/> instance for a linear network layer, without keeping track of spatial info
            </summary>
            <param name="size">The input size</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Structs.TensorInfo.Image``1(System.Int32,System.Int32)">
            <summary>
            Creates a new <see cref="T:NeuralNetworkNET.APIs.Structs.TensorInfo"/> instance for an image with a user-defined pixel type
            </summary>
            <typeparam name="TPixel">The type of image pixels. It must be either <see cref="T:SixLabors.ImageSharp.PixelFormats.Alpha8"/>, <see cref="T:SixLabors.ImageSharp.PixelFormats.Rgb24"/> or <see cref="T:SixLabors.ImageSharp.PixelFormats.Argb32"/></typeparam>
            <param name="height">The height of the input image</param>
            <param name="width">The width of the input image</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Structs.TensorInfo.Volume(System.Int32,System.Int32,System.Int32)">
            <summary>
            Creates a new <see cref="T:NeuralNetworkNET.APIs.Structs.TensorInfo"/> instance for with a custom 3D shape
            </summary>
            <param name="height">The input volume height</param>
            <param name="width">The input volume width</param>
            <param name="channels">The number of channels in the input volume</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Structs.TensorInfo.Equals(NeuralNetworkNET.APIs.Structs.TensorInfo)">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Structs.TensorInfo.Equals(System.Object)">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Structs.TensorInfo.GetHashCode">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Structs.TensorInfo.op_Equality(NeuralNetworkNET.APIs.Structs.TensorInfo@,NeuralNetworkNET.APIs.Structs.TensorInfo@)">
            <summary>
            Checks whether or not two <see cref="T:NeuralNetworkNET.APIs.Structs.TensorInfo"/> instances have the same parameters
            </summary>
            <param name="a">The first instance</param>
            <param name="b">The second instance</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.Structs.TensorInfo.op_Inequality(NeuralNetworkNET.APIs.Structs.TensorInfo@,NeuralNetworkNET.APIs.Structs.TensorInfo@)">
            <summary>
            Checks whether or not two <see cref="T:NeuralNetworkNET.APIs.Structs.TensorInfo"/> instances have different parameters
            </summary>
            <param name="a">The first instance</param>
            <param name="b">The second instance</param>
        </member>
        <member name="T:NeuralNetworkNET.APIs.TrainingAlgorithms">
            <summary>
            A static class that produces info for different available training algorithms
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.APIs.TrainingAlgorithms.StochasticGradientDescent(System.Single,System.Single)">
            <summary>
            Gets an instance implementing <see cref="T:NeuralNetworkNET.APIs.Interfaces.ITrainingAlgorithmInfo"/> for the <see cref="F:NeuralNetworkNET.SupervisedLearning.Algorithms.TrainingAlgorithmType.StochasticGradientDescent"/> algorithm
            </summary>
            <param name="eta">The learning rate</param>
            <param name="lambda">The lambda regularization parameter</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.TrainingAlgorithms.Momentum(System.Single,System.Single,System.Single)">
            <summary>
            Gets an instance implementing <see cref="T:NeuralNetworkNET.APIs.Interfaces.ITrainingAlgorithmInfo"/> for the <see cref="F:NeuralNetworkNET.SupervisedLearning.Algorithms.TrainingAlgorithmType.Momentum"/> algorithm
            </summary>
            <param name="eta">The learning rate</param>
            <param name="lambda">The lambda regularization parameter</param>
            <param name="momentum">The momentum value</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.TrainingAlgorithms.AdaGrad(System.Single,System.Single,System.Single)">
            <summary>
            Gets an instance implementing <see cref="T:NeuralNetworkNET.APIs.Interfaces.ITrainingAlgorithmInfo"/> for the <see cref="F:NeuralNetworkNET.SupervisedLearning.Algorithms.TrainingAlgorithmType.AdaGrad"/> algorithm
            </summary>
            <param name="eta">The learning rate</param>
            <param name="lambda">The lambda regularization parameter</param>
            <param name="epsilon">The AdaGrad epsilon parameter</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.TrainingAlgorithms.AdaDelta(System.Single,System.Single,System.Single)">
            <summary>
            Gets an instance implementing <see cref="T:NeuralNetworkNET.APIs.Interfaces.ITrainingAlgorithmInfo"/> for the <see cref="F:NeuralNetworkNET.SupervisedLearning.Algorithms.TrainingAlgorithmType.AdaDelta"/> algorithm
            </summary>
            <param name="rho">The AdaDelta rho parameter</param>
            <param name="epsilon">The AdaDelta epsilon parameter</param>
            <param name="l2">An optional L2 regularization parameter</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.TrainingAlgorithms.Adam(System.Single,System.Single,System.Single,System.Single)">
            <summary>
            Gets an instance implementing <see cref="T:NeuralNetworkNET.APIs.Interfaces.ITrainingAlgorithmInfo"/> for the <see cref="F:NeuralNetworkNET.SupervisedLearning.Algorithms.TrainingAlgorithmType.Adam"/> algorithm
            </summary>
            <param name="eta">The learning rate factor</param>
            <param name="beta1">The beta1 factor for the first moment vector</param>
            <param name="beta2">The beta2 factor for the second moment vector</param>
            <param name="epsilon">The Adam epsilon parameter</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.TrainingAlgorithms.AdaMax(System.Single,System.Single,System.Single)">
            <summary>
            Gets an instance implementing <see cref="T:NeuralNetworkNET.APIs.Interfaces.ITrainingAlgorithmInfo"/> for the <see cref="F:NeuralNetworkNET.SupervisedLearning.Algorithms.TrainingAlgorithmType.AdaMax"/> algorithm
            </summary>
            <param name="eta">The learning rate factor</param>
            <param name="beta1">The beta1 factor for the first moment vector</param>
            <param name="beta2">The beta2 factor for the second moment vector</param>
        </member>
        <member name="M:NeuralNetworkNET.APIs.TrainingAlgorithms.RMSProp(System.Single,System.Single,System.Single,System.Single)">
            <summary>
            Gets an instance implementing <see cref="T:NeuralNetworkNET.APIs.Interfaces.ITrainingAlgorithmInfo"/> for the <see cref="F:NeuralNetworkNET.SupervisedLearning.Algorithms.TrainingAlgorithmType.RMSProp"/> algorithm
            </summary>
            <param name="eta">The learning rate</param>
            <param name="rho">The RMSProp rho parameter</param>
            <param name="lambda">The lambda regularization parameter</param>
            <param name="epsilon">The RMSProp epsilon parameter</param>
        </member>
        <member name="T:NeuralNetworkNET.cpuDNN.CpuBlas">
            <summary>
            A class that exposes static BLAS (Basic Linear Algebra Subprograms) methods working on <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> instances
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.cpuDNN.CpuBlas.Transpose(NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <summary>
            Transposes the input <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/>
            </summary>
            <param name="x">The <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> to transpose</param>
            <param name="y">The output <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/></param>
        </member>
        <member name="M:NeuralNetworkNET.cpuDNN.CpuBlas.Multiply(NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <summary>
            Performs the multiplication between two matrices
            </summary>
            <param name="x1">The first matrix to multiply</param>
            <param name="x2">The second matrix to multiply</param>
            <param name="y">The resulting matrix</param>
        </member>
        <member name="M:NeuralNetworkNET.cpuDNN.CpuBlas.MultiplyElementwise(NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <summary>
            Performs the elementwise multiplication (Hadamard product) product between two <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> instances
            </summary>
            <param name="x1">The first <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/></param>
            <param name="x2">The second <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/></param>
            <param name="y">The resulting <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/></param>
        </member>
        <member name="M:NeuralNetworkNET.cpuDNN.CpuBlas.Sum(System.Span{NeuralNetworkNET.APIs.Structs.Tensor},NeuralNetworkNET.APIs.Structs.Tensor@)">
            <summary>
            Sums a series of input <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> instances
            </summary>
            <param name="inputs">A <see cref="T:System.Span`1"/> containing the input <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> instances to sum</param>
            <param name="y">The output <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/></param>
        </member>
        <member name="M:NeuralNetworkNET.cpuDNN.CpuBlas.Subtract(NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <summary>
            Subtracts two <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> instances, element wise
            </summary>
            <param name="x1">The first <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/></param>
            <param name="x2">The second <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/></param>
            <param name="y">The resulting <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> - it can be the same as one of the inputs</param>
        </member>
        <member name="T:NeuralNetworkNET.cpuDNN.CpuDnn">
            <inheritdoc cref="T:NeuralNetworkNET.cpuDNN.CpuDnn"/>
            <summary>
            A static class that contains primitives to implement a CNN running on CPU
            </summary>
            <inheritdoc cref="T:NeuralNetworkNET.cpuDNN.CpuDnn"/>
        </member>
        <member name="M:NeuralNetworkNET.cpuDNN.CpuDnn.ConvolutionForward(NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.TensorInfo@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.TensorInfo@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <summary>
            Performs a forward convolution operation for a network layer
            </summary>
            <param name="x">The source <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/>, where each row is a sample in the dataset and each one contains a series of images in row-first order</param>
            <param name="xInfo">The source volume info (depth and 2D slices size)</param>
            <param name="w">The list of convolution kernels to apply to each image</param>
            <param name="wInfo">The kernels volume info (depth and 2D slices size)</param>
            <param name="b">The bias <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> to sum to the resulting images</param>
            <param name="y">The resulting convolution <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/></param>
            <exception cref="T:System.ArgumentException">The size of one of the input <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> instances isn't valid</exception>
        </member>
        <member name="M:NeuralNetworkNET.cpuDNN.CpuDnn.ConvolutionBackwardData(NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.TensorInfo@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.TensorInfo@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.TensorInfo@)">
            <summary>
            Performs the backwards pass on a convolutional layer
            </summary>
            <param name="dy">The output error <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/></param>
            <param name="dyInfo">The info on the output <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/></param>
            <param name="w">The layer convolution kernels</param>
            <param name="wInfo">The kernels volume info (depth and 2D slices size)</param>
            <param name="dx">The resulting backpropagated error <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/></param>
            <param name="dxInfo">The info on the layer inputs</param>
            <exception cref="T:System.ArgumentException">The size of one of the input <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> instances isn't valid</exception>
        </member>
        <member name="M:NeuralNetworkNET.cpuDNN.CpuDnn.ConvolutionBackwardFilter(NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.TensorInfo@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.TensorInfo@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.TensorInfo@)">
            <summary>
            Performs a the backward convolution operation for a network layer and computes the gradient with respect to the layer weights
            </summary>
            <param name="x">The source <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/>, where each row is a sample in the dataset and each one contains a series of images in row-first order</param>
            <param name="xInfo">The source volume info (depth and 2D slices size)</param>
            <param name="dy">The output error <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/></param>
            <param name="dyInfo">The output error volume info (depth and 2D slices size)</param>
            <param name="dw">The resulting weights gradient</param>
            <param name="wInfo">The info on the layer kernels</param>
            <exception cref="T:System.ArgumentException">The size of one of the input <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> instances isn't valid</exception>
        </member>
        <member name="M:NeuralNetworkNET.cpuDNN.CpuDnn.ConvolutionBackwardBias(NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.TensorInfo@,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <summary>
            Performs a the backward convolution operation for a network layer and computes the gradient with respect to the layer biases
            </summary>
            <param name="dy">The output error <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/></param>
            <param name="dyInfo">The info on the output <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/></param>
            <param name="db">The resulting gradient</param>
            <exception cref="T:System.ArgumentException">The size of one of the input <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> instances isn't valid</exception>
        </member>
        <member name="M:NeuralNetworkNET.cpuDNN.CpuDnn.Rotate180(NeuralNetworkNET.APIs.Structs.Tensor@,System.Int32,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <summary>
            Rotates the input volume by 180 degrees
            </summary>
            <param name="x">The input <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> to rotate</param>
            <param name="depth">The number of images per row</param>
            <param name="y">The rotated input <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/></param>
        </member>
        <member name="M:NeuralNetworkNET.cpuDNN.CpuDnn.CompressVertically(NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <summary>
            Compresses a <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> into a row by summing the components column by column
            </summary>
            <param name="x">The <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> to compress</param>
            <param name="y">The resulting <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/></param>
        </member>
        <member name="M:NeuralNetworkNET.cpuDNN.CpuDnn.ActivationForward(NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.Networks.Activations.Delegates.ActivationFunction,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <summary>
            Executes the input activation function on the target <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/>
            </summary>
            <param name="x">The layer input <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/></param>
            <param name="f">The activation function to apply to the input</param>
            <param name="y">The output <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> - it can be the same as the input</param>
        </member>
        <member name="M:NeuralNetworkNET.cpuDNN.CpuDnn.SoftmaxForward(NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <summary>
            Performs the softmax activation on the input <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> and applies the output normalization
            </summary>
            <param name="x">The input <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/></param>
            <param name="y">The output <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/></param>
        </member>
        <member name="M:NeuralNetworkNET.cpuDNN.CpuDnn.ActivationBackward(NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.Networks.Activations.Delegates.ActivationFunction,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <summary>
            Executes the backward activation function on the target <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/>, with the given error delta
            </summary>
            <param name="y">The activity computed in the forwaard pass</param>
            <param name="dy">The current error delta to backpropagate</param>
            <param name="f_">The derivative of the activation function used in the forward pass</param>
            <param name="dx">The resulting input error delta - it can be the same as the input <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/></param>
        </member>
        <member name="M:NeuralNetworkNET.cpuDNN.CpuDnn.FullyConnectedForward(NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <summary>
            Executes the forward pass on a fully connected layer
            </summary>
            <param name="x">The input <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> to process</param>
            <param name="w">The layer weights</param>
            <param name="b">The layer biases</param>
            <param name="y">The output <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> for the current layer</param>
        </member>
        <member name="M:NeuralNetworkNET.cpuDNN.CpuDnn.FullyConnectedBackwardData(NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <summary>
            Executes the backward pass on a fully connected layer
            </summary>
            <param name="w">The layer weights</param>
            <param name="dy">The output error delta</param>
            <param name="dx">The resulting input error delta</param>
        </member>
        <member name="M:NeuralNetworkNET.cpuDNN.CpuDnn.FullyConnectedBackwardFilter(NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <summary>
            Executes the backward pass on a fully connected layer to calculate the gradient with respect to the weights
            </summary>
            <param name="x">The layer inputs</param>
            <param name="dy">The layer output error delta</param>
            <param name="dw">The resulting weights gradient <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/></param>
        </member>
        <member name="M:NeuralNetworkNET.cpuDNN.CpuDnn.FullyConnectedBackwardBias(NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <summary>
            Executes the backward pass on a fully connected layer to calculate the gradient with respect to the biases
            </summary>
            <param name="dy">The layer output error delta</param>
            <param name="db">The resulting biases gradient <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/></param>
        </member>
        <member name="M:NeuralNetworkNET.cpuDNN.CpuDnn.DepthConcatenationForward(System.Span{NeuralNetworkNET.APIs.Structs.Tensor},NeuralNetworkNET.APIs.Structs.Tensor@)">
            <summary>
            Executes the forward pass on a depth stacking layer
            </summary>
            <param name="inputs">A <see cref="T:System.Span`1"/> containing the input <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> instances to stack</param>
            <param name="y">The output <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/></param>
        </member>
        <member name="M:NeuralNetworkNET.cpuDNN.CpuDnn.DepthConcatenationBackward(NeuralNetworkNET.APIs.Structs.Tensor@,System.Int32,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <summary>
            Executes the backward pass on a depth stacking layer
            </summary>
            <param name="dy">The input <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> with the error delta to backpropagate</param>
            <param name="offset">The left offset for the <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> instance to extract</param>
            <param name="dx">A <see cref="T:System.Span`1"/> with the target <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> instances</param>
        </member>
        <member name="F:NeuralNetworkNET.cpuDNN.CpuDnn.CUDNN_BN_MIN_EPSILON">
            <summary>
            Gets the minimum epsilon allowed to be used in batch normalization methods
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.cpuDNN.CpuDnn.BatchNormalizationForward(NeuralNetworkNET.APIs.Enums.NormalizationMode,NeuralNetworkNET.APIs.Structs.TensorInfo@,NeuralNetworkNET.APIs.Structs.Tensor@,System.Single,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <summary>
            Executes the forward pass in a batch normalization layer
            </summary>
            <param name="mode">The desired normalization mode to apply</param>
            <param name="info">The ifo on the input <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> to process</param>
            <param name="x">The input <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> to normalize</param>
            <param name="factor">The factor for the cumulative moving average</param>
            <param name="mu">A <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> to use to store the temporary median values (used for backpropagation too)</param>
            <param name="sigma2">A <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> to use to store the temporary standard deviation values (used for backpropagation too)</param>
            <param name="gamma">The layer gamma parameters</param>
            <param name="beta">The layer beta parameters</param>
            <param name="y">The output <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> for the current layer</param>
        </member>
        <member name="M:NeuralNetworkNET.cpuDNN.CpuDnn.BatchNormalizationForward(NeuralNetworkNET.APIs.Enums.NormalizationMode,NeuralNetworkNET.APIs.Structs.TensorInfo@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <summary>
            Executes the forward pass in a batch normalization layer in inference mode
            </summary>
            <param name="mode">The desired normalization mode to apply</param>
            <param name="info">The ifo on the input <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> to process</param>
            <param name="x">The input <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> to normalize</param>
            <param name="mu">A <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> to use to store the temporary median values (used for backpropagation too)</param>
            <param name="sigma2">A <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> to use to store the temporary standard deviation values (used for backpropagation too)</param>
            <param name="gamma">The layer gamma parameters</param>
            <param name="beta">The layer beta parameters</param>
            <param name="y">The output <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> for the current layer</param>
        </member>
        <member name="M:NeuralNetworkNET.cpuDNN.CpuDnn.BatchNormalizationBackwardData(NeuralNetworkNET.APIs.Enums.NormalizationMode,NeuralNetworkNET.APIs.Structs.TensorInfo@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <summary>
            Executes the backward pass through a batch normalization layer
            </summary>
            <param name="mode">The desired normalization mode to apply</param>
            <param name="info">The ifo on the input <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> to process</param>
            <param name="x">The input <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> to normalize</param>
            <param name="mu">A <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> with the temporary median values calculated in the forward pass</param>
            <param name="sigma2">A <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> with the temporary standard deviation values calculated in the forward pass</param>
            <param name="gamma">The layer gamma parameters</param>
            <param name="dy">The output error delta <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/></param>
            <param name="dx">The resulting backpropagated error delta <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/></param>
        </member>
        <member name="M:NeuralNetworkNET.cpuDNN.CpuDnn.BatchNormalizationBackwardGamma(NeuralNetworkNET.APIs.Enums.NormalizationMode,NeuralNetworkNET.APIs.Structs.TensorInfo@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <summary>
            Calculates the gradient with respect to the gamma <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> in a batch normalization layer
            </summary>
            <param name="mode">The desired normalization mode to apply</param>
            <param name="info">The ifo on the input <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> to process</param>
            <param name="x">The input <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> used in the forward pass</param>
            <param name="mu">A <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> with the temporary median values calculated in the forward pass</param>
            <param name="sigma2">A <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> with the temporary standard deviation values calculated in the forward pass</param>
            <param name="dy">The output <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> error delta for the current layer</param>
            <param name="dgamma">The resulting gamma gradient <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/></param>
        </member>
        <member name="M:NeuralNetworkNET.cpuDNN.CpuDnn.BatchNormalizationBackwardBeta(NeuralNetworkNET.APIs.Enums.NormalizationMode,NeuralNetworkNET.APIs.Structs.TensorInfo@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <summary>
            Calculates the gradient with respect to the beta <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> in a batch normalization layer
            </summary>
            <param name="mode">The desired normalization mode to apply</param>
            <param name="info">The ifo on the input <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> to process</param>
            <param name="dy">The output <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> error delta for the current layer</param>
            <param name="dbeta">The resulting beta gradient <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/></param>
        </member>
        <member name="M:NeuralNetworkNET.cpuDNN.CpuDnn.PoolingForward(NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.TensorInfo@,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <summary>
            Executes the forward pass on a max pooling layer with a 2x2 window and a stride of 2
            </summary>
            <param name="x">The input <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> to pool</param>
            <param name="xInfo">The info on the input <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/></param>
            <param name="y">The resulting pooled <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/></param>
            <exception cref="T:System.ArgumentException">The size of one of the input <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> instances isn't valid</exception>
        </member>
        <member name="M:NeuralNetworkNET.cpuDNN.CpuDnn.PoolingBackward(NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.TensorInfo@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <summary>
            Executes the backward pass on a max pooling layer with a 2x2 window and a stride of 2
            </summary>
            <param name="x">The original input <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> used during the forward pass</param>
            <param name="xInfo">The info on the input <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/></param>
            <param name="dy">The output error for the current layer</param>
            <param name="dx">The resulting backpropagated error</param>
            <exception cref="T:System.ArgumentException">The size of one of the input <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> instances isn't valid</exception>
        </member>
        <member name="T:NeuralNetworkNET.cuDNN.CuDnnExtensions">
            <summary>
            A static class with some extensions for the <see cref="T:Alea.cuDNN.Dnn"/> class
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.cuDNN.CuDnnExtensions.ActivationForward(Alea.cuDNN.Dnn,System.Int32,System.Int32,Alea.deviceptr{System.Single},Alea.deviceptr{System.Single},NeuralNetworkNET.Networks.Activations.Delegates.ActivationFunction)">
            <summary>
            Executes the input activation function on the target memory area. The input and output pointers can be the same, if needed
            </summary>
            <param name="dnn">The current <see cref="T:Alea.cuDNN.Dnn"/> instance being used</param>
            <param name="n">The number of samples in the input tensor</param>
            <param name="w">The size of each sample to process</param>
            <param name="x">A pointer to the input memory area</param>
            <param name="y">The output memory area</param>
            <param name="f">The activation function to use</param>
        </member>
        <member name="M:NeuralNetworkNET.cuDNN.CuDnnExtensions.ActivationBackward(Alea.cuDNN.Dnn,System.Int32,System.Int32,Alea.deviceptr{System.Single},Alea.deviceptr{System.Single},NeuralNetworkNET.Networks.Activations.Delegates.ActivationFunction,Alea.deviceptr{System.Single})">
            <summary>
            Executes the backward activation function on the target memory area, with the given error delta
            </summary>
            <param name="dnn">The current <see cref="T:Alea.cuDNN.Dnn"/> instance being used</param>
            <param name="n">The number of samples in the input tensor</param>
            <param name="w">The size of each sample to process</param>
            <param name="y">A pointer to the memory area with the forward pass outputs</param>
            <param name="dy">The delta memory area</param>
            <param name="f">The activation function to use</param>
            <param name="dx">The backpropagated error</param>
        </member>
        <member name="M:NeuralNetworkNET.cuDNN.CuDnnExtensions.FullyConnectedForward(Alea.cuDNN.Dnn,System.Int32,System.Int32,System.Int32,Alea.deviceptr{System.Single},Alea.deviceptr{System.Single},Alea.deviceptr{System.Single},Alea.deviceptr{System.Single})">
            <summary>
            Executes the forward pass on a fully connected layer
            </summary>
            <param name="dnn">The current <see cref="T:Alea.cuDNN.Dnn"/> instance being used</param>
            <param name="n">The number of samples in the input tensor</param>
            <param name="l">The size of each input sample to process</param>
            <param name="k">The number of output features for each sample</param>
            <param name="x">A pointer to the input memory area</param>
            <param name="w">A pointer to the layer weights</param>
            <param name="b">A pointer to the network biases</param>
            <param name="y">A pointer to the output memory area</param>
        </member>
        <member name="M:NeuralNetworkNET.cuDNN.CuDnnExtensions.FullyConnectedBackwardData(Alea.cuDNN.Dnn,System.Int32,System.Int32,System.Int32,Alea.deviceptr{System.Single},Alea.deviceptr{System.Single},Alea.deviceptr{System.Single})">
            <summary>
            Executes the backward pass on a fully connected layer
            </summary>
            <param name="dnn">The current <see cref="T:Alea.cuDNN.Dnn"/> instance being used</param>
            <param name="n">The number of samples in the input tensor</param>
            <param name="k">The number of input features in the resulting backpropagated error delta</param>
            <param name="l">The number of features in the input delta</param>
            <param name="dy">A pointer to the output error delta</param>
            <param name="w">A pointer to the layer weights</param>
            <param name="dx">The backpropagated error delta</param>
        </member>
        <member name="M:NeuralNetworkNET.cuDNN.CuDnnExtensions.FullyConnectedBackwardFilter(Alea.cuDNN.Dnn,System.Int32,System.Int32,System.Int32,Alea.deviceptr{System.Single},Alea.deviceptr{System.Single},Alea.deviceptr{System.Single})">
            <summary>
            Executes the backward pass on a fully connected layer to calculate the gradient with respect to the weights
            </summary>
            <param name="dnn">The current <see cref="T:Alea.cuDNN.Dnn"/> instance being used</param>
            <param name="n">The number of samples in the input tensor</param>
            <param name="l">The number of features for each input sample</param>
            <param name="k">The number of features in the output error delta</param>
            <param name="x">A pointer to the input tensor</param>
            <param name="dy">A pointer to the output error delta</param>
            <param name="dw">A pointer to a memory area to use to saave the computed weights gradient</param>
        </member>
        <member name="M:NeuralNetworkNET.cuDNN.CuDnnExtensions.FullyConnectedBackwardBias(Alea.cuDNN.Dnn,System.Int32,System.Int32,Alea.deviceptr{System.Single},Alea.deviceptr{System.Single})">
            <summary>
            Executes the backward pass on a fully connected layer to calculate the gradient with respect to the biases
            </summary>
            <param name="dnn">The current <see cref="T:Alea.cuDNN.Dnn"/> instance being used</param>
            <param name="n">The number of samples in the input tensor</param>
            <param name="l">The number of features for each input sample</param>
            <param name="dy">A pointer to the layer output error delta</param>
            <param name="db">A pointer to the resulting biases gradient</param>
        </member>
        <member name="T:NeuralNetworkNET.cuDNN.CuDnnService">
            <summary>
            A static class that handles a shared, disposable instance of the <see cref="T:Alea.cuDNN.Dnn"/> class
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.cuDNN.CuDnnService.SynchronizeDnnContext">
            <summary>
            Synchronizes the context of the <see cref="T:Alea.Gpu"/> instance in use, if needed
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.cuDNN.CuDnnService.Instance">
            <summary>
            Gets a the shared <see cref="T:Alea.cuDNN.Dnn"/> instance in use
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.cuDNN.CuDnnService.IsAvailable">
            <summary>
            Gets whether or not the cuDNN support is available on the current system
            </summary>
        </member>
        <member name="T:NeuralNetworkNET.cuDNN.CuDnnService.CuDnnSupportHelper">
            <summary>
            A private class that is used to create a new standalone type that contains the actual test method (decoupling is needed to &lt;Module&gt; loading crashes)
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.cuDNN.CuDnnService.CuDnnSupportHelper.IsGpuAccelerationSupported">
            <summary>
            Checks whether or not the Cuda features are currently supported
            </summary>
        </member>
        <member name="T:NeuralNetworkNET.cuDNN.GpuExtensions">
            <summary>
            An extension class with some additions to the <see cref="T:Alea.Gpu"/> class
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.cuDNN.GpuExtensions.AllocateDevice(Alea.Gpu,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <summary>
            Allocates a memory area on device memory and copies the contents of the input <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/>
            </summary>
            <param name="gpu">The <see cref="T:Alea.Gpu"/> device to use</param>
            <param name="source">The source <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> with the data to copy</param>
        </member>
        <member name="M:NeuralNetworkNET.cuDNN.GpuExtensions.AllocateDevice(Alea.Gpu,NeuralNetworkNET.APIs.Structs.Tensor@,System.Int32,System.Int32)">
            <summary>
            Allocates a memory area on device memory, reading the target values at a given offset from the input <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/>
            </summary>
            <param name="gpu">The <see cref="T:Alea.Gpu"/> device to use</param>
            <param name="source">The source <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> with the data to copy</param>
            <param name="offset">The column offset for the data to read from each row</param>
            <param name="length"></param>
        </member>
        <member name="M:NeuralNetworkNET.cuDNN.GpuExtensions.CopyTo(Alea.DeviceMemory{System.Single},NeuralNetworkNET.APIs.Structs.Tensor@)">
            <summary>
            Copies the contents of the input <see cref="T:Alea.DeviceMemory`1"/> instance to the target host memory area
            </summary>
            <param name="source">The <see cref="T:Alea.DeviceMemory`1"/> area to read</param>
            <param name="destination">The destination <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> to write on</param>
        </member>
        <member name="M:NeuralNetworkNET.cuDNN.GpuExtensions.CopyTo(Alea.DeviceMemory{System.Single},System.Single[])">
            <summary>
            Copies the contents of the input <see cref="T:Alea.DeviceMemory`1"/> instance to the target host array
            </summary>
            <param name="source">The <see cref="T:Alea.DeviceMemory`1"/> area to read</param>
            <param name="destination">The destination array to write on</param>
        </member>
        <member name="M:NeuralNetworkNET.cuDNN.GpuExtensions.CopyTo(Alea.DeviceMemory{System.Single},NeuralNetworkNET.APIs.Structs.Tensor@,System.Int32,System.Int32)">
            <summary>
            Copies the source data into the target <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/>, splitting each individual entry into its own row
            </summary>
            <param name="source">The source memory area with the concatenated data for each entry</param>
            <param name="destination">The destination <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> that will store the data</param>
            <param name="offset">The column offset for the data for each entry</param>
            <param name="length">The number of values to copy for each entry</param>
        </member>
        <member name="M:NeuralNetworkNET.cuDNN.GpuExtensions.CopyToHost(Alea.DeviceMemory{System.Single},System.Int32,System.Int32,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <summary>
            Copies the contents of the input <see cref="T:Alea.DeviceMemory`1"/> to a new memory area on the unmanaged heap
            </summary>
            <param name="source">The source <see cref="T:Alea.DeviceMemory`1"/> memory to copy</param>
            <param name="n">The height of the input memory area</param>
            <param name="chw">The width of the input memory area</param>
            <param name="result">The resulting matrix</param>
        </member>
        <member name="M:NeuralNetworkNET.cuDNN.GpuExtensions.GetFreeMemory(Alea.Gpu)">
            <summary>
            Gets the amount of available GPU memory for a given GPU
            </summary>
            <param name="gpu">The target <see cref="T:Alea.Gpu"/> to use to retrieve the info</param>
        </member>
        <member name="T:NeuralNetworkNET.Exceptions.ComputationGraphBuildException">
            <summary>
            An exception that indicates a failure during the build process of a computation graph
            </summary>
        </member>
        <member name="T:NeuralNetworkNET.Exceptions.NetworkBuildException">
            <summary>
            A class that represents an error during the setup of a neural network
            </summary>
        </member>
        <member name="T:NeuralNetworkNET.Exceptions.ParallelLoopExecutionException">
            <summary>
            A simple class that represents a runtime error during the execution of a parallel loop
            </summary>
        </member>
        <member name="T:NeuralNetworkNET.Extensions.ArrayExtensions">
            <summary>
            A static class with some general methods for linear and 2D arrays
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.Extensions.ArrayExtensions.Flatten``1(``0[0:,0:])">
            <summary>
            Flattens a 2D array to a 1D array
            </summary>
            <typeparam name="T">The type of each element in the input matrix</typeparam>
            <param name="m">The input 2D array to flatten</param>
        </member>
        <member name="M:NeuralNetworkNET.Extensions.ArrayExtensions.MergeLines``1(System.Collections.Generic.IEnumerable{System.ValueTuple{``0[],``0[]}})">
            <summary>
            Merges the line pairs in the input collection into two 2D arrays
            </summary>
            <typeparam name="T">The type of each element in the input lines</typeparam>
            <param name="lines">The lines to merge</param>
        </member>
        <member name="T:NeuralNetworkNET.Extensions.DebugExtensions">
            <summary>
            A static class that contains some extensions to check the contents of various types of <see cref="T:System.Single"/> vectors
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.Extensions.DebugExtensions.ContentEquals(System.Span{System.Single},System.Span{System.Single},System.Single,System.Single)">
            <summary>
            Checks if two <see cref="T:System.Span`1"/> instances have the same size and content
            </summary>
            <param name="x1">The first <see cref="T:System.Span`1"/> to test</param>
            <param name="x2">The second <see cref="T:System.Span`1"/> to test</param>
            <param name="absolute">The relative comparison threshold</param>
            <param name="relative">The relative comparison threshold</param>
        </member>
        <member name="M:NeuralNetworkNET.Extensions.DebugExtensions.ContentEquals(NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,System.Single,System.Single)">
            <summary>
            Checks if two <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> instances have the same size and content
            </summary>
            <param name="m">The first <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> to test</param>
            <param name="o">The second <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> to test</param>
            <param name="absolute">The relative comparison threshold</param>
            <param name="relative">The relative comparison threshold</param>
        </member>
        <member name="M:NeuralNetworkNET.Extensions.DebugExtensions.ContentEquals(System.Single[0:,0:],System.Single[0:,0:],System.Single,System.Single)">
            <summary>
            Checks if two matrices have the same size and content
            </summary>
            <param name="m">The first matrix to test</param>
            <param name="o">The second matrix to test</param>
            <param name="absolute">The relative comparison threshold</param>
            <param name="relative">The relative comparison threshold</param>
        </member>
        <member name="M:NeuralNetworkNET.Extensions.DebugExtensions.ContentEquals(System.Single[],System.Single[],System.Single,System.Single)">
            <summary>
            Checks if two vectors have the same size and content
            </summary>
            <param name="v">The first vector to test</param>
            <param name="o">The second vector to test</param>
            <param name="absolute">The relative comparison threshold</param>
            <param name="relative">The relative comparison threshold</param>
        </member>
        <member name="T:NeuralNetworkNET.Extensions.HttpClientExtensions">
            <summary>
            A simple class with some extension methods for the <see cref="T:System.Net.Http.HttpClient"/> class
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.Extensions.HttpClientExtensions.GetAsync(System.Net.Http.HttpClient,System.String,System.IProgress{NeuralNetworkNET.Extensions.HttpProgress},System.Threading.CancellationToken)">
            <summary>
            Downloads a <see cref="T:System.IO.Stream"/> from the given URL, and reports the download progress using the input callback
            </summary>
            <param name="client">The <see cref="T:System.Net.Http.HttpClient"/> instance to use to download the data</param>
            <param name="url">The URL to download</param>
            <param name="callback">The optional progress calback</param>
            <param name="token">The optional token for the download operation</param>
        </member>
        <member name="T:NeuralNetworkNET.Extensions.HttpProgress">
            <summary>
            A <see langword="struct"/> that contains info on a pending download
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.Extensions.HttpProgress.DownloadedBytes">
            <summary>
            Gets the total number of downloaded bytes
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.Extensions.HttpProgress.Percentage">
            <summary>
            Gets the current download percentage
            </summary>
        </member>
        <member name="T:NeuralNetworkNET.Extensions.MiscExtensions">
            <summary>
            A simple class with some extension methods
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.Extensions.MiscExtensions.To``2(``0)">
            <summary>
            Casts the input item to a class or interface that inherits from the initial type
            </summary>
            <typeparam name="TIn">The input type</typeparam>
            <typeparam name="TOut">The output type</typeparam>
            <param name="item">The item to cast</param>
        </member>
        <member name="M:NeuralNetworkNET.Extensions.MiscExtensions.SwitchRef``1(System.Boolean,``0@,``0@)">
            <summary>
            Returns a reference according to the input flag
            </summary>
            <typeparam name="T">The reference type to return</typeparam>
            <param name="flag">The switch flag</param>
            <param name="left">The first option</param>
            <param name="right">The second option</param>
        </member>
        <member name="M:NeuralNetworkNET.Extensions.MiscExtensions.Max(System.Int32,System.Int32)">
            <summary>
            Returns the maximum value between two numbers
            </summary>
            <param name="a">The first number</param>
            <param name="b">The second number</param>
        </member>
        <member name="M:NeuralNetworkNET.Extensions.MiscExtensions.Max(System.Single,System.Single)">
            <summary>
            Returns the maximum value between two numbers
            </summary>
            <param name="a">The first number</param>
            <param name="b">The second number</param>
        </member>
        <member name="M:NeuralNetworkNET.Extensions.MiscExtensions.Min(System.Int32,System.Int32)">
            <summary>
            Returns the minimum value between two numbers
            </summary>
            <param name="a">The first number</param>
            <param name="b">The second number</param>
        </member>
        <member name="M:NeuralNetworkNET.Extensions.MiscExtensions.Abs(System.Single)">
            <summary>
            Calculates the absolute value of the input number
            </summary>
            <param name="value">The input value</param>
        </member>
        <member name="M:NeuralNetworkNET.Extensions.MiscExtensions.ToApproximatedFloat(System.Double)">
            <summary>
            Returns the minimum possible upper <see cref="T:System.Single"/> approximation of the given <see cref="T:System.Double"/> value
            </summary>
            <param name="value">The value to approximate</param>
        </member>
        <member name="M:NeuralNetworkNET.Extensions.MiscExtensions.EqualsWithDelta(System.Single,System.Single,System.Single,System.Single)">
            <summary>
            Calculates if two values are within a given distance from one another
            </summary>
            <param name="value">The first value</param>
            <param name="other">The second value</param>
            <param name="absolute">The relative comparison threshold</param>
            <param name="relative">The relative comparison threshold</param>
        </member>
        <member name="M:NeuralNetworkNET.Extensions.MiscExtensions.IntegerSquare(System.Int32)">
            <summary>
            Calculates the integer square of the input value
            </summary>
            <param name="x">The value to use to calculate the square root</param>
        </member>
        <member name="M:NeuralNetworkNET.Extensions.MiscExtensions.Pick``2(System.Collections.Generic.IEnumerable{``0})">
            <summary>
            Filters the input sequence by only choosing the items of type <typeparamref name="TOut"/>
            </summary>
            <typeparam name="TIn">The base type of the sequence items</typeparam>
            <typeparam name="TOut">The type of the items to select</typeparam>
            <param name="enumerable">The source sequence</param>
        </member>
        <member name="M:NeuralNetworkNET.Extensions.MiscExtensions.RoundToSeconds(System.TimeSpan)">
            <summary>
            Rounds the given <see cref="T:System.TimeSpan"/> to an interval with an integer number of total seconds
            </summary>
            <param name="timeSpan">The instance to round</param>
        </member>
        <member name="M:NeuralNetworkNET.Extensions.MiscExtensions.Partition``1(System.Collections.Generic.IEnumerable{``0},System.Int32)">
            <summary>
            Partitions the input sequence into a series of batches of the given size
            </summary>
            <typeparam name="T">The type of the sequence items</typeparam>
            <param name="values">The sequence of items to batch</param>
            <param name="size">The desired batch size</param>
        </member>
        <member name="M:NeuralNetworkNET.Extensions.MiscExtensions.AssertCompleted(System.Threading.Tasks.ParallelLoopResult@)">
            <summary>
            Raises an <see cref="T:System.InvalidOperationException"/> if the loop wasn't completed successfully
            </summary>
            <param name="result">The <see cref="T:System.Threading.Tasks.ParallelLoopResult"/> to test</param>
        </member>
        <member name="M:NeuralNetworkNET.Extensions.MiscExtensions.TrimVerbatim(System.String)">
            <summary>
            Removes the left spaces from the input verbatim string
            </summary>
            <param name="text">The string to trim</param>
        </member>
        <member name="M:NeuralNetworkNET.Extensions.MiscExtensions.AsIProgress``1(System.Action{``0})">
            <summary>
            Tries to convert the input <see cref="T:System.Action`1"/> into an <see cref="T:System.IProgress`1"/> instance
            </summary>
            <typeparam name="T">The type returned by the input <see cref="T:System.Action`1"/></typeparam>
            <param name="action">The input <see cref="T:System.Action`1"/> to convert</param>
        </member>
        <member name="M:NeuralNetworkNET.Extensions.MiscExtensions.IndexOf``1(System.Collections.Generic.IEnumerable{``0},``0)">
            <summary>
            Gets the index of the target item (by reference) in the source sequence
            </summary>
            <typeparam name="T">The type of items in the input sequence</typeparam>
            <param name="sequence">The input sequence</param>
            <param name="value">The item to look for</param>
        </member>
        <member name="T:NeuralNetworkNET.Extensions.SpanExtensions">
            <summary>
            An helper class with methods to process fixed-size matrices
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.Extensions.SpanExtensions.Fill``1(System.Span{``0},System.Func{``0})">
            <summary>
            Fills the target <see cref="T:System.Span`1"/> with the input values provider
            </summary>
            <param name="span">The <see cref="T:System.Span`1"/> to fill up</param>
            <param name="provider">The values provider to use</param>
        </member>
        <member name="M:NeuralNetworkNET.Extensions.SpanExtensions.AsMatrix``1(System.Span{``0},System.Int32,System.Int32)">
            <summary>
            Returns a matrix with the reshaped content of the input <see cref="T:System.Span`1"/>
            </summary>
            <param name="span">The input <see cref="T:System.Span`1"/></param>
            <param name="h">The number of matrix rows</param>
            <param name="w">The number of matrix columns</param>
        </member>
        <member name="M:NeuralNetworkNET.Extensions.SpanExtensions.GetContentHashCode``1(System.Span{``0})">
            <summary>
            Returns an hash code for the contents of the input <see cref="T:System.Span`1"/>
            </summary>
            <typeparam name="T">The type of each value in the input <see cref="T:System.Span`1"/></typeparam>
            <param name="span">The input <see cref="T:System.Span`1"/> to read</param>
        </member>
        <member name="M:NeuralNetworkNET.Extensions.SpanExtensions.Slice``1(``0[0:,0:],System.Int32)">
            <summary>
            Extracts a single row from a given matrix
            </summary>
            <param name="m">The source matrix</param>
            <param name="row">The target row to return</param>
        </member>
        <member name="M:NeuralNetworkNET.Extensions.SpanExtensions.Argmax(System.Span{System.Single})">
            <summary>
            Returns the index of the maximum value in the input <see cref="T:System.Span`1"/>
            </summary>
            <param name="span">The source <see cref="T:System.Span`1"/> instance</param>
        </member>
        <member name="M:NeuralNetworkNET.Extensions.SpanExtensions.MatchElementwiseThreshold(System.Span{System.Single},System.Span{System.Single},System.Single)">
            <summary>
            Returns whether or not all the elements in the two input <see cref="T:System.Span`1"/> instances respect the input threshold
            </summary>
            <param name="x1">The first <see cref="T:System.Span`1"/> instance to check</param>
            <param name="x2">The second <see cref="T:System.Span`1"/> instance to check</param>
            <param name="threshold">The target threshold</param>
            <remarks>This method is <see langword="internal"/> as it's meant to be exposed through the <see cref="T:NeuralNetworkNET.APIs.Settings.AccuracyTesters"/> class only</remarks>
        </member>
        <member name="M:NeuralNetworkNET.Extensions.SpanExtensions.IsCloseTo(System.Span{System.Single},System.Span{System.Single},System.Single)">
            <summary>
            Returns whether or not all the elements in the two input <see cref="T:System.Span`1"/> respect the maximum distance between each other
            </summary>
            <param name="x1">The first <see cref="T:System.Span`1"/> instance to check</param>
            <param name="x2">The second <see cref="T:System.Span`1"/> instance to check</param>
            <param name="threshold">The target maximum distance</param>
            <remarks>This method is <see langword="internal"/> as it's meant to be exposed through the <see cref="T:NeuralNetworkNET.APIs.Settings.AccuracyTesters"/> class only</remarks>
        </member>
        <member name="T:NeuralNetworkNET.Extensions.StreamExtensions">
            <summary>
            A static class with some <see cref="T:System.IO.Stream"/> extensions to load/write contents
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.Extensions.StreamExtensions.Write``1(System.IO.Stream,``0)">
            <summary>
            Writes the input <see langword="struct"/> to the target <see cref="T:System.IO.Stream"/> instance
            </summary>
            <typeparam name="T">The <see langword="struct"/> type to serialize</typeparam>
            <param name="stream">The target <see cref="T:System.IO.Stream"/> to use to write the data</param>
            <param name="value">The <see langword="struct"/> to write to the <see cref="T:System.IO.Stream"/> instance</param>
        </member>
        <member name="M:NeuralNetworkNET.Extensions.StreamExtensions.TryRead``1(System.IO.Stream,``0@)">
            <summary>
            Reads a value of the given <see langword="struct"/> type from the input <see cref="T:System.IO.Stream"/> instance
            </summary>
            <typeparam name="T">The <see langword="struct"/> type to read and return</typeparam>
            <param name="stream">The source <see cref="T:System.IO.Stream"/> instance to use to read the data</param>
            <param name="value">The resulting <typeparamref name="T"/> value that is read from the <see cref="T:System.IO.Stream"/></param>
        </member>
        <member name="M:NeuralNetworkNET.Extensions.StreamExtensions.WriteShuffled(System.IO.Stream,System.Single[])">
            <summary>
            Writes a shuffled <see cref="T:System.Single"/> vector to the target <see cref="T:System.IO.Stream"/>
            </summary>
            <param name="stream">The target <see cref="T:System.IO.Stream"/></param>
            <param name="v">The source vector to shuffle and write to the <see cref="T:System.IO.Stream"/> instance</param>
        </member>
        <member name="M:NeuralNetworkNET.Extensions.StreamExtensions.ReadUnshuffled(System.IO.Stream,System.Int32)">
            <summary>
            Reads a shuffled <see cref="T:System.Single"/> vector from the target <see cref="T:System.IO.Stream"/>
            </summary>
            <param name="stream">The source <see cref="T:System.IO.Stream"/></param>
            <param name="n">The vector length</param>
        </member>
        <member name="T:NeuralNetworkNET.Helpers.DatasetsDownloader">
            <summary>
            A static class that handles in-app resources downloaded from the web
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.Helpers.DatasetsDownloader.DatasetsPath">
            <summary>
            Gets the default datasets path to use to store and load fdata files
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.Helpers.DatasetsDownloader.Client">
            <summary>
            Gets the singleton <see cref="T:System.Net.Http.HttpClient"/> to use, since it is reentrant and thread-safe, see <a href="https://docs.microsoft.com/it-it/dotnet/api/system.net.http.httpclient">docs.microsoft.com/it-it/dotnet/api/system.net.http.httpclient</a>
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.Helpers.DatasetsDownloader.GetFileAsync(System.String,System.IProgress{NeuralNetworkNET.Extensions.HttpProgress},System.Threading.CancellationToken)">
            <summary>
            Gets a <see cref="T:System.Func`1"/> instance returning a <see cref="T:System.IO.Stream"/> with the contents of the input URL
            </summary>
            <param name="url">The target URL to use to download the resources</param>
            <param name="callback">The optional progress calback</param>
            <param name="token">A cancellation token for the operation</param>
        </member>
        <member name="M:NeuralNetworkNET.Helpers.DatasetsDownloader.GetArchiveAsync(System.String,System.IProgress{NeuralNetworkNET.Extensions.HttpProgress},System.Threading.CancellationToken)">
            <summary>
            Gets an <see cref="T:System.Collections.Generic.IDictionary`2"/> with a collection of <see cref="T:System.Func`1"/> instances for each file in the tar.gz archive pointed by the input URL
            </summary>
            <param name="url">The target URL to use to download the archive</param>
            <param name="callback">The optional progress calback</param>
            <param name="token">A cancellation token for the operation</param>
        </member>
        <member name="M:NeuralNetworkNET.Helpers.DatasetsDownloader.GetFilename(System.String)">
            <summary>
            Gets a unique filename from the input URL
            </summary>
            <param name="url">The URL to convert to filename</param>
        </member>
        <member name="T:NeuralNetworkNET.Helpers.ImageLoader">
            <summary>
            A static class with some helper methods to quickly load a sample from a target image file
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.Helpers.ImageLoader.Load``1(System.String,NeuralNetworkNET.APIs.Enums.ImageNormalizationMode,System.Action{SixLabors.ImageSharp.Processing.IImageProcessingContext{``0}})">
            <summary>
            Loads the target image and applies the requested changes, then converts it to a dataset sample
            </summary>
            <param name="path">The path of the image to load</param>
            <param name="normalization">The image normalization mode to apply</param>
            <param name="modify">The optional changes to apply to the image</param>
        </member>
        <member name="M:NeuralNetworkNET.Helpers.ImageLoader.Normalize``1(``0,NeuralNetworkNET.APIs.Enums.ImageNormalizationMode)">
            <summary>
            Normalizes the input <see cref="T:SixLabors.ImageSharp.PixelFormats.IPixel"/> value using the specified mode
            </summary>
            <typeparam name="TPixel">Tye input pixel type</typeparam>
            <param name="pixel">The input pixel to normalize</param>
            <param name="normalization">The normalization mode to use</param>
        </member>
        <member name="T:NeuralNetworkNET.Helpers.Sha256">
            <summary>
            A static class that can be used to quickly calculate hashes from array of an arbitrary <see langword="struct"/> type
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.Helpers.Sha256.Hash``1(``0[])">
            <summary>
            Calculates an hash for the input <typeparamref name="T"/> array
            </summary>
            <typeparam name="T">The type of items in the input array</typeparam>
            <param name="array">The input array to process</param>
        </member>
        <member name="M:NeuralNetworkNET.Helpers.Sha256.Hash``1(``0[][])">
            <summary>
            Calculates an aggregate hash for the input <typeparamref name="T"/> arrays
            </summary>
            <typeparam name="T">The type of items in the input arrays</typeparam>
            <param name="arrays">The arrays to process</param>
        </member>
        <member name="T:NeuralNetworkNET.Helpers.SharedEvent">
            <summary>
            A shared event with an <see cref="T:System.Action"/> <see langword="delegate"/> as its handler
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.Helpers.SharedEvent.Add(System.Action)">
            <summary>
            Adds the input <see cref="T:System.Action"/> to the list of handlers
            </summary>
            <param name="action">The <see cref="T:System.Action"/> to add</param>
        </member>
        <member name="M:NeuralNetworkNET.Helpers.SharedEvent.Raise">
            <summary>
            Raises the shared event
            </summary>
        </member>
        <member name="T:NeuralNetworkNET.Helpers.TensorMap`1">
            <summary>
            A simple disposable map that stores <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> instances while training or using a network
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.Helpers.TensorMap`1.System#IDisposable#Dispose">
            <inheritdoc/>
        </member>
        <member name="T:NeuralNetworkNET.Helpers.ThreadSafeRandom">
            <summary>
            A thread-safe <see cref="T:System.Random"/> wrapper class with some extension methods
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.Helpers.ThreadSafeRandom.NextUniform(System.Single)">
            <summary>
            Returns the next uniform value with the desired range
            </summary>
            <param name="scale">The desired scale for the uniform distribution</param>
        </member>
        <member name="M:NeuralNetworkNET.Helpers.ThreadSafeRandom.NextGaussian(System.Single)">
            <summary>
            Returns the next gaussian random value (mean 0, standard deviation 1)
            </summary>
            <param name="deviation">The standard deviation for the distribution</param>
        </member>
        <member name="M:NeuralNetworkNET.Helpers.ThreadSafeRandom.NextInt(System.Int32,System.Int32)">
            <summary>
            Returns the next <see cref="T:System.Int32"/> random value
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.Helpers.ThreadSafeRandom.NextFloat">
            <summary>
            Returns the next <see cref="T:System.Single"/> random value
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.Helpers.ThreadSafeRandom.NextBool">
            <summary>
            Returns the next <see cref="T:System.Boolean"/> random value
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.Helpers.ThreadSafeRandom.NextDropoutMask(System.Int32,System.Int32,System.Single,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <summary>
            Creates a dropout mask with the given size and probability
            </summary>
            <param name="x">The height of the matrix</param>
            <param name="y">The width of the matrix</param>
            <param name="dropout">The dropout probability (indicates the probability of keeping a neuron active)</param>
            <param name="mask">The resulting mask</param>
        </member>
        <member name="T:NeuralNetworkNET.Helpers.TrainingProgressExportHelpers">
            <summary>
            A class with some helper methods to quickly convert a training report to a 2D chart
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.Helpers.TrainingProgressExportHelpers.AsPythonMatplotlibChart(System.Collections.Generic.IReadOnlyList{NeuralNetworkNET.APIs.Results.DatasetEvaluationResult},NeuralNetworkNET.APIs.Enums.TrainingReportType)">
            <summary>
            Returns a Python script to plot a 2D chart from the given progress reports
            </summary>
            <param name="results">The input results to plot</param>
            <param name="type">The type of progress report to plot</param>
        </member>
        <member name="T:NeuralNetworkNET.Networks.Activations.ActivationFunctionProvider">
            <summary>
            A static class that returns the right activation function for the given type
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Activations.ActivationFunctionProvider.GetActivations(NeuralNetworkNET.APIs.Enums.ActivationType)">
            <summary>
            Gets an activation and activation prime functions for the given type
            </summary>
            <param name="type">The activation function type</param>
        </member>
        <member name="T:NeuralNetworkNET.Networks.Activations.ActivationFunctions">
            <summary>
            A static collection of available activation functions
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Activations.ActivationFunctions.Sigmoid(System.Single)">
            <summary>
            Applies the sigmoid function, 1 / (1 + e^(-x))
            </summary>
            <param name="x">The input to process</param>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Activations.ActivationFunctions.SigmoidPrime(System.Single)">
            <summary>
            Applies the sigmoid prime function, 1 / (1 + e^(-x))
            </summary>
            <param name="x">The input to process</param>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Activations.ActivationFunctions.Tanh(System.Single)">
            <summary>
            Applies the tanh function
            </summary>
            <param name="x">The input to process</param>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Activations.ActivationFunctions.TanhPrime(System.Single)">
            <summary>
            Applies the derivative of the <see cref="M:NeuralNetworkNET.Networks.Activations.ActivationFunctions.Tanh(System.Single)"/> function
            </summary>
            <param name="x">The input to process</param>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Activations.ActivationFunctions.LeCunTanh(System.Single)">
            <summary>
            Applies the LeCun tanh function
            </summary>
            <param name="x">The input to process</param>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Activations.ActivationFunctions.LeCunTanhPrime(System.Single)">
            <summary>
            Applies the derivative of the <see cref="M:NeuralNetworkNET.Networks.Activations.ActivationFunctions.LeCunTanh(System.Single)"/> function
            </summary>
            <param name="x">The input to process</param>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Activations.ActivationFunctions.ReLU(System.Single)">
            <summary>
            Applies the rectifier function
            </summary>
            <param name="x">The input to process</param>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Activations.ActivationFunctions.ReLUPrime(System.Single)">
            <summary>
            Applies the derivative of the <see cref="M:NeuralNetworkNET.Networks.Activations.ActivationFunctions.Tanh(System.Single)"/> function
            </summary>
            <param name="x">The input to process</param>
            <remarks>The real derivative is indetermined when x is 0</remarks>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Activations.ActivationFunctions.LeakyReLU(System.Single)">
            <summary>
            Applies the leaky ReLU function
            </summary>
            <param name="x">The input to process</param>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Activations.ActivationFunctions.LeakyReLUPrime(System.Single)">
            <summary>
            Applies the derivative of the <see cref="M:NeuralNetworkNET.Networks.Activations.ActivationFunctions.LeakyReLU(System.Single)"/> function
            </summary>
            <param name="x">The input to process</param>
            <remarks>The real derivative is indetermined when x is 0</remarks>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Activations.ActivationFunctions.Softmax(System.Single)">
            <summary>
            Applies the the numerator part of the softmax activation function, e^x
            </summary>
            <param name="x">The input to process</param>
            <remarks>The derivative is not available, as it doesn't appear in the derivative of the log-likelyhood cost function</remarks>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Activations.ActivationFunctions.Softplus(System.Single)">
            <summary>
            Applies the softplus function, ln(1 + e^x)
            </summary>
            <param name="x">The input to process</param>
            <remarks>The derivative of the softplus is the <see cref="M:NeuralNetworkNET.Networks.Activations.ActivationFunctions.Sigmoid(System.Single)"/> function</remarks>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Activations.ActivationFunctions.ELU(System.Single)">
            <summary>
            Applies the exponential linear unit function
            </summary>
            <param name="x">The input to process</param>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Activations.ActivationFunctions.ELUPrime(System.Single)">
            <summary>
            Applies the derivative of the <see cref="M:NeuralNetworkNET.Networks.Activations.ActivationFunctions.ELU(System.Single)"/> function
            </summary>
            <param name="x">The input to process</param>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Activations.ActivationFunctions.AbsoluteReLU(System.Single)">
            <summary>
            Applies the absolute ReLU linear unit function
            </summary>
            <param name="x">The input to process</param>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Activations.ActivationFunctions.AbsoluteReLUPrime(System.Single)">
            <summary>
            Applies the derivative of the <see cref="M:NeuralNetworkNET.Networks.Activations.ActivationFunctions.AbsoluteReLU(System.Single)"/> function
            </summary>
            <param name="x">The input to process</param>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Activations.ActivationFunctions.Identity(System.Single)">
            <summary>
            Applies the identity function
            </summary>
            <param name="x">The input to process</param>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Activations.ActivationFunctions.Identityprime(System.Single)">
            <summary>
            Applies the derivative of the <see cref="M:NeuralNetworkNET.Networks.Activations.ActivationFunctions.Identity(System.Single)"/> function
            </summary>
            <param name="x">The input to process</param>
        </member>
        <member name="T:NeuralNetworkNET.Networks.Activations.Delegates.ActivationFunction">
            <summary>
            A delegate that represents an activation function (or the derivative of an activation function) used in a neural network
            </summary>
            <param name="x">The input value</param>
        </member>
        <member name="T:NeuralNetworkNET.Networks.Cost.CostFunctionProvider">
            <summary>
            A static class that returns the right acost function for the given type
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Cost.CostFunctionProvider.GetCostFunctions(NeuralNetworkNET.Networks.Cost.CostFunctionType)">
            <summary>
            Gets the right cost function with the given type
            </summary>
            <param name="type">The cost function type</param>
        </member>
        <member name="T:NeuralNetworkNET.Networks.Cost.CostFunctions">
            <summary>
            A collection of cost functions available for the neural networks
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Cost.CostFunctions.QuadraticCost(NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <summary>
            Calculates the quadratic cost for the given outputs and expected results
            </summary>
            <param name="yHat">The current results</param>
            <param name="y">The expected results for the dataset</param>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Cost.CostFunctions.CrossEntropyCost(NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <summary>
            Calculates the cross-entropy cost for a given feedforward result
            </summary>
            <param name="yHat">The current results</param>
            <param name="y">The expected results for the dataset</param>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Cost.CostFunctions.LogLikelyhoodCost(NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <summary>
            Calculates the log-likelyhood cost for the given outputs and expected results
            </summary>
            <param name="yHat">The current results</param>
            <param name="y">The expected results for the dataset</param>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Cost.CostFunctions.QuadraticCostPrime(NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.Networks.Activations.Delegates.ActivationFunction,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <summary>
            Calculates the derivative of the quadratic cost function for the given outputs, expected results and activity
            </summary>
            <param name="yHat">The current results</param>
            <param name="y">The expected results for the dataset</param>
            <param name="z">The activity on the last network layer</param>
            <param name="activationPrime">The activation pime function for the last network layer</param>
            <param name="dx">The backpropagated error</param>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Cost.CostFunctions.CrossEntropyCostPrime(NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.Networks.Activations.Delegates.ActivationFunction,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <summary>
            Calculates the derivative cross-entropy cost for a given feedforward result
            </summary>
            <param name="yHat">The current results</param>
            <param name="y">The expected results for the dataset</param>
            <param name="z">The activity on the last network layer</param>
            <param name="activationPrime">The activation pime function for the last network layer</param>
            <param name="dx">The backpropagated error</param>
        </member>
        <member name="T:NeuralNetworkNET.Networks.Cost.CostFunctionType">
            <summary>
            Indicates the cost function to use to evaluate and train a neural network
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.Networks.Cost.CostFunctionType.Quadratic">
            <summary>
            The classic quadratic cost function, 1/n(yHat - y)^2
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.Networks.Cost.CostFunctionType.CrossEntropy">
            <summary>
            The cross-entropy cost function, -1/n(y*ln(yHat) + (1 - y)ln(1 - yHat))
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.Networks.Cost.CostFunctionType.LogLikelyhood">
            <summary>
            The log-likelyhood cost function (for a softmax output layer), -ln(yHat[y])
            </summary>
        </member>
        <member name="T:NeuralNetworkNET.Networks.Cost.Delegates.CostFunction">
            <summary>
            A delegate that represents a cost function used to compute the accuracy of a neural network
            </summary>
            <param name="yHat">The output of the network being trained</param>
            <param name="y">The expected output for the network</param>
        </member>
        <member name="T:NeuralNetworkNET.Networks.Cost.Delegates.CostFunctionPrime">
            <summary>
            A delegates for a function that computes the derivative of the cost function used to train a neural network
            </summary>
            <param name="yHat">The current results</param>
            <param name="y">The expected results for the dataset</param>
            <param name="z">The activity on the last network layer</param>
            <param name="activationPrime">The activation pime function for the last network layer</param>
            <param name="dx">The backpropagated error</param>
        </member>
        <member name="T:NeuralNetworkNET.Networks.Graph.ComputationGraph">
            <summary>
            A graph of <see cref="T:NeuralNetworkNET.APIs.Interfaces.INetworkLayer"/> instances, with O(1) pre-order access time for nodes
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.Networks.Graph.ComputationGraph.Root">
            <summary>
            Gets the root <see cref="T:NeuralNetworkNET.Networks.Graph.Nodes.InputNode"/> for the current graph
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.Networks.Graph.ComputationGraph.Nodes">
            <summary>
            Gets the in-order serialized view of the network graph nodes
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.Networks.Graph.ComputationGraph.ProcessingNodes">
            <summary>
            Gets the in-order serialized view of the <see cref="T:NeuralNetworkNET.Networks.Graph.Nodes.ProcessingNode"/> instances in the current graph
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.Networks.Graph.ComputationGraph.OutputNode">
            <summary>
            Gets the graph main output node
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.Networks.Graph.ComputationGraph.TrainingOutputNodes">
            <summary>
            Gets the training output nodes, if present
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Graph.ComputationGraph.New(NeuralNetworkNET.APIs.Structs.TensorInfo,NeuralNetworkNET.Networks.Graph.NodeBuilder)">
            <summary>
            Builds a computation graph from the input node and <see cref="T:NeuralNetworkNET.APIs.Structs.TensorInfo"/> shape
            </summary>
            <param name="input">The shape of the inputs for the graph to create</param>
            <param name="root">The <see cref="T:NeuralNetworkNET.Networks.Graph.NodeBuilder"/> instance that represents the graph root node</param>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Graph.ComputationGraph.Serialize(System.IO.Stream)">
            <summary>
            Writes the current graph to the input <see cref="T:System.IO.Stream"/>
            </summary>
            <param name="stream">The target <see cref="T:System.IO.Stream"/> to use to write the graph data</param>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Graph.ComputationGraph.Deserialize(System.IO.Stream,NeuralNetworkNET.APIs.Enums.ExecutionModePreference)">
            <summary>
            Tries to deserialize a new <see cref="T:NeuralNetworkNET.Networks.Graph.ComputationGraph"/> from the input <see cref="T:System.IO.Stream"/> and returns a <see cref="T:System.Func`2"/> to rebuild it
            </summary>
            <param name="stream">The input <see cref="T:System.IO.Stream"/> to use to read the network data</param>
            <param name="preference">The layers deserialization preference</param>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Graph.ComputationGraph.Equals(NeuralNetworkNET.Networks.Graph.ComputationGraph)">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Graph.ComputationGraph.GetCloneFactory">
            <summary>
            Creates a <see cref="T:System.Func`2"/> to clone the current instance, given the input <see cref="T:NeuralNetworkNET.APIs.Structs.TensorInfo"/>
            </summary>
        </member>
        <member name="T:NeuralNetworkNET.Networks.Graph.ComputationGraphJsonConverter">
            <summary>
            A simple class that handles the Json srializaation of the metadata of aa given <see cref="T:NeuralNetworkNET.Networks.Graph.ComputationGraph"/> instance
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Graph.ComputationGraphJsonConverter.WriteJson(Newtonsoft.Json.JsonWriter,System.Object,Newtonsoft.Json.JsonSerializer)">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Graph.ComputationGraphJsonConverter.ReadJson(Newtonsoft.Json.JsonReader,System.Type,System.Object,Newtonsoft.Json.JsonSerializer)">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Graph.ComputationGraphJsonConverter.CanConvert(System.Type)">
            <inheritdoc/>
        </member>
        <member name="T:NeuralNetworkNET.Networks.Graph.NodeBuilder">
            <summary>
            A class that represents a declaration for a graph node to build
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.Networks.Graph.NodeBuilder.Parents">
            <summary>
            Gets the current list of parent <see cref="T:NeuralNetworkNET.Networks.Graph.NodeBuilder"/> instances
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.Networks.Graph.NodeBuilder.Children">
            <summary>
            Gets the current list of child <see cref="T:NeuralNetworkNET.Networks.Graph.NodeBuilder"/> instances
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Graph.NodeBuilder.Input">
            <summary>
            Creates an input node to use to build a new graph
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Graph.NodeBuilder.GetParameter``1">
            <summary>
            Extracts the node parameter
            </summary>
            <typeparam name="T">The target parameter type</typeparam>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Graph.NodeBuilder.op_Addition(NeuralNetworkNET.Networks.Graph.NodeBuilder,NeuralNetworkNET.Networks.Graph.NodeBuilder)">
            <summary>
            Creates a new linear sum node that merges two input nodes
            </summary>
            <param name="first">The first node to sum</param>
            <param name="second">The second node to sum</param>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Graph.NodeBuilder.Sum(NeuralNetworkNET.Networks.Graph.NodeBuilder[])">
            <summary>
            Creates a new linear sum node that merges multiple input nodes
            </summary>
            <param name="inputs">The sequence of parent nodes for the new instance</param>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Graph.NodeBuilder.Sum(NeuralNetworkNET.APIs.Enums.ActivationType,NeuralNetworkNET.Networks.Graph.NodeBuilder[])">
            <summary>
            Creates a new linear sum node that merges multiple input nodes
            </summary>
            <param name="activation">The activation function to use in the sum layer</param>
            <param name="inputs">The sequence of parent nodes for the new instance</param>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Graph.NodeBuilder.Sum(NeuralNetworkNET.APIs.Enums.ActivationType,NeuralNetworkNET.APIs.Enums.ExecutionModePreference,NeuralNetworkNET.Networks.Graph.NodeBuilder[])">
            <summary>
            Creates a new linear sum node that merges multiple input nodes
            </summary>
            <param name="activation">The activation function to use in the sum layer</param>
            <param name="mode">The desired execution preference for the activation function</param>
            <param name="inputs">The sequence of parent nodes for the new instance</param>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Graph.NodeBuilder.DepthConcatenation(NeuralNetworkNET.Networks.Graph.NodeBuilder[])">
            <summary>
            Creates a new depth concatenation node that merges multiple input nodes with the same output shape
            </summary>
            <param name="inputs">The sequence of parent nodes for the new instance</param>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Graph.NodeBuilder.Layer(NeuralNetworkNET.APIs.Delegates.LayerFactory)">
            <summary>
            Creates a new node that will host a network layer to process its inputs
            </summary>
            <param name="factory">The <see cref="T:NeuralNetworkNET.APIs.Delegates.LayerFactory"/> instance used to create the network layer</param>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Graph.NodeBuilder.Pipeline(NeuralNetworkNET.APIs.Delegates.LayerFactory[])">
            <summary>
            Creates a pipeline that contains a sequence of network layers
            </summary>
            <param name="factories">The list of <see cref="T:NeuralNetworkNET.APIs.Delegates.LayerFactory"/> instances to use to create the pipeline nodes</param>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Graph.NodeBuilder.TrainingBranch">
            <summary>
            Creates a new node that will route its inputs to a training sug-graph
            </summary>
        </member>
        <member name="T:NeuralNetworkNET.Networks.Graph.Nodes.Abstract.MergeNodeBase">
            <summary>
            A class representing a junction node in a computation graph
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.Networks.Graph.Nodes.Abstract.MergeNodeBase.Parents">
            <summary>
            Gets the list of parents nodes to merge for the current node
            </summary>
        </member>
        <member name="T:NeuralNetworkNET.Networks.Graph.Nodes.Abstract.NodeBase">
            <summary>
            A base class for all the available node types
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.Networks.Graph.Nodes.Abstract.NodeBase.Type">
            <inheritdoc/>
        </member>
        <member name="P:NeuralNetworkNET.Networks.Graph.Nodes.Abstract.NodeBase.Children">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Graph.Nodes.Abstract.NodeBase.Equals(NeuralNetworkNET.APIs.Interfaces.IComputationGraphNode)">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Graph.Nodes.Abstract.NodeBase.Serialize(System.IO.Stream)">
            <summary>
            Writes the current node to the input <see cref="T:System.IO.Stream"/>
            </summary>
            <param name="stream">The target <see cref="T:System.IO.Stream"/> to use to write the node data</param>
        </member>
        <member name="T:NeuralNetworkNET.Networks.Graph.Nodes.DepthConcatenationNode">
            <summary>
            A class representing a depth concatenation node
            </summary>
        </member>
        <member name="T:NeuralNetworkNET.Networks.Graph.Nodes.InputNode">
            <summary>
            A class that represents the root node for a computation graph
            </summary>
        </member>
        <member name="T:NeuralNetworkNET.Networks.Graph.Nodes.ProcessingNode">
            <summary>
            A class representing a single node in a computation graph
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.Networks.Graph.Nodes.ProcessingNode.Layer">
            <summary>
            Gets the layer associated with the current graph node
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.Networks.Graph.Nodes.ProcessingNode.Parent">
            <summary>
            Gets the parent node for the current graph node
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Graph.Nodes.ProcessingNode.Equals(NeuralNetworkNET.APIs.Interfaces.IComputationGraphNode)">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Graph.Nodes.ProcessingNode.Serialize(System.IO.Stream)">
            <inheritdoc/>
        </member>
        <member name="T:NeuralNetworkNET.Networks.Graph.Nodes.SumNode">
            <summary>
            A class representing a sum node in a computation graph
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.Networks.Graph.Nodes.SumNode.ActivationType">
            <summary>
            Gets the activation type used in the current node
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.Networks.Graph.Nodes.SumNode.ActivationFunctions">
            <summary>
            Gets the list of activation and activation prime functions used in the sum node
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.Networks.Graph.Nodes.SumNode.ExecutionMode">
            <summary>
            Gets the execution mode set for this <see cref="T:NeuralNetworkNET.Networks.Graph.Nodes.SumNode"/> instance
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Graph.Nodes.SumNode.New(NeuralNetworkNET.APIs.Enums.ActivationType,NeuralNetworkNET.APIs.Enums.ExecutionModePreference,System.Collections.Generic.IReadOnlyList{NeuralNetworkNET.APIs.Interfaces.IComputationGraphNode})">
            <summary>
            Creates a new <see cref="T:NeuralNetworkNET.Networks.Graph.Nodes.SumNode"/> with the given parameters
            </summary>
            <param name="activation">The sum node activation function</param>
            <param name="mode">The desired execution mode</param>
            <param name="parents">The parent nodes for the new sum mode to create</param>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Graph.Nodes.SumNode.Forward(System.Span{NeuralNetworkNET.APIs.Structs.Tensor},NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <summary>
            Forwards the inputs through the graph node and returns the resulting activity (Z) and activation (A)
            </summary>
            <param name="inputs">The inputs to process</param>
            <param name="z">The output activity on the current node</param>
            <param name="a">The output activation on the current node</param>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Graph.Nodes.SumNode.Backpropagate(NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <summary>
            Backpropagates the error to compute the delta for the inputs of the graph node
            </summary>
            <param name="y">The output <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> computed in the forward pass</param>
            <param name="dy">The output error delta to backpropagate</param>
            <param name="dx">The resulting backpropagated error</param>
        </member>
        <member name="T:NeuralNetworkNET.Networks.Graph.Nodes.SumNode.CpuSumNode">
            <summary>
            A CPU-powered sum node
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Graph.Nodes.SumNode.CpuSumNode.Forward(System.Span{NeuralNetworkNET.APIs.Structs.Tensor},NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Graph.Nodes.SumNode.CpuSumNode.Backpropagate(NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <inheritdoc/>
        </member>
        <member name="T:NeuralNetworkNET.Networks.Graph.Nodes.SumNode.CudaSumNode">
            <summary>
            A CUDA-powered sum node
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.Networks.Graph.Nodes.SumNode.CudaSumNode.DnnInstance">
            <summary>
            Gets the <see cref="T:Alea.cuDNN.Dnn"/> instance for the current node
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Graph.Nodes.SumNode.CudaSumNode.Forward(System.Span{NeuralNetworkNET.APIs.Structs.Tensor},NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Graph.Nodes.SumNode.CudaSumNode.Backpropagate(NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Graph.Nodes.SumNode.Serialize(System.IO.Stream)">
            <inheritdoc/>
        </member>
        <member name="T:NeuralNetworkNET.Networks.Graph.Nodes.TrainingNode">
            <summary>
            A class that represents the root node for a training sub-graph
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.Networks.Graph.Nodes.TrainingNode.Parent">
            <summary>
            Gets the root node for the current sub-graph
            </summary>
        </member>
        <member name="T:NeuralNetworkNET.Networks.Implementations.ComputationGraphNetwork">
            <summary>
            A computational graph network with an arbitrary internal structure and number of computation branches
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.Networks.Implementations.ComputationGraphNetwork.InputInfo">
            <inheritdoc/>
        </member>
        <member name="P:NeuralNetworkNET.Networks.Implementations.ComputationGraphNetwork.OutputInfo">
            <inheritdoc/>
        </member>
        <member name="P:NeuralNetworkNET.Networks.Implementations.ComputationGraphNetwork.Layers">
            <inheritdoc/>
        </member>
        <member name="P:NeuralNetworkNET.Networks.Implementations.ComputationGraphNetwork.OutputLayer">
            <inheritdoc/>
        </member>
        <member name="P:NeuralNetworkNET.Networks.Implementations.ComputationGraphNetwork.Size">
            <inheritdoc/>
        </member>
        <member name="F:NeuralNetworkNET.Networks.Implementations.ComputationGraphNetwork.Graph">
            <summary>
            The underlying layers graph for the network
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Implementations.ComputationGraphNetwork.Forward(NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Implementations.ComputationGraphNetwork.Backpropagate(NeuralNetworkNET.SupervisedLearning.Data.SamplesBatch@,System.Single,NeuralNetworkNET.SupervisedLearning.Optimization.WeightsUpdater)">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Implementations.ComputationGraphNetwork.ExtractDeepFeatures(System.Single[])">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Implementations.ComputationGraphNetwork.ExtractDeepFeatures(System.Single[0:,0:])">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Implementations.ComputationGraphNetwork.Serialize(System.IO.Stream)">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Implementations.ComputationGraphNetwork.Deserialize(System.IO.Stream,NeuralNetworkNET.APIs.Enums.ExecutionModePreference)">
            <summary>
            Tries to deserialize a new <see cref="T:NeuralNetworkNET.Networks.Implementations.ComputationGraphNetwork"/> from the input <see cref="T:System.IO.Stream"/>
            </summary>
            <param name="stream">The input <see cref="T:System.IO.Stream"/> to use to read the network data</param>
            <param name="preference">The layers deserialization preference</param>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Implementations.ComputationGraphNetwork.Equals(NeuralNetworkNET.APIs.Interfaces.INeuralNetwork)">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Implementations.ComputationGraphNetwork.Clone">
            <inheritdoc/>
        </member>
        <member name="T:NeuralNetworkNET.Networks.Implementations.NeuralNetworkBase">
            <summary>
            An abstract class used within the library that is the base for all the types of neural networks
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.Networks.Implementations.NeuralNetworkBase.NetworkType">
            <inheritdoc/>
        </member>
        <member name="P:NeuralNetworkNET.Networks.Implementations.NeuralNetworkBase.InputInfo">
            <inheritdoc/>
        </member>
        <member name="P:NeuralNetworkNET.Networks.Implementations.NeuralNetworkBase.OutputInfo">
            <inheritdoc/>
        </member>
        <member name="P:NeuralNetworkNET.Networks.Implementations.NeuralNetworkBase.Layers">
            <inheritdoc/>
        </member>
        <member name="P:NeuralNetworkNET.Networks.Implementations.NeuralNetworkBase.Size">
            <inheritdoc/>
        </member>
        <member name="P:NeuralNetworkNET.Networks.Implementations.NeuralNetworkBase.Parameters">
            <inheritdoc/>
        </member>
        <member name="P:NeuralNetworkNET.Networks.Implementations.NeuralNetworkBase.WeightedLayersIndexes">
            <summary>
            Gets the list of indexes corresponding to layers with weights to update during training
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.Networks.Implementations.NeuralNetworkBase.IsInNumericOverflow">
            <inheritdoc/>
        </member>
        <member name="P:NeuralNetworkNET.Networks.Implementations.NeuralNetworkBase.OutputLayer">
            <summary>
            Gets the output layer of the network, used to compute the cost of a samples batch
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Implementations.NeuralNetworkBase.Forward(System.Single[])">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Implementations.NeuralNetworkBase.Forward(System.Single[0:,0:])">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Implementations.NeuralNetworkBase.ExtractDeepFeatures(System.Single[])">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Implementations.NeuralNetworkBase.ExtractDeepFeatures(System.Single[0:,0:])">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Implementations.NeuralNetworkBase.CalculateCost(System.Single[],System.Single[])">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Implementations.NeuralNetworkBase.CalculateCost(System.Single[0:,0:],System.Single[0:,0:])">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Implementations.NeuralNetworkBase.Forward(NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <summary>
            Forwards the input <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> through the network
            </summary>
            <param name="x">The <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> instance to process</param>
            <param name="yHat">The resulting <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/></param>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Implementations.NeuralNetworkBase.CalculateCost(NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <summary>
            Calculates the cost for the input <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> inputs and expected outputs
            </summary>
            <param name="x">The input <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/></param>
            <param name="y">The expected results</param>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Implementations.NeuralNetworkBase.Backpropagate(NeuralNetworkNET.SupervisedLearning.Data.SamplesBatch@,System.Single,NeuralNetworkNET.SupervisedLearning.Optimization.WeightsUpdater)">
            <summary>
            Calculates the gradient of the cost function with respect to the individual weights and biases
            </summary>
            <param name="batch">The input training batch</param>
            <param name="dropout">The dropout probability for eaach neuron in a <see cref="F:NeuralNetworkNET.APIs.Enums.LayerType.FullyConnected"/> layer</param>
            <param name="updater">The function to use to update the network weights after calculating the gradient</param>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Implementations.NeuralNetworkBase.Evaluate(System.ValueTuple{System.Single[0:,0:],System.Single[0:,0:]})">
            <summary>
            Calculates the current network performances with the given test samples
            </summary>
            <param name="evaluationSet">The inputs and expected outputs to test the network</param>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Implementations.NeuralNetworkBase.Evaluate(NeuralNetworkNET.SupervisedLearning.Data.BatchesCollection)">
            <summary>
            Calculates the current network performances with the given test samples
            </summary>
            <param name="batches">The training batches currently used to train the network</param>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Implementations.NeuralNetworkBase.Evaluate(NeuralNetworkNET.APIs.Interfaces.Data.IDataset)">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Implementations.NeuralNetworkBase.SerializeMetadataAsJson">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Implementations.NeuralNetworkBase.Save(System.IO.FileInfo)">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Implementations.NeuralNetworkBase.Save(System.IO.Stream)">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Implementations.NeuralNetworkBase.Serialize(System.IO.Stream)">
            <summary>
            Writes the current network data to the input <see cref="T:System.IO.Stream"/>
            </summary>
            <param name="stream">The target <see cref="T:System.IO.Stream"/> to use to write the network data</param>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Implementations.NeuralNetworkBase.Equals(NeuralNetworkNET.APIs.Interfaces.INeuralNetwork)">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Implementations.NeuralNetworkBase.Clone">
            <inheritdoc/>
        </member>
        <member name="T:NeuralNetworkNET.Networks.Implementations.SequentialNetwork">
            <summary>
            A complete and fully connected neural network with an arbitrary number of hidden layers
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.Networks.Implementations.SequentialNetwork.InputInfo">
            <inheritdoc/>
        </member>
        <member name="P:NeuralNetworkNET.Networks.Implementations.SequentialNetwork.OutputInfo">
            <inheritdoc/>
        </member>
        <member name="P:NeuralNetworkNET.Networks.Implementations.SequentialNetwork.Size">
            <inheritdoc/>
        </member>
        <member name="P:NeuralNetworkNET.Networks.Implementations.SequentialNetwork.Layers">
            <inheritdoc/>
        </member>
        <member name="P:NeuralNetworkNET.Networks.Implementations.SequentialNetwork.OutputLayer">
            <inheritdoc/>
        </member>
        <member name="F:NeuralNetworkNET.Networks.Implementations.SequentialNetwork._Layers">
            <summary>
            The list of layers that make up the neural network
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Implementations.SequentialNetwork.#ctor(NeuralNetworkNET.APIs.Interfaces.INetworkLayer[])">
            <summary>
            Initializes a new network with the given parameters
            </summary>
            <param name="layers">The layers that make up the neural network</param>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Implementations.SequentialNetwork.ExtractDeepFeatures(System.Single[])">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Implementations.SequentialNetwork.ExtractDeepFeatures(System.Single[0:,0:])">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Implementations.SequentialNetwork.Forward(NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Implementations.SequentialNetwork.Backpropagate(NeuralNetworkNET.SupervisedLearning.Data.SamplesBatch@,System.Single,NeuralNetworkNET.SupervisedLearning.Optimization.WeightsUpdater)">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Implementations.SequentialNetwork.Serialize(System.IO.Stream)">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Implementations.SequentialNetwork.Deserialize(System.IO.Stream,NeuralNetworkNET.APIs.Enums.ExecutionModePreference)">
            <summary>
            Tries to deserialize a new <see cref="T:NeuralNetworkNET.Networks.Implementations.SequentialNetwork"/> from the input <see cref="T:System.IO.Stream"/>
            </summary>
            <param name="stream">The input <see cref="T:System.IO.Stream"/> to use to read the network data</param>
            <param name="preference">The layers deserialization preference</param>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Implementations.SequentialNetwork.Equals(NeuralNetworkNET.APIs.Interfaces.INeuralNetwork)">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Implementations.SequentialNetwork.Clone">
            <inheritdoc/>
        </member>
        <member name="T:NeuralNetworkNET.Networks.Layers.Abstract.BatchNormalizationLayerBase">
            <summary>
            A base claass for a batch normalization layer
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.Networks.Layers.Abstract.BatchNormalizationLayerBase.Mu">
            <summary>
            The cached mu tensor
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.Networks.Layers.Abstract.BatchNormalizationLayerBase.Sigma2">
            <summary>
            The cached sigma^2 tensor
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.Networks.Layers.Abstract.BatchNormalizationLayerBase.Iteration">
            <summary>
            Gets the current iteration number (for the Cumulative Moving Average)
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.Networks.Layers.Abstract.BatchNormalizationLayerBase.CumulativeMovingAverageFactor">
            <summary>
            Gets the current CMA factor used to update the <see cref="P:NeuralNetworkNET.Networks.Layers.Abstract.BatchNormalizationLayerBase.Mu"/> and <see cref="P:NeuralNetworkNET.Networks.Layers.Abstract.BatchNormalizationLayerBase.Sigma2"/> tensors
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.Networks.Layers.Abstract.BatchNormalizationLayerBase.Hash">
            <inheritdoc/>
        </member>
        <member name="P:NeuralNetworkNET.Networks.Layers.Abstract.BatchNormalizationLayerBase.LayerType">
            <inheritdoc/>
        </member>
        <member name="P:NeuralNetworkNET.Networks.Layers.Abstract.BatchNormalizationLayerBase.NormalizationMode">
            <summary>
            Gets the current normalization mode used in the layer
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Abstract.BatchNormalizationLayerBase.Forward(NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Abstract.BatchNormalizationLayerBase.ForwardInference(NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <summary>
            Forwards the inputs through the batch normalization layer during an inference pass
            </summary>
            <param name="x">The input to process</param>
            <param name="z">The output activity on the current layer</param>
            <param name="a">The output activation on the current layer</param>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Abstract.BatchNormalizationLayerBase.ForwardTraining(System.Single,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <summary>
            Forwards the inputs through the batch normalization layer during a training pass, updating the CMA mean and variance <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> instances
            </summary>
            <param name="factor">The factor to use to update the cumulative moving average</param>
            <param name="x">The input to process</param>
            <param name="z">The output activity on the current layer</param>
            <param name="a">The output activation on the current layer</param>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Abstract.BatchNormalizationLayerBase.Equals(NeuralNetworkNET.APIs.Interfaces.INetworkLayer)">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Abstract.BatchNormalizationLayerBase.Serialize(System.IO.Stream)">
            <inheritdoc/>
        </member>
        <member name="T:NeuralNetworkNET.Networks.Layers.Abstract.ConstantLayerBase">
            <summary>
            A base class for a network layer with no parameters to optimize during training
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Abstract.ConstantLayerBase.Backpropagate(NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <summary>
            Backpropagates the error to compute the delta for the inputs of the layer
            </summary>
            <param name="x">The layer inputs used in the forward pass</param>
            <param name="y">The output <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> computed in the forward pass</param>
            <param name="dy">The output error delta to backpropagate</param>
            <param name="dx">The resulting backpropagated error</param>
        </member>
        <member name="T:NeuralNetworkNET.Networks.Layers.Abstract.NetworkLayerBase">
            <summary>
            The base class for all the neural network layer implementations
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.Networks.Layers.Abstract.NetworkLayerBase.LayerType">
            <inheritdoc/>
        </member>
        <member name="P:NeuralNetworkNET.Networks.Layers.Abstract.NetworkLayerBase.InputInfo">
            <inheritdoc/>
        </member>
        <member name="P:NeuralNetworkNET.Networks.Layers.Abstract.NetworkLayerBase.OutputInfo">
            <inheritdoc/>
        </member>
        <member name="P:NeuralNetworkNET.Networks.Layers.Abstract.NetworkLayerBase.ActivationType">
            <summary>
            Gets the activation type used in the current layer
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.Networks.Layers.Abstract.NetworkLayerBase.ActivationFunctions">
            <summary>
            Gets the list of activation and activation prime functions used in the network
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Abstract.NetworkLayerBase.Forward(NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <summary>
            Forwards the inputs through the network layer and returns the resulting activity (Z) and activation (A)
            </summary>
            <param name="x">The input to process</param>
            <param name="z">The output activity on the current layer</param>
            <param name="a">The output activation on the current layer</param>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Abstract.NetworkLayerBase.Equals(NeuralNetworkNET.APIs.Interfaces.INetworkLayer)">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Abstract.NetworkLayerBase.Clone">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Abstract.NetworkLayerBase.Serialize(System.IO.Stream)">
            <summary>
            Writes the current layer to the input <see cref="T:System.IO.Stream"/>
            </summary>
            <param name="stream">The target <see cref="T:System.IO.Stream"/> to use to write the layer data</param>
        </member>
        <member name="T:NeuralNetworkNET.Networks.Layers.Abstract.OutputLayerBase">
            <summary>
            The base class for all the output network layers
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.Networks.Layers.Abstract.OutputLayerBase.CostFunctionType">
            <summary>
            Gets the cost function for the current layer
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.Networks.Layers.Abstract.OutputLayerBase.CostFunctions">
            <summary>
            Gets the cost function implementations used in the current layer
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Abstract.OutputLayerBase.Backpropagate(NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <summary>
            Computes the output delta, with respect to the cost function of the network
            </summary>
            <param name="x">The layer inputs in the forward pass</param>
            <param name="yHat">The estimated outputs for the network</param>
            <param name="y">The expected outputs for the used inputs</param>
            <param name="z">The activity on the output layer</param>
            <param name="dx">The backpropagated error</param>
            <param name="dJdw">The resulting gradient with respect to the weights</param>
            <param name="dJdb">The resulting gradient with respect to the biases</param>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Abstract.OutputLayerBase.CalculateCost(NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <summary>
            Calculates the output cost with respect to the cost function currently in use
            </summary>
            <param name="yHat">The estimated output for the network</param>
            <param name="y">The Expected outputs for the inputs used</param>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Abstract.OutputLayerBase.Equals(NeuralNetworkNET.APIs.Interfaces.INetworkLayer)">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Abstract.OutputLayerBase.Serialize(System.IO.Stream)">
            <inheritdoc/>
        </member>
        <member name="T:NeuralNetworkNET.Networks.Layers.Abstract.WeightedLayerBase">
            <summary>
            The base class for all the network layers that have weights and biases as parameters
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.Networks.Layers.Abstract.WeightedLayerBase.Hash">
            <summary>
            Gets an SHA256 hash calculated on both the weights and biases of the layer
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.Networks.Layers.Abstract.WeightedLayerBase.Weights">
            <summary>
            Gets the weights for the current network layer
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.Networks.Layers.Abstract.WeightedLayerBase.Biases">
            <summary>
            Gets the biases for the current network layer
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Abstract.WeightedLayerBase.Backpropagate(NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <summary>
            Backpropagates the error to compute the delta for the inputs of the layer
            </summary>
            <param name="x">The layer inputs used in the forward pass</param>
            <param name="y">The output <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> computed in the forward pass</param>
            <param name="dy">The output error delta to backpropagate</param>
            <param name="dx">The resulting backpropagated error</param>
            <param name="dJdw">The resulting gradient with respect to the weights</param>
            <param name="dJdb">The resulting gradient with respect to the biases</param>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Abstract.WeightedLayerBase.ValidateWeights">
            <summary>
            Checks whether or not all the weights in the current layer are valid and the layer can be safely used
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Abstract.WeightedLayerBase.Equals(NeuralNetworkNET.APIs.Interfaces.INetworkLayer)">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Abstract.WeightedLayerBase.Serialize(System.IO.Stream)">
            <inheritdoc/>
        </member>
        <member name="T:NeuralNetworkNET.Networks.Layers.Cpu.BatchNormalizationLayer">
            <summary>
            A batch normalization layer, used to improve the convergence speed of a neural network
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Cpu.BatchNormalizationLayer.ForwardInference(NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Cpu.BatchNormalizationLayer.ForwardTraining(System.Single,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Cpu.BatchNormalizationLayer.Backpropagate(NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Cpu.BatchNormalizationLayer.Deserialize(System.IO.Stream)">
            <summary>
            Tries to deserialize a new <see cref="T:NeuralNetworkNET.Networks.Layers.Cpu.BatchNormalizationLayer"/> from the input <see cref="T:System.IO.Stream"/>
            </summary>
            <param name="stream">The input <see cref="T:System.IO.Stream"/> to use to read the layer data</param>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Cpu.BatchNormalizationLayer.Clone">
            <inheritdoc/>
        </member>
        <member name="T:NeuralNetworkNET.Networks.Layers.Cpu.ConvolutionalLayer">
            <summary>
            A convolutional layer, used in a CNN network
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.Networks.Layers.Cpu.ConvolutionalLayer.LayerType">
            <inheritdoc/>
        </member>
        <member name="P:NeuralNetworkNET.Networks.Layers.Cpu.ConvolutionalLayer.OperationInfo">
            <summary>
            Gets the info on the convolution operation performed by the layer
            </summary>    
        </member>
        <member name="P:NeuralNetworkNET.Networks.Layers.Cpu.ConvolutionalLayer.KernelInfo">
            <summary>
            Gets the <see cref="T:NeuralNetworkNET.APIs.Structs.TensorInfo"/> associated with each kernel in the layer
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.Networks.Layers.Cpu.ConvolutionalLayer.Kernels">
            <summary>
            Gets the number of kernels in the current layer
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Cpu.ConvolutionalLayer.Forward(NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Cpu.ConvolutionalLayer.Backpropagate(NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Cpu.ConvolutionalLayer.Equals(NeuralNetworkNET.APIs.Interfaces.INetworkLayer)">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Cpu.ConvolutionalLayer.Clone">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Cpu.ConvolutionalLayer.Serialize(System.IO.Stream)">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Cpu.ConvolutionalLayer.Deserialize(System.IO.Stream)">
            <summary>
            Tries to deserialize a new <see cref="T:NeuralNetworkNET.Networks.Layers.Cpu.ConvolutionalLayer"/> from the input <see cref="T:System.IO.Stream"/>
            </summary>
            <param name="stream">The input <see cref="T:System.IO.Stream"/> to use to read the layer data</param>
        </member>
        <member name="T:NeuralNetworkNET.Networks.Layers.Cpu.FullyConnectedLayer">
            <summary>
            A fully connected (dense) network layer
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.Networks.Layers.Cpu.FullyConnectedLayer.LayerType">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Cpu.FullyConnectedLayer.Forward(NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Cpu.FullyConnectedLayer.Backpropagate(NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Cpu.FullyConnectedLayer.Clone">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Cpu.FullyConnectedLayer.Deserialize(System.IO.Stream)">
            <summary>
            Tries to deserialize a new <see cref="T:NeuralNetworkNET.Networks.Layers.Cpu.FullyConnectedLayer"/> from the input <see cref="T:System.IO.Stream"/>
            </summary>
            <param name="stream">The input <see cref="T:System.IO.Stream"/> to use to read the layer data</param>
        </member>
        <member name="T:NeuralNetworkNET.Networks.Layers.Cpu.OutputLayer">
            <summary>
            An output layer with a variable cost function
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.Networks.Layers.Cpu.OutputLayer.LayerType">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Cpu.OutputLayer.Clone">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Cpu.OutputLayer.Deserialize(System.IO.Stream)">
            <summary>
            Tries to deserialize a new <see cref="T:NeuralNetworkNET.Networks.Layers.Cpu.OutputLayer"/> from the input <see cref="T:System.IO.Stream"/>
            </summary>
            <param name="stream">The input <see cref="T:System.IO.Stream"/> to use to read the layer data</param>
        </member>
        <member name="T:NeuralNetworkNET.Networks.Layers.Cpu.PoolingLayer">
            <summary>
            A pooling layer, with a 2x2 window and a stride of 2
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.Networks.Layers.Cpu.PoolingLayer.LayerType">
            <inheritdoc/>
        </member>
        <member name="P:NeuralNetworkNET.Networks.Layers.Cpu.PoolingLayer.OperationInfo">
            <summary>
            Gets the info on the pooling operation performed by the layer
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Cpu.PoolingLayer.Forward(NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Cpu.PoolingLayer.Backpropagate(NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Cpu.PoolingLayer.Clone">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Cpu.PoolingLayer.Serialize(System.IO.Stream)">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Cpu.PoolingLayer.Deserialize(System.IO.Stream)">
            <summary>
            Tries to deserialize a new <see cref="T:NeuralNetworkNET.Networks.Layers.Cpu.PoolingLayer"/> from the input <see cref="T:System.IO.Stream"/>
            </summary>
            <param name="stream">The input <see cref="T:System.IO.Stream"/> to use to read the layer data</param>
        </member>
        <member name="T:NeuralNetworkNET.Networks.Layers.Cpu.SoftmaxLayer">
            <summary>
            An output layer with the softmax activation function and log-likelyhood cost function
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.Networks.Layers.Cpu.SoftmaxLayer.LayerType">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Cpu.SoftmaxLayer.Forward(NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Cpu.SoftmaxLayer.Clone">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Cpu.SoftmaxLayer.Deserialize(System.IO.Stream)">
            <summary>
            Tries to deserialize a new <see cref="T:NeuralNetworkNET.Networks.Layers.Cpu.SoftmaxLayer"/> from the input <see cref="T:System.IO.Stream"/>
            </summary>
            <param name="stream">The input <see cref="T:System.IO.Stream"/> to use to read the layer data</param>
        </member>
        <member name="T:NeuralNetworkNET.Networks.Layers.Cuda.CuDnnBatchNormalizationLayer">
            <summary>
            A cuDNN-powered batch normalization layer
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.Networks.Layers.Cuda.CuDnnBatchNormalizationLayer.DnnInstance">
            <summary>
            Gets the <see cref="T:Alea.cuDNN.Dnn"/> instance for the current layer
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Cuda.CuDnnBatchNormalizationLayer.ForwardInference(NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Cuda.CuDnnBatchNormalizationLayer.ForwardTraining(System.Single,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Cuda.CuDnnBatchNormalizationLayer.Backpropagate(NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Cuda.CuDnnBatchNormalizationLayer.Deserialize(System.IO.Stream)">
            <summary>
            Tries to deserialize a new <see cref="T:NeuralNetworkNET.Networks.Layers.Cuda.CuDnnBatchNormalizationLayer"/> from the input <see cref="T:System.IO.Stream"/>
            </summary>
            <param name="stream">The input <see cref="T:System.IO.Stream"/> to use to read the layer data</param>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Cuda.CuDnnBatchNormalizationLayer.Clone">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Cuda.CuDnnBatchNormalizationLayer.System#IDisposable#Dispose">
            <inheritdoc/>
        </member>
        <member name="T:NeuralNetworkNET.Networks.Layers.Cuda.CuDnnConvolutionalLayer">
            <summary>
            A 2D convolutional layer based on the cuDNN back-end
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.Networks.Layers.Cuda.CuDnnConvolutionalLayer.DnnInstance">
            <summary>
            Gets the <see cref="T:Alea.cuDNN.Dnn"/> instance for the current layer
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Cuda.CuDnnConvolutionalLayer.Forward(NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Cuda.CuDnnConvolutionalLayer.Backpropagate(NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Cuda.CuDnnConvolutionalLayer.Deserialize(System.IO.Stream)">
            <summary>
            Tries to deserialize a new <see cref="T:NeuralNetworkNET.Networks.Layers.Cuda.CuDnnConvolutionalLayer"/> from the input <see cref="T:System.IO.Stream"/>
            </summary>
            <param name="stream">The input <see cref="T:System.IO.Stream"/> to use to read the layer data</param>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Cuda.CuDnnConvolutionalLayer.Clone">
            <inheritdoc/>
        </member>
        <member name="T:NeuralNetworkNET.Networks.Layers.Cuda.CuDnnFullyConnectedLayer">
            <summary>
            A cuDNN-powered fully connected layer
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.Networks.Layers.Cuda.CuDnnFullyConnectedLayer.DnnInstance">
            <summary>
            Gets the <see cref="T:Alea.cuDNN.Dnn"/> instance for the current layer
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Cuda.CuDnnFullyConnectedLayer.Forward(NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Cuda.CuDnnFullyConnectedLayer.Backpropagate(NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Cuda.CuDnnFullyConnectedLayer.Deserialize(System.IO.Stream)">
            <summary>
            Tries to deserialize a new <see cref="T:NeuralNetworkNET.Networks.Layers.Cuda.CuDnnFullyConnectedLayer"/> from the input <see cref="T:System.IO.Stream"/>
            </summary>
            <param name="stream">The input <see cref="T:System.IO.Stream"/> to use to read the layer data</param>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Cuda.CuDnnFullyConnectedLayer.Clone">
            <inheritdoc/>
        </member>
        <member name="T:NeuralNetworkNET.Networks.Layers.Cuda.CuDnnInceptionLayer">
            <summary>
            An inception module with 4 pipelines combining 1x1 convolution, 1x1 + 3x3, 1x1 + 5x5 and pooling + 1x1, see <a href="https://arxiv.org/pdf/1409.4842.pdf">arxiv.org/pdf/1409.4842.pdf</a>
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.Networks.Layers.Cuda.CuDnnInceptionLayer.LayerType">
            <sinheritdoc/>
        </member>
        <member name="P:NeuralNetworkNET.Networks.Layers.Cuda.CuDnnInceptionLayer.OperationInfo">
            <summary>
            Gets the info on the inception parameters used by the layer
            </summary>    
        </member>
        <member name="F:NeuralNetworkNET.Networks.Layers.Cuda.CuDnnInceptionLayer.DnnInstance">
            <summary>
            Gets the <see cref="T:Alea.cuDNN.Dnn"/> instance for the current layer
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Cuda.CuDnnInceptionLayer.Forward(NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Cuda.CuDnnInceptionLayer.Backpropagate(NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Cuda.CuDnnInceptionLayer.Serialize(System.IO.Stream)">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Cuda.CuDnnInceptionLayer.Deserialize(System.IO.Stream)">
            <summary>
            Tries to deserialize a new <see cref="T:NeuralNetworkNET.Networks.Layers.Cuda.CuDnnInceptionLayer"/> from the input <see cref="T:System.IO.Stream"/>
            </summary>
            <param name="stream">The input <see cref="T:System.IO.Stream"/> to use to read the layer data</param>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Cuda.CuDnnInceptionLayer.Clone">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Cuda.CuDnnInceptionLayer.System#IDisposable#Dispose">
            <inheritdoc/>
        </member>
        <member name="T:NeuralNetworkNET.Networks.Layers.Cuda.CuDnnPoolingLayer">
            <summary>
            A pooling layer running on cuDNN, with a custom pooling mode
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.Networks.Layers.Cuda.CuDnnPoolingLayer.DnnInstance">
            <summary>
            Gets the <see cref="T:Alea.cuDNN.Dnn"/> instance for the current layer
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Cuda.CuDnnPoolingLayer.Forward(NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Cuda.CuDnnPoolingLayer.Backpropagate(NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Cuda.CuDnnPoolingLayer.Deserialize(System.IO.Stream)">
            <summary>
            Tries to deserialize a new <see cref="T:NeuralNetworkNET.Networks.Layers.Cuda.CuDnnPoolingLayer"/> from the input <see cref="T:System.IO.Stream"/>
            </summary>
            <param name="stream">The input <see cref="T:System.IO.Stream"/> to use to read the layer data</param>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Cuda.CuDnnPoolingLayer.Clone">
            <inheritdoc/>
        </member>
        <member name="T:NeuralNetworkNET.Networks.Layers.Cuda.CuDnnSoftmaxLayer">
            <summary>
            A softmax output layer based on the cuDNN back-end
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.Networks.Layers.Cuda.CuDnnSoftmaxLayer.LayerType">
            <inheritdoc/>
        </member>
        <member name="F:NeuralNetworkNET.Networks.Layers.Cuda.CuDnnSoftmaxLayer.DnnInstance">
            <summary>
            Gets the <see cref="T:Alea.cuDNN.Dnn"/> instance for the current layer
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Cuda.CuDnnSoftmaxLayer.Forward(NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Cuda.CuDnnSoftmaxLayer.Backpropagate(NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@,NeuralNetworkNET.APIs.Structs.Tensor@)">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Cuda.CuDnnSoftmaxLayer.Deserialize(System.IO.Stream)">
            <summary>
            Tries to deserialize a new <see cref="T:NeuralNetworkNET.Networks.Layers.Cuda.CuDnnSoftmaxLayer"/> from the input <see cref="T:System.IO.Stream"/>
            </summary>
            <param name="stream">The input <see cref="T:Alea.Stream"/> to use to read the layer data</param>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Cuda.CuDnnSoftmaxLayer.Clone">
            <inheritdoc/>
        </member>
        <member name="T:NeuralNetworkNET.Networks.Layers.Initialization.KerasWeightsProvider">
            <summary>
            A static class with some weights initialization methods ported over the Keras library
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Initialization.KerasWeightsProvider.FillWithLeCunUniform(NeuralNetworkNET.APIs.Structs.Tensor@,System.Int32)">
            <summary>
            Fills the target <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> with values from the LeCun uniform distribution
            </summary>
            <param name="tensor">The target <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> to fill</param>
            <param name="fanIn">The input neurons</param>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Initialization.KerasWeightsProvider.FillWithGlorotNormal(NeuralNetworkNET.APIs.Structs.Tensor@,System.Int32,System.Int32)">
            <summary>
            Fills the target <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> with values from the Glorot &amp; Bengio normal distribution
            </summary>
            <param name="tensor">The target <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> to fill</param>
            <param name="fanIn">The input neurons</param>
            <param name="fanOut">The output neurons</param>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Initialization.KerasWeightsProvider.FillWithGlorotUniform(NeuralNetworkNET.APIs.Structs.Tensor@,System.Int32,System.Int32)">
            <summary>
            Fills the target <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> with values from the Glorot &amp; Bengio uniform distribution
            </summary>
            <param name="tensor">The target <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> to fill</param>
            <param name="fanIn">The input neurons</param>
            <param name="fanOut">The output neurons</param>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Initialization.KerasWeightsProvider.FillWithHeEtAlNormal(NeuralNetworkNET.APIs.Structs.Tensor@,System.Int32)">
            <summary>
            Fills the target <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> with values from the He et al. normal distribution
            </summary>
            <param name="tensor">The target <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> to fill</param>
            <param name="fanIn">The input neurons</param>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Initialization.KerasWeightsProvider.FillWithHeEtAlUniform(NeuralNetworkNET.APIs.Structs.Tensor@,System.Int32)">
            <summary>
            Fills the target <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> with values from the He et al. uniform distribution
            </summary>
            <param name="tensor">The target <see cref="T:NeuralNetworkNET.APIs.Structs.Tensor"/> to fill</param>
            <param name="fanIn">The input neurons</param>
        </member>
        <member name="T:NeuralNetworkNET.Networks.Layers.Initialization.WeightsProvider">
            <summary>
            A simple class that provides some helper methods to quickly initialize the weights of a neural network layer
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Initialization.WeightsProvider.NewFullyConnectedWeights(NeuralNetworkNET.APIs.Structs.TensorInfo@,System.Int32,NeuralNetworkNET.APIs.Enums.WeightsInitializationMode)">
            <summary>
            Creates a weights vector for a fully connected layer
            </summary>
            <param name="input">The layer inputs</param>
            <param name="outputs">The output neurons</param>
            <param name="mode">The initialization mode for the weights</param>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Initialization.WeightsProvider.NewConvolutionalKernels(NeuralNetworkNET.APIs.Structs.TensorInfo@,System.Int32,System.Int32,System.Int32)">
            <summary>
            Creates a weights vector for a convolutional layer
            </summary>
            <param name="input">The layer inputs</param>
            <param name="kernelsHeight">The height of each kernel</param>
            <param name="kernelsWidth">The width of each kernel</param>
            <param name="kernels">The number of kernels in the layer</param>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Initialization.WeightsProvider.NewInceptionWeights(NeuralNetworkNET.APIs.Structs.TensorInfo@,NeuralNetworkNET.APIs.Structs.InceptionInfo@)">
            <summary>
            Creates a new mixed weights vector for an inception layer
            </summary>
            <param name="input">The layer inputs</param>
            <param name="info">The info on the target inception layer</param>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Initialization.WeightsProvider.NewBiases(System.Int32,NeuralNetworkNET.APIs.Enums.BiasInitializationMode)">
            <summary>
            Creates a vector of biases for a network layer
            </summary>
            <param name="length">The length of the vector</param>
            <param name="mode">The initialization mode for the bias values</param>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Initialization.WeightsProvider.NewGammaParameters(NeuralNetworkNET.APIs.Structs.TensorInfo@,NeuralNetworkNET.APIs.Enums.NormalizationMode)">
            <summary>
            Creates a new weights vector for a batch normalization layer
            </summary>
            <param name="shape">The layer inputs and ouputs</param>
            <param name="mode">The normalization mode to use</param>
        </member>
        <member name="M:NeuralNetworkNET.Networks.Layers.Initialization.WeightsProvider.NewBetaParameters(NeuralNetworkNET.APIs.Structs.TensorInfo@,NeuralNetworkNET.APIs.Enums.NormalizationMode)">
            <summary>
            Creates a new beta weights vector for a batch normalization layer
            </summary>
            <param name="shape">The layer inputs and ouputs</param>
            <param name="mode">The normalization mode to use</param>
        </member>
        <member name="T:NeuralNetworkNET.Services.SharedEventsService">
            <summary>
            A static class with events that signal different status changes for the library
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.Services.SharedEventsService.TrainingStarting">
            <summary>
            An <see cref="T:System.Action"/> that is executed right before the training starts on a network
            </summary>
        </member>
        <member name="T:NeuralNetworkNET.SupervisedLearning.Algorithms.Info.AdaDeltaInfo">
            <summary>
            A class containing all the info needed to use the <see cref="F:NeuralNetworkNET.SupervisedLearning.Algorithms.TrainingAlgorithmType.AdaDelta"/> algorithm
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.SupervisedLearning.Algorithms.Info.AdaDeltaInfo.AlgorithmType">
            <inheritdoc/>
        </member>
        <member name="P:NeuralNetworkNET.SupervisedLearning.Algorithms.Info.AdaDeltaInfo.Rho">
            <summary>
            Gets the AdaDelta rho parameter
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.SupervisedLearning.Algorithms.Info.AdaDeltaInfo.Epsilon">
            <summary>
            Gets the AdaDelta epsilon parameter
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.SupervisedLearning.Algorithms.Info.AdaDeltaInfo.L2">
            <summary>
            Gets the L2 regularization parameter
            </summary>
        </member>
        <member name="T:NeuralNetworkNET.SupervisedLearning.Algorithms.Info.AdaGradInfo">
            <summary>
            A class containing all the info needed to use the <see cref="F:NeuralNetworkNET.SupervisedLearning.Algorithms.TrainingAlgorithmType.AdaGrad"/> algorithm
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.SupervisedLearning.Algorithms.Info.AdaGradInfo.AlgorithmType">
            <inheritdoc/>
        </member>
        <member name="P:NeuralNetworkNET.SupervisedLearning.Algorithms.Info.AdaGradInfo.Eta">
            <summary>
            Gets the current learning rate
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.SupervisedLearning.Algorithms.Info.AdaGradInfo.Lambda">
            <summary>
            Gets the lambda regularization parameter
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.SupervisedLearning.Algorithms.Info.AdaGradInfo.Epsilon">
            <summary>
            Gets the AdaGrad epsilon parameter
            </summary>
        </member>
        <member name="T:NeuralNetworkNET.SupervisedLearning.Algorithms.Info.AdaMaxInfo">
            <summary>
            A class containing all the info needed to use the <see cref="F:NeuralNetworkNET.SupervisedLearning.Algorithms.TrainingAlgorithmType.AdaMax"/> algorithm
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.SupervisedLearning.Algorithms.Info.AdaMaxInfo.AlgorithmType">
            <inheritdoc/>
        </member>
        <member name="P:NeuralNetworkNET.SupervisedLearning.Algorithms.Info.AdaMaxInfo.Eta">
            <summary>
            Gets the learning rate factor
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.SupervisedLearning.Algorithms.Info.AdaMaxInfo.Beta1">
            <summary>
            Gets the beta1 factor
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.SupervisedLearning.Algorithms.Info.AdaMaxInfo.Beta2">
            <summary>
            Gets the beta2 factor
            </summary>
        </member>
        <member name="T:NeuralNetworkNET.SupervisedLearning.Algorithms.Info.AdamInfo">
            <summary>
            A class containing all the info needed to use the <see cref="F:NeuralNetworkNET.SupervisedLearning.Algorithms.TrainingAlgorithmType.Adam"/> algorithm
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.SupervisedLearning.Algorithms.Info.AdamInfo.AlgorithmType">
            <inheritdoc/>
        </member>
        <member name="P:NeuralNetworkNET.SupervisedLearning.Algorithms.Info.AdamInfo.Eta">
            <summary>
            Gets the learning rate factor
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.SupervisedLearning.Algorithms.Info.AdamInfo.Beta1">
            <summary>
            Gets the beta1 factor
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.SupervisedLearning.Algorithms.Info.AdamInfo.Beta2">
            <summary>
            Gets the beta2 factor
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.SupervisedLearning.Algorithms.Info.AdamInfo.Epsilon">
            <summary>
            Gets the Adam epsilon parameter
            </summary>
        </member>
        <member name="T:NeuralNetworkNET.SupervisedLearning.Algorithms.Info.MomentumInfo">
            <summary>
            A class containing all the info needed to use the <see cref="F:NeuralNetworkNET.SupervisedLearning.Algorithms.TrainingAlgorithmType.Momentum"/> algorithm
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.SupervisedLearning.Algorithms.Info.MomentumInfo.AlgorithmType">
            <inheritdoc/>
        </member>
        <member name="P:NeuralNetworkNET.SupervisedLearning.Algorithms.Info.MomentumInfo.Momentum">
            <summary>
            Gets the current momentum
            </summary>
        </member>
        <member name="T:NeuralNetworkNET.SupervisedLearning.Algorithms.Info.RMSPropInfo">
            <summary>
            A class containing all the info needed to use the <see cref="F:NeuralNetworkNET.SupervisedLearning.Algorithms.TrainingAlgorithmType.RMSProp"/> algorithm
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.SupervisedLearning.Algorithms.Info.RMSPropInfo.AlgorithmType">
            <inheritdoc/>
        </member>
        <member name="P:NeuralNetworkNET.SupervisedLearning.Algorithms.Info.RMSPropInfo.Eta">
            <summary>
            Gets the current learning rate
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.SupervisedLearning.Algorithms.Info.RMSPropInfo.Rho">
            <summary>
            Gets the RMSProp rho parameter
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.SupervisedLearning.Algorithms.Info.RMSPropInfo.Lambda">
            <summary>
            Gets the lambda regularization parameter
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.SupervisedLearning.Algorithms.Info.RMSPropInfo.Epsilon">
            <summary>
            Gets the RMSProp epsilon parameter
            </summary>
        </member>
        <member name="T:NeuralNetworkNET.SupervisedLearning.Algorithms.Info.StochasticGradientDescentInfo">
            <summary>
            A class containing all the info needed to use the <see cref="F:NeuralNetworkNET.SupervisedLearning.Algorithms.TrainingAlgorithmType.StochasticGradientDescent"/> algorithm
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.SupervisedLearning.Algorithms.Info.StochasticGradientDescentInfo.AlgorithmType">
            <inheritdoc/>
        </member>
        <member name="P:NeuralNetworkNET.SupervisedLearning.Algorithms.Info.StochasticGradientDescentInfo.Eta">
            <summary>
            Gets the current learning rate
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.SupervisedLearning.Algorithms.Info.StochasticGradientDescentInfo.Lambda">
            <summary>
            Gets the lambda regularization parameter
            </summary>
        </member>
        <member name="T:NeuralNetworkNET.SupervisedLearning.Algorithms.TrainingAlgorithmType">
            <summary>
            An <see langword="enum"/> that indicates a supervised learning training algorithm
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.SupervisedLearning.Algorithms.TrainingAlgorithmType.StochasticGradientDescent">
            <summary>
            The plain stochastic gradient descent training algorithm
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.SupervisedLearning.Algorithms.TrainingAlgorithmType.Momentum">
            <summary>
            A variant of the stochastic gradient descent algorithm with momentum
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.SupervisedLearning.Algorithms.TrainingAlgorithmType.AdaGrad">
            <summary>
            The AdaGrad learning method, by John Duchi, Elad Hazan and Yoram Singer, see <a href="http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf">jmlr.org/papers/volume12/duchi11a/duchi11a.pdf</a>
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.SupervisedLearning.Algorithms.TrainingAlgorithmType.AdaDelta">
            <summary>
            The AdaDelta adaptive learning method, by Matthew D. Zeiler, see <a href="https://arxiv.org/abs/1212.5701">arxiv.org/abs/1212.5701</a>
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.SupervisedLearning.Algorithms.TrainingAlgorithmType.Adam">
            <summary>
            The Adam learning method, by Diederik P. Kingma and Jimmy Lei Ba, see <a href="https://arxiv.org/pdf/1412.6980v8.pdf">arxiv.org/pdf/1412.6980v8.pdf</a>
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.SupervisedLearning.Algorithms.TrainingAlgorithmType.AdaMax">
            <summary>
            The AdaMax learning method, by Diederik P. Kingma and Jimmy Lei Ba, see section 7.1 of <a href="https://arxiv.org/pdf/1412.6980v8.pdf">arxiv.org/pdf/1412.6980v8.pdf</a>
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.SupervisedLearning.Algorithms.TrainingAlgorithmType.RMSProp">
            <summary>
            The RMSProp learning method, see <a href="http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf">cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf</a>
            </summary>
        </member>
        <member name="T:NeuralNetworkNET.SupervisedLearning.Data.BatchesCollection">
            <summary>
            A class that represents a set of samples batches to be used in circular order
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.SupervisedLearning.Data.BatchesCollection.Batches">
            <summary>
            Gets the collection of samples batches to use
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.SupervisedLearning.Data.BatchesCollection.Count">
            <inheritdoc/>
        </member>
        <member name="P:NeuralNetworkNET.SupervisedLearning.Data.BatchesCollection.BatchesCount">
            <inheritdoc/>
        </member>
        <member name="P:NeuralNetworkNET.SupervisedLearning.Data.BatchesCollection.InputFeatures">
            <inheritdoc/>
        </member>
        <member name="P:NeuralNetworkNET.SupervisedLearning.Data.BatchesCollection.OutputFeatures">
            <inheritdoc/>
        </member>
        <member name="P:NeuralNetworkNET.SupervisedLearning.Data.BatchesCollection.Item(System.Int32)">
            <inheritdoc/>
        </member>
        <member name="P:NeuralNetworkNET.SupervisedLearning.Data.BatchesCollection.BatchSize">
            <inheritdoc/>
        </member>
        <member name="P:NeuralNetworkNET.SupervisedLearning.Data.BatchesCollection.ByteSize">
            <inheritdoc/>
        </member>
        <member name="P:NeuralNetworkNET.SupervisedLearning.Data.BatchesCollection.Id">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.SupervisedLearning.Data.BatchesCollection.Expand(System.Func{System.Single[],System.Single[]}[])">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.SupervisedLearning.Data.BatchesCollection.PartitionWithTest(System.Single,System.Action{NeuralNetworkNET.SupervisedLearning.Progress.TrainingProgressEventArgs})">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.SupervisedLearning.Data.BatchesCollection.PartitionWithValidation(System.Single,System.Single,System.Int32)">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.SupervisedLearning.Data.BatchesCollection.ExtractTest(System.Single,System.Action{NeuralNetworkNET.SupervisedLearning.Progress.TrainingProgressEventArgs})">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.SupervisedLearning.Data.BatchesCollection.ExtractValidation(System.Single,System.Single,System.Int32)">
            <inheritdoc/>
        </member>
        <member name="M:NeuralNetworkNET.SupervisedLearning.Data.BatchesCollection.From(System.ValueTuple{System.Single[0:,0:],System.Single[0:,0:]},System.Int32)">
            <summary>
            Creates a series of batches from the input dataset and expected results
            </summary>
            <param name="dataset">The source dataset to create the batches</param>
            <param name="size">The desired batch size</param>
            <exception cref="T:System.ArgumentOutOfRangeException">The dataset and result matrices have a different number of rows</exception>
        </member>
        <member name="M:NeuralNetworkNET.SupervisedLearning.Data.BatchesCollection.From(System.Collections.Generic.IEnumerable{System.Func{System.ValueTuple{System.Single[],System.Single[]}}},System.Int32)">
            <summary>
            Creates a series of batches from the input dataset and expected results
            </summary>
            <param name="dataset">The source dataset to create the batches</param>
            <param name="size">The desired batch size</param>
            <exception cref="T:System.ArgumentOutOfRangeException">The dataset and result matrices have a different number of rows</exception>
        </member>
        <member name="M:NeuralNetworkNET.SupervisedLearning.Data.BatchesCollection.From(System.Collections.Generic.IEnumerable{System.ValueTuple{System.Single[],System.Single[]}},System.Int32)">
            <summary>
            Creates a series of batches from the input dataset and expected results
            </summary>
            <param name="dataset">The source dataset to create the batches</param>
            <param name="size">The desired batch size</param>
            <exception cref="T:System.ArgumentOutOfRangeException">The dataset and result matrices have a different number of rows</exception>
        </member>
        <member name="M:NeuralNetworkNET.SupervisedLearning.Data.BatchesCollection.CrossShuffle">
            <summary>
            Cross-shuffles the current dataset (shuffles samples in each batch, then shuffles the batches list)
            </summary>
        </member>
        <member name="T:NeuralNetworkNET.SupervisedLearning.Data.Pin`1">
            <summary>
            A simple <see langword="struct"/> that holds a reference to a pinned target.
            It can be used to replace <see cref="T:System.Span`1"/> to quickly pass pointers around in non stack-only methods.
            </summary>
            <typeparam name="T">The type of the target referenced by the exposed pointer</typeparam>
        </member>
        <member name="F:NeuralNetworkNET.SupervisedLearning.Data.Pin`1.Ptr">
            <summary>
            The pinned pointer
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.SupervisedLearning.Data.Pin`1.Ref">
            <summary>
            Gets a managed reference for the pointer target
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.SupervisedLearning.Data.Pin`1.As``1">
            <summary>
            Gets a managed reference of the input type, using an unsafe cast
            </summary>
            <typeparam name="TTo">The target reference type to return</typeparam>
        </member>
        <member name="M:NeuralNetworkNET.SupervisedLearning.Data.Pin`1.From(`0@)">
            <summary>
            Creates a new instance from the input reference
            </summary>
            <param name="value">The target reference</param>
        </member>
        <member name="T:NeuralNetworkNET.SupervisedLearning.Data.SamplesBatch">
            <summary>
            A simple struct that keeps a reference of a training set and its expected results
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.SupervisedLearning.Data.SamplesBatch.X">
            <summary>
            Gets the current dataset
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.SupervisedLearning.Data.SamplesBatch.Y">
            <summary>
            Gets the expected results for the current dataset
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.SupervisedLearning.Data.SamplesBatch.#ctor(System.Single[0:,0:],System.Single[0:,0:])">
            <summary>
            Creates a new dataset wrapper batch with the given data
            </summary>
            <param name="x">The batch data</param>
            <param name="y">The batch expected results</param>
        </member>
        <member name="M:NeuralNetworkNET.SupervisedLearning.Data.SamplesBatch.From(System.Span{System.Single},System.Span{System.Single},System.Int32,System.Int32)">
            <summary>
            Creates a new instance from the input partition
            </summary>
            <param name="x">A <see cref="T:System.Span`1"/> representing the input samples partition</param>
            <param name="y">A <see cref="T:System.Span`1"/> representing the output samples partition</param>
            <param name="inputs">The number of input features</param>
            <param name="outputs">The number of output features</param>
        </member>
        <member name="M:NeuralNetworkNET.SupervisedLearning.Data.SamplesBatch.From(System.Collections.Generic.IReadOnlyList{System.ValueTuple{System.Single[],System.Single[]}})">
            <summary>
            Creates a new instance from the input partition
            </summary>
            <param name="batch">The source batch</param>
        </member>
        <member name="M:NeuralNetworkNET.SupervisedLearning.Data.SamplesBatch.From(System.Collections.Generic.IReadOnlyList{System.ValueTuple{NeuralNetworkNET.SupervisedLearning.Data.Pin{System.Single},NeuralNetworkNET.SupervisedLearning.Data.Pin{System.Single}}},System.Int32,System.Int32)">
            <summary>
            Creates a new instance from the input partition
            </summary>
            <param name="batch">The source batch</param>
            <param name="inputs">The number of input features</param>
            <param name="outputs">The number of output features</param>
        </member>
        <member name="T:NeuralNetworkNET.SupervisedLearning.Optimization.NetworkTrainer">
            <summary>
            A static class that contains various network optimization algorithms
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.SupervisedLearning.Optimization.NetworkTrainer.TrainNetwork(NeuralNetworkNET.Networks.Implementations.NeuralNetworkBase,NeuralNetworkNET.SupervisedLearning.Data.BatchesCollection,System.Int32,System.Single,NeuralNetworkNET.APIs.Interfaces.ITrainingAlgorithmInfo,System.IProgress{NeuralNetworkNET.SupervisedLearning.Progress.BatchProgress},System.IProgress{NeuralNetworkNET.SupervisedLearning.Progress.TrainingProgressEventArgs},NeuralNetworkNET.SupervisedLearning.Parameters.ValidationDataset,NeuralNetworkNET.SupervisedLearning.Parameters.TestDataset,System.Threading.CancellationToken)">
            <summary>
            Trains the target <see cref="T:NeuralNetworkNET.Networks.Implementations.NeuralNetworkBase"/> instance with the given parameters and data
            </summary>
            <param name="network">The target <see cref="T:NeuralNetworkNET.Networks.Implementations.NeuralNetworkBase"/> to train</param>
            <param name="batches">The training baatches for the current session</param>
            <param name="epochs">The desired number of training epochs to run</param>
            <param name="dropout">Indicates the dropout probability for neurons in a <see cref="F:NeuralNetworkNET.APIs.Enums.LayerType.FullyConnected"/> layer</param>
            <param name="algorithm">The info on the training algorithm to use</param>
            <param name="batchProgress">An optional callback to monitor the training progress (in terms of dataset completion)</param>
            <param name="trainingProgress">An optional progress callback to monitor progress on the training dataset (in terms of classification performance)</param>
            <param name="validationDataset">The optional <see cref="T:NeuralNetworkNET.SupervisedLearning.Parameters.ValidationDataset"/> instance (used for early-stopping)</param>
            <param name="testDataset">The optional <see cref="T:NeuralNetworkNET.SupervisedLearning.Parameters.TestDataset"/> instance (used to monitor the training progress)</param>
            <param name="token">The <see cref="T:System.Threading.CancellationToken"/> for the training session</param>
        </member>
        <member name="P:NeuralNetworkNET.SupervisedLearning.Optimization.NetworkTrainer.BackpropagationInProgress">
            <summary>
            Gets whether or not a neural network is currently processing the training samples through backpropagation (as opposed to evaluating them)
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.SupervisedLearning.Optimization.NetworkTrainer.Optimize(NeuralNetworkNET.Networks.Implementations.NeuralNetworkBase,NeuralNetworkNET.SupervisedLearning.Data.BatchesCollection,System.Int32,System.Single,NeuralNetworkNET.SupervisedLearning.Optimization.WeightsUpdater,System.IProgress{NeuralNetworkNET.SupervisedLearning.Progress.BatchProgress},System.IProgress{NeuralNetworkNET.SupervisedLearning.Progress.TrainingProgressEventArgs},NeuralNetworkNET.SupervisedLearning.Parameters.ValidationDataset,NeuralNetworkNET.SupervisedLearning.Parameters.TestDataset,System.Threading.CancellationToken)">
            <summary>
            Trains the target <see cref="T:NeuralNetworkNET.Networks.Implementations.SequentialNetwork"/> using the input algorithm
            </summary>
        </member>
        <member name="T:NeuralNetworkNET.SupervisedLearning.Optimization.WeightsUpdater">
            <summary>
            A delegate used to update the weights in a neural network after each training batch is processed
            </summary>
            <param name="i">The current offset with respect to the list of weighted layers in the network (it can be used to track external resources for each layer to update)</param>
            <param name="dJdw">The gradient with respect to the weights for the current layer</param>
            <param name="dJdb">The gradient with respect to the biases for the current layer</param>
            <param name="samples">The number of training samples evaluated in the current training batch</param>
            <param name="layer">The target layer to update</param>
        </member>
        <member name="T:NeuralNetworkNET.SupervisedLearning.Optimization.WeightsUpdaters">
            <summary>
            A static class that produces <see cref="T:NeuralNetworkNET.SupervisedLearning.Optimization.WeightsUpdater"/> instances for the available optimization methods
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.SupervisedLearning.Optimization.WeightsUpdaters.StochasticGradientDescent(NeuralNetworkNET.SupervisedLearning.Algorithms.Info.StochasticGradientDescentInfo)">
            <summary>
            Creates a stochastic gradient descent optimizer
            </summary>
            <param name="info">The optimizer parameters</param>
        </member>
        <member name="M:NeuralNetworkNET.SupervisedLearning.Optimization.WeightsUpdaters.Momentum(NeuralNetworkNET.SupervisedLearning.Algorithms.Info.MomentumInfo,NeuralNetworkNET.Networks.Implementations.NeuralNetworkBase)">
            <summary>
            Creates a momentum optimizer
            </summary>
            <param name="info">The optimizer parameters</param>
            <param name="network">The target network to optimize</param>
        </member>
        <member name="M:NeuralNetworkNET.SupervisedLearning.Optimization.WeightsUpdaters.AdaGrad(NeuralNetworkNET.SupervisedLearning.Algorithms.Info.AdaGradInfo,NeuralNetworkNET.Networks.Implementations.NeuralNetworkBase)">
            <summary>
            Creates an AdaGrad optimizer
            </summary>
            <param name="info">The optimizer parameters</param>
            <param name="network">The target network to optimize</param>
        </member>
        <member name="M:NeuralNetworkNET.SupervisedLearning.Optimization.WeightsUpdaters.AdaDelta(NeuralNetworkNET.SupervisedLearning.Algorithms.Info.AdaDeltaInfo,NeuralNetworkNET.Networks.Implementations.NeuralNetworkBase)">
            <summary>
            Creates an AdaDelta optimizer
            </summary>
            <param name="info">The optimizer parameters</param>
            <param name="network">The target network to optimize</param>
        </member>
        <member name="M:NeuralNetworkNET.SupervisedLearning.Optimization.WeightsUpdaters.RMSProp(NeuralNetworkNET.SupervisedLearning.Algorithms.Info.RMSPropInfo,NeuralNetworkNET.Networks.Implementations.NeuralNetworkBase)">
            <summary>
            Creates an RMSProp optimizer
            </summary>
            <param name="info">The optimizer parameters</param>
            <param name="network">The target network to optimize</param>
        </member>
        <member name="M:NeuralNetworkNET.SupervisedLearning.Optimization.WeightsUpdaters.Adam(NeuralNetworkNET.SupervisedLearning.Algorithms.Info.AdamInfo,NeuralNetworkNET.Networks.Implementations.NeuralNetworkBase)">
            <summary>
            Creates an Adam optimizer
            </summary>
            <param name="info">The optimizer parameters</param>
            <param name="network">The target network to optimize</param>
        </member>
        <member name="M:NeuralNetworkNET.SupervisedLearning.Optimization.WeightsUpdaters.AdaMax(NeuralNetworkNET.SupervisedLearning.Algorithms.Info.AdaMaxInfo,NeuralNetworkNET.Networks.Implementations.NeuralNetworkBase)">
            <summary>
            Creates an AdaMax optimizer
            </summary>
            <param name="info">The optimizer parameters</param>
            <param name="network">The target network to optimize</param>
        </member>
        <member name="T:NeuralNetworkNET.SupervisedLearning.Parameters.DatasetBase">
            <summary>
            A base class for an optional dataset to use in a training session
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.SupervisedLearning.Parameters.DatasetBase.Dataset">
            <summary>
            Gets the raw content of the current dataset
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.SupervisedLearning.Parameters.DatasetBase.Count">
            <inheritdoc/>
        </member>
        <member name="P:NeuralNetworkNET.SupervisedLearning.Parameters.DatasetBase.InputFeatures">
            <inheritdoc/>
        </member>
        <member name="P:NeuralNetworkNET.SupervisedLearning.Parameters.DatasetBase.OutputFeatures">
            <inheritdoc/>
        </member>
        <member name="P:NeuralNetworkNET.SupervisedLearning.Parameters.DatasetBase.ByteSize">
            <inheritdoc/>
        </member>
        <member name="P:NeuralNetworkNET.SupervisedLearning.Parameters.DatasetBase.Item(System.Int32)">
            <inheritdoc/>
        </member>
        <member name="P:NeuralNetworkNET.SupervisedLearning.Parameters.DatasetBase.Id">
            <inheritdoc/>
        </member>
        <member name="T:NeuralNetworkNET.SupervisedLearning.Parameters.TestDataset">
            <summary>
            A class that contains additional parameters to test a network being trained to monitor the general progress
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.SupervisedLearning.Parameters.TestDataset.ProgressCallback">
            <inheritdoc/>
        </member>
        <member name="P:NeuralNetworkNET.SupervisedLearning.Parameters.TestDataset.ThreadSafeProgressCallback">
            <summary>
            Gets the <see cref="T:System.IProgress`1"/> instance to safely report the progress from any source thread
            </summary>
        </member>
        <member name="T:NeuralNetworkNET.SupervisedLearning.Parameters.ValidationDataset">
            <summary>
            A class that contains additional parameters to validate the training data during a training session
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.SupervisedLearning.Parameters.ValidationDataset.Tolerance">
            <inheritdoc/>
        </member>
        <member name="P:NeuralNetworkNET.SupervisedLearning.Parameters.ValidationDataset.EpochsInterval">
            <inheritdoc/>
        </member>
        <member name="T:NeuralNetworkNET.SupervisedLearning.Progress.BatchProgress">
            <summary>
            A simple <see langword="struct"/> containing info on the current batch progress for the training dataset
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.SupervisedLearning.Progress.BatchProgress.ProcessedItems">
            <summary>
            Gets the current total number of processed samples
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.SupervisedLearning.Progress.BatchProgress.Percentage">
            <summary>
            Gets the current training dataset progress percentage
            </summary>
        </member>
        <member name="T:NeuralNetworkNET.SupervisedLearning.Progress.TrainingProgressEventArgs">
            <summary>
            A structure that contains the base progress data while optimizing a network
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.SupervisedLearning.Progress.TrainingProgressEventArgs.Iteration">
            <summary>
            Gets the current iteration number
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.SupervisedLearning.Progress.TrainingProgressEventArgs.Result">
            <summary>
            Gets the current cost value for the network
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.SupervisedLearning.Progress.TrainingProgressEventArgs.#ctor(System.Int32,System.Single,System.Single)">
            <summary>
            Internal constructor for the event args base
            </summary>
            <param name="iteration">The current iteration</param>
            <param name="cost">The current function cost</param>
            <param name="accuracy">The current network accuracy</param>
        </member>
        <member name="T:NeuralNetworkNET.SupervisedLearning.Trackers.BatchProgressMonitor">
            <summary>
            A simple monitor object that keeps track of the current training progress and notifies every time a batch is completed
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.SupervisedLearning.Trackers.BatchProgressMonitor.NotifyCompletedBatch(System.Int32)">
            <summary>
            Notifies the completion of a new training batch
            </summary>
            <param name="size">The size of the completed training batch</param>
        </member>
        <member name="M:NeuralNetworkNET.SupervisedLearning.Trackers.BatchProgressMonitor.Reset">
            <summary>
            Resets the progress monitor at the end of a training epoch
            </summary>
        </member>
        <member name="T:NeuralNetworkNET.SupervisedLearning.Trackers.RelativeConvergence">
            <summary>
              Relative convergence criteria
            </summary>
            
            <remarks>
              This class can be used to track progress and convergence
              of methods which rely on the relative change of a value
            </remarks>
            
            <example>
            <code>
              // Create a new convergence criteria with unlimited iterations
              var criteria = new RelativeConvergence(iterations: 0, tolerance: 0.1);
              
              int progress = 1;
              
              do
              {
                  // Do some processing...
              
              
                  // Update current iteration information:
                  criteria.NewValue = 12345.6 / progress++;
              
              } while (!criteria.HasConverged);
              
              
              // The method will converge after reaching the 
              // maximum of 11 iterations with a final value
              // of 1234.56:
              
              int iterations = criteria.CurrentIteration; // 11
              float value = criteria.OldValue; // 1234.56
            </code>
            </example>
        </member>
        <member name="P:NeuralNetworkNET.SupervisedLearning.Trackers.RelativeConvergence.Tolerance">
            <summary>
            Gets or sets the maximum relative change in the watched value
            after an iteration of the algorithm used to detect convergence
            </summary>
        </member>
        <member name="F:NeuralNetworkNET.SupervisedLearning.Trackers.RelativeConvergence.ConvergenceWindow">
            <summary>
            Gets the size of the convergence window
            </summary>
        </member>
        <member name="M:NeuralNetworkNET.SupervisedLearning.Trackers.RelativeConvergence.#ctor(System.Single,System.Int32)">
            <summary>
            Initializes a new instance of the <see cref="T:NeuralNetworkNET.SupervisedLearning.Trackers.RelativeConvergence"/> class
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.SupervisedLearning.Trackers.RelativeConvergence.Value">
            <summary>
            Gets or sets the watched value after the iteration
            </summary>
        </member>
        <member name="P:NeuralNetworkNET.SupervisedLearning.Trackers.RelativeConvergence.HasConverged">
            <summary>
            Gets whether the algorithm has converged
            </summary>
        </member>
    </members>
</doc>
